[{"url": "http://blog.kaggle.com/2014/01/07/qa-with-bryan-miroslaw-2nd-place-in-the-see-click-predict-fix-competition/", "link_text": "Q&A with Bryan & Miroslaw, 2nd Place in the See Click Predict Fix Competition", "id": 0, "title": "Permalink to Q&A with Bryan & Miroslaw, 2nd Place in the See Click Predict Fix Competition", "date": "2014-01-07", "text": "\nWhat was your background prior to entering this challenge?\nMy professional background is in business intelligence and analytics/reporting and Miroslaw\u2019s background is in mathematics, so neither of us has a formal background in machine learning. However, we have both taken multiple online classes in machine learning topics, including Andrew Ng\u2019s excellent StanfordX Machine Learning course.\nWe also have both competed in quite a few Kaggle competitions in the past year, steadily improving our skills and knowledge with each finish. Kaggle and its community have proven to be a goldmine for anyone interested in learning real-world machine learning techniques outside of academia.\nWhat made you decide to enter?\nMiroslaw and I both competed independently in the first portion of the SeeClickFix competition, a 24-hour Hackathon, and we performed well. We really enjoyed the dataset and the objective, so competing in the second portion of the contest seemed a natural progression for us both.\nIn the second portion of the contest, we both competed independently until about two weeks before the contest completion. At that time, we had both been steadily climbing the leaderboard with myself hitting 1st place on Nov. 16th and Miroslaw right behind me at 4th place. However, J.A. Guerrero had just joined the contest, and he and other top competitors were making steady gains on the leaderboard as well. At that time we decided it was best to hedge our bets and improve our odds of placing in the money by combining our models into a powerful ensemble.\nWhat preprocessing and supervised learning methods did you use?\nMy approach was composed of a segmentation ensemble. This consisted of a distinct base model trained independently on each city and with remote API sourced issues trained together separately as well -- for a total of five segments. Because the variables for each of the cities seemed quite distinct (a lot of variable interaction present), this data seemed ideally suited to a segmentation ensemble. One advantage of segmentation is that it greatly reduces the number of samples and the dimensionality that each base model must deal with, so I was able to utilize gradient boosted regressors (GBMs) for most of the base models.\nMiroslaw\u2019s approach was more focused on the text analysis side of the data. Using text from the issue summary and description fields, he created a tri-gram TFIDF vector from the entire training set, then combined it with other features that we had engineered. Because of the high dimensionality, only linear models were practical for this approach, and of those he found that ridge regression performed best.\nAfter we teamed up, our first week was spent strengthening our individual models by integrating some of the stronger features from each other\u2019s code into our own and measuring the improvement, based on a combination of cross-validation score and leaderboard feedback. Our intuition was that while we did want to keep our models distinct to reduce bias within our ensemble, if a feature was proven to improve both cross-validation and leaderboard scores significantly, than its signal was powerful enough to justify inclusion in both base models.\nThen our final week was spent determining the ideal weights for averaging our models\u2019 predictions together. A simple 50/50 blend made a surprisingly large gain on the leaderboard, but we then went more in-depth and performed segment based weighting for each of the models. For this we used a linear regression model to derive weights based on optimal cross-validation scores for each segment. Not surprisingly, the findings were that in some segments and targets Miroslaw's model performed better and needed to be weighted higher, while on others mine performed better. Lastly, to avoid overfitting the cross-validation test set we reduced weights to be less extreme if the linear model weighted either of our individual models too strongly.\nThe features used in our models were largely similar with other top competitors. We found that some of the best signals in the data came from description length, geographic location, issue tag type, issue source, summary/description text, and whether the issue was created on a weekend. From these, we created various binary and one hot encoded features, and we also used a reverse-geocoding service to derive zipcodes and neighborhoods from the issue\u2019s latitude/longitude fields given in the data. This ended up paying off for us as neighborhoods in particular ended up being powerful predictors, more powerful than simply using latitude and longitude to approximate location.\nWhat was your most important insight into the data?\nBecause this contest was temporal in nature, using time-series models to make future predictions, most competitors quickly realized that proper calibration of predictions was a major factor in reducing error. Even during the initial Hackathon stage of the contest, it became well known on the competition forum that one needed to apply scalars to predictions in order to optimize leaderboard scores.\nBut while scaling was common knowledge, our most important insight came in applying our segmentation approach to the scalars. For example, rather than apply one optimized scalar to all predicted views for the entire test set, we applied optimized scalars for each distinct segment of the test set (the remote API sourced issues and the four cities). We then optimized the scalars using a combination of leaderboard feedback and cross-validation scores. What we found was that each segment responded differently to scaling-- so trying to apply one scalar to all issues, as many of our competitors were doing, was not optimal.\nWere you surprised by any of your insights?\nBy far our biggest surprise was the effectiveness of creating an ensemble model from our individual models. Initially, before deciding to team up, we were concerned that we may see minimal gain from combining our predictions, but we quickly realized that this was not the case. Even a simple average of the two gave a huge decrease in error.\nThe important lesson for us was that two distinctly developed models will yield huge gains when combined together, particularly when the two have a high degree of diversity (variance in errors) which is critical for an ensemble to perform well. We were fortunate that while our models shared many of the same features, Miroslaw\u2019s had been designed to take more advantage of the text-based features and mine had been designed more to take advantage of the segments in the data.\nWhich tools did you use?\nWe both used the same stack of Python tools: scikit-learn, PANDAS, and NumPy. This ended up being a fortunate coincidence as it made it easy for us to share code snippets with each other.\nWhat have you taken away from this competition?\nFirst, it was a great experience collaborating together as a team. This was our first experience working on a team in a Kaggle contest, and we both agreed that it will not be the last. Working together allowed us to see the same problem from new angles and we learned many new techniques in just the two weeks we worked together. It was enlightening seeing the many creative solutions and insights that we each had developed. Our only regret is not teaming up earlier in the contest.\nSecond, we both learned the power of ensembles. Prior to this contest, neither of us had utilized higher level ensembles in previous competitions, always instead focusing on improving one strong model. No longer. Going forward, ensembles will be an important and often used tool in our toolkit. In fact, we were so motivated by our results that Miroslaw and I are developing a Python module to help facilitate the creation and use of ensembles.\nLastly, we thoroughly enjoyed participating in this contest, and we greatly appreciate SeeClickFix.com, David Eaves, and Kaggle for graciously providing us the opportunity to work with their data. Working with this data set was a blast and we had a great time learning from it and gaining new insight.\n------------------------------------------------------------------------------\n\nBryan Gregory holds a B.S. in Information Systems from Texas A&M University and an M.B.A. with a concentration in Information Technology from Baylor University. He is a frequent Kaggler with an interest in the practical application of machine learning and business intelligence.\nMiroslaw Horbal has a B.S. in Mathematics with a specialization in Combinatorics and Optimization from the University of Waterloo. He is a self-taught machine learning enthusiast with an addiction to data science competitions.\n"}, {"url": "http://blog.kaggle.com/2014/01/07/qa-with-james-petterson-3rd-place-winner-see-click-predict-fix-competition/", "link_text": "Q&A with James Petterson, 3rd Place Winner, See Click Predict Fix Competition", "id": 1, "title": "Permalink to Q&A with James Petterson, 3rd Place Winner, See Click Predict Fix Competition", "date": "2014-01-07", "text": "\nWhat was your background prior to entering this challenge?\nI studied Electrical Engineering during undergraduate school, and worked as a software engineer in the telecom industry for several years. Later on I moved to Australia to pursue a PhD in Machine Learning at ANU/NICTA, which I finished a couple of years ago. I'm currently working as a Data Scientist at Commonwealth Bank.\nWhat made you decide to enter?\nI'm currently refraining from participating in long competitions, given how time consuming they can be, but since I had good results in the See Click Predict Fix - Hackathon I thought it would be worth trying this one.\nWhat preprocessing and supervised learning methods did you use?\nI combined several methods in an ensemble, the main ones being boosting trees (GBM) and linear regression. I also tried random forests and neural networks, but I didn't invest much time in them.\nWhat was your most important insight into the data?\nProbably the most important insight was the fact that the distributions of the features and the labels were highly dependant on physical location and time.\nThe first aspect is easy to model, either by building separate models for each city, or by using an algorithm that can capture feature interactions.\nThe temporal aspect, however, is harder to deal with. Assuming the conditional distribution of the labels given the features is the same across all periods, we can apply covariate shift corrections. I tried a few methods, such as Kernel Mean Matching, but the performance gains were negligible.\nWhat did work was applying a simple constant scaling to the predictions (one for each city and each target variable). It only worked well, however, because I used feedback from the leaderboard to adjust these constants. This is not ideal, as we won't be able to do that in a real life situation, but given the way the competition was designed, it is unlikely that it would be possible for anyone to win without resorting to this kind of adjustment.\nWere you surprised by any of your insights?\nI was surprised that I couldn't extract much information from the description texts, as I thought that would be one of the richest sources of information.\nWhich tools did you use?\nMost of the work was done in R. For linear models I used vowpal wabbit, and to compute vector representations of the description texts I tried word2vec.\nWhat have you taken away from this competition?\nIt reminded me once again of the power of building several models together. That was the winner's big column approach (described here), where all three target variables (comments, views and votes) where trained in a single model.\n"}, {"url": "http://blog.kaggle.com/2013/12/23/qa-with-jose-guerrero-1st-place-winner-see-click-predict-fix-competition/", "link_text": "Q&A with Jos\u00e9 Guerrero, 1st Place Winner, See Click Predict Fix Competition", "id": 2, "title": "Permalink to Q&A with Jos\u00e9 Guerrero, 1st Place Winner, See Click Predict Fix Competition", "date": "2013-12-23", "text": "\nWhat was your background prior to entering this challenge?\nI have a BSc + MSc in Mathematics, Statistic and Operations Research and postgraduate in Scientific Programming. I'm a health data analyst in a university hospital. I discovered Kaggle when Heritage Health Prize was launched and since then I've participated in several challenges.\nWhat made you decide to enter?\nI couldn't participate in\u00a0the\u00a0hackathon, so I think this was like a second opportunity. The problem was very attractive and I thought the idea of citizen participation for detect community issues was very interesting.\nWhat preprocessing and supervised learning methods did you use?\nAfter reading the hackathon forum and seeing how the better results were obtained with a short training data set with most recent observations, I decided\u00a0as a priority to use as much data as possible for giving the model enough robustness.\nThe main problem was dealing with the time anomalies, so I forced the models to learn without using absolute time features (day the issue was sent).\nThe hypotheses were:\n\nThe response to an issue depends (directly or inversely) of number of recent issues and similar issues (time dimension).\nThe response to an issue depends (directly or inversely) of number of issues and similar issues reported close (geographic dimension).\nThere are geographic zones more sensitive to some issues (geographic dimension).\n\nWith that in mind I defined three time windows -- short, middle and long -- and three epsilon parameters for using them in a radial basis distance-weighted average for each issue.\nThe selection of these values were for adjusting the decay shape in a way the weights represent city, district and neighbour ambits.\nFor each issue I computed a 3x3 grid of features for each tag group, using radial basis weights respects the distance in kms between issues.\nFor a period of last 150 days and for each issue, I computed the LOO (Leave One Out) weighted radial basis average for comments, votes and views for city, district and neighbour parameters.\nFor summary feature I created a binary bag of more frequents words.\nI fitted several models: boosted trees, random forest and general linear models and ensembled them with a ridge regression to calibrate the estimations.\nWhat was your most important insight into the data?\nThe source of the issues had a great importance in the responses, and in each city the features' range of values and variability were very different. This explains why using stratified models worked so well. The preprocessing of the data was crucial for fitting models for all the cities at a time.\nWere you surprised by any of your insights?\nIn my models, bag of words of summary had a small influence in predictions. I think probably a binary bag of words is\u00a0too simple and using high order tuples would be necessary.\nThe models trained with the grid of features got extract the time and geographical information without using an absolute time feature nor longitude and latitude.\nWhich tools did you use?\nR packages gbm, randomForest and glmnet.\nWhat have you taken away from this competition?\nI'm surprised how well the big column approach (train the responses all together and stacking the dataset one time for each one of them) works in this case.\nAnd with this competition I got the #1 spot in Kaggle rankings. I'll never forget that!\n------------------------------------------------------------------------------\n\nJos\u00e9\u00a0Guerrero won First\u00a0Place in the See Click Predict Fix Competition. He has worked more than 25 years in the\u00a0health sector in Spain in epidemiology, research, electronic medical records, and\u00a0senior management\u00a0at a university hospital.\u00a0 He is currently crunching\u00a0big databases at the region's main hospital.\n"}, {"url": "http://blog.kaggle.com/2013/08/29/qa-with-amazon-access-challenge-first-prize-winner-paul-duan/", "link_text": "Q&A With Amazon Access Challenge First Prize Winner Paul Duan", "id": 3, "title": "Permalink to Q&A With Amazon Access Challenge First Prize Winner Paul Duan", "date": "2013-08-29", "text": "\nWe caught up with the winner of the immensely popular Amazon Access Challenge\u00a0to see how he edged out thousands of competitors to predict which employees should have access to which resources.\nWhat was your background prior to entering this challenge? What did you study in school, and what has your career path been like?\nMy background is a bit eclectic; I spent my time in undergrad multitasking between three universities (UC Berkeley, Sciences Po Paris, and the Sorbonne), where I studied mathematics, econom(etr)ics, and social sciences. I\u2019ve since been self-learning machine learning and programming on the job -- the engineers I work with would tell you that I still have a long way to go regarding the latter.\nI recently moved from France to San Francisco. I currently work as a Data Scientist at Eventbrite, where I am in charge of building the fraud and spam detection models. I also teach data science on occasion, most recently at Zipfian Academy.\nWhy did you enter?\nFirst, I was curious to see how well I could place using the knowledge I had recently acquired. Given of the huge number of participants in the Amazon challenge, this was the ideal competition to enter.\nSecond, the fact you\u2019re studying the same dataset as many other people makes for great dialogue opportunities. This is why I tried sharing as much as possible during the competition, and the response has been very inspiring. People were starting interesting discussions left and right, notably Miroslaw Horbal who sparked a great exchange about feature selection.\nWhat preprocessing and supervised learning methods did you use?\nI used an ensemble of linear and tree-based models that were each trained on a slightly different feature set. The features themselves were extracted by cross-tabulating each categorical variable so as to get an idea of how rare each combination is -- because most requests end up being approved and because the number of different categories made up for a lot of noise, I chose to treat the problem more as an outlier detection problem and I created my features as such.\nThe models were then combined by using their output as an input for a modified linear regression. This second-stage model also incorporated the size of the support for each category as meta features in order to dynamically decide which base model to trust the most in different situations. I ultimately teamed up with Benjamin Solecki, who used a very similar method with slightly different features, which further improved our score when incorporated into the ensemble.\nYou can find the code on Github\u00a0and a more detailed explanation of the methodology on the forums.\nWhat was your most important insight into the data?\nNot spending too much time in feature selection vs. feature engineering. \u00a0Because there was a lot of noise in the dataset and the variance seemed to be high depending on how I would split my train/cross-validation sets, I focused mostly on improving the generalization power of my algorithms by creating classifiers with different strengths.\nWere you surprised by any of your insights?\nI noticed that fine tuning (both in terms of feature selection and hyperparameter optimization) didn\u2019t seem as critical in the context of ensembles of different classifiers. In fact, I would sometimes notice that changes that improved the performance of each of my individual models would actually decrease the performance of the overall ensemble!\nWhich tools did you use?\nI used mostly Python with the scikit-learn library, with a mix of pandas and R for data exploration.\nWhat have you taken away from this competition?\nI learned quite a bit about how to find a balance between feature engineering and feature selection, both in the context of single models and more complex ensembles. The fact the data only consisted of categorical variable was an added challenge as well.\nI think what is great about Kaggle competitions is the fact they are self-contained: you have a well-defined problem, a well-defined dataset, and a clear evaluation metric. As such, they are an ideal testing ground for new ideas and algorithms. In real life, things are unfortunately not as easy. You often end up having to figure out how you want to build your model, what data you want to use (and how to get it), and the optimization objectives all at once -- sometimes for problems that don\u2019t even exist yet.\n"}, {"url": "http://blog.kaggle.com/2013/05/06/qa-with-job-salary-prediction-first-prize-winner-vlad-mnih/", "link_text": "Q&A With Job Salary Prediction First Prize Winner Vlad Mnih", "id": 4, "title": "Permalink to Q&A With Job Salary Prediction First Prize Winner Vlad Mnih", "date": "2013-05-06", "text": "\nWhat was your background prior to entering this challenge?\nI just completed a PhD in Machine Learning at the University of Toronto, where\u00a0Geoffrey Hinton\u00a0was my advisor. Most of my work is on applying deep learning techniques to aerial image analysis, so I have a lot of experience in training neural networks with tens of millions of parameters on big datasets.\nWhy did you enter?\nI had a bit more spare time after completing my thesis so I decided to do a quick project before leaving Toronto.\u00a0 I chose this particular competition because it involved text data and, while that is not something I had a lot of experience with, it seemed like a problem where neural nets should do well (and indeed the 2nd place finisher also used a neural net).\nWhat preprocessing and supervised learning methods did you use?\nI did relatively little preprocessing and feature engineering.\u00a0 I used separate bags of words for the job title, description, and the raw location.\u00a0 I also found that stemming the words in the title and description using the Porter stemmer and encoding them using tf-idf slightly improved the performance. The other fields, like the category, contract, and source, were represented using a 1-of-K encoding.\u00a0 The resulting input representation had between 10000 and 15000 features depending on how many of the top words I used.\u00a0 I did experiment with a number of alternative features and encodings but I did not get any noticeable improvements.\nFor the supervised learning part, I used deep neural networks implemented on a GPU.\u00a0 I trained the neural nets by optimizing mean absolute error (the evaluation metric for this contest) using minibatch stochastic (sub)-gradient descent and used dropout in order to help avoid overfitting.\u00a0 My best single neural network achieved a score of about 3475 on the public leaderboard, but my final submission averaged the predictions of three neural networks to get down to about 3435.\u00a0 I did not combine neural networks with any other learning methods.\nThis approach might sound familiar to readers of this blog because my office mates,\u00a0George Dahl\u00a0and\u00a0Navdeep Jaitly, and their team mates recently used a nearly identical architecture in their winning entry for the Merck Molecular Activity Challenge, although there are some differences due to the particulars of that contest.\nWhat was your most important insight into the data?\nMy most important insight was to simply train a powerful and flexible model by directly optimizing the loss function used to determine the winner. Some competitors used complicated ensembles of many disparate models, most of which were not optimizing the correct objective. These people needed to use\u00a0leaderboard and validation error feedback much more heavily than I did since their model selection process was the only part of their pipeline that directly optimized the evaluation metric.\nWere you surprised by any of your insights?\nI was somewhat surprised by how little improvement I got from my attempts to engineer better features.\u00a0 For example, I didn't get any improvement from using bigrams or from adding information derived from the normalized location or location tree.\u00a0 Since other competitors have reported noticeable gains in performance from using these features on the competition forum, I suspect that the deep nets I trained were able to learn some of these features automatically.\u00a0 While this is definitely a pleasing result, it is a little surprising even to neural network experts because neural nets are generally considered to be quite sensitive to the input representation.\nWhich tools did you use?\nI used Python along with a number of open-source Python packages.\u00a0 I used pandas for loading and exploring the data and scikit-learn for its feature extraction pipeline, although I ended up implementing my own text vectorizers for improved memory efficiency.\u00a0 I also used NLTK for its implementation of the\nPorter stemmer.\u00a0 Finally, I used my own implementation of deep neural networks which relies on\u00a0Tijmen Tieleman's\u00a0gnumpy\u00a0library and my own\u00a0cudamat\u00a0library for GPU support.\nWhat have you taken away from this competition?\nI learned quite a bit about how feature engineering interacts with different neural network architectures.\u00a0 In particular, I thought it was really interesting that\u00a0Vlado Boza\u00a0placed 2nd with a completely different neural network architecture and set of features.\nVlad Mnih\u00a0is a machine learning researcher based in London, England. \u00a0He holds a PhD in Machine Learning from\u00a0the University of Toronto and an MSc in Machine Learning from the University of Alberta.\n"}, {"url": "http://blog.kaggle.com/2013/05/06/summary-of-the-whale-detection-competition/", "link_text": "Summary of the Whale Detection Competition", "id": 5, "title": "Permalink to Summary of the Whale Detection Competition", "date": "2013-05-06", "text": "\nPosting a summary on behalf of Cornell researchers. From my side I would like to add, that Marinexplore has partnered with Cornell University to develop acoustics related capabilities of our spatio-temporal data platform. Improved analytics of acoustic data is relevant not only to shipping industry, but also to other businesses like offshore industry. Globally there are many public acoustic datasets yet to be integrated with\u00a0marinexplore.org\u00a0as well.\nThank you everyone for participating in our challenge and pushing the boundaries together.\u00a0Feel free to contact me directly should you want to use our solutions in your organization, explore collaboration options, join our team or just learn more about Marinexplore.\nMeanwhile we posted a summary of the competition in\u00a0our blog\u00a0and launched an exploratory data challenge for finding the best use of public ocean data with a\u00a0prize of $3000.\nAndr\u00e9 Karpi\u0161t\u0161enko\nCo-founder at Marinexplore, Chief Scientist\nandre@marinexplore.com\nskype:andre\n\nThe Bioacoustic research program (BRP) at Cornell University has had the honor to co-host with Marinexplore the first ever North Atlantic right whale call-classification competition. Thank you all for contributing your time and never-ending brainstorms, and for making the competition exciting, interesting, intellectually rewarding and totally successful.\nWe received the documents and source codes from the top two winning Kaggle participants. Many participants also kindly share their insightful thoughts and even source codes on the competition\u2019s message board. We are currently building a new automated right whale detection-classification system, which will include the algorithms from the Kaggle competition and will apply it to a 44-month, continuous recording dataset. We expect that this system will yield a greater understanding of right whale calling behavior, such as their daily & seasonal communication patterns, as well a deeper understanding of the influences of human noise on the whales\u2019 acoustic communication and habitat. You, the participants in this competition, have been and still are the most important partners in our efforts to save right whales.\nMethods\nBoth winners used an approach that defines a frequency-time \u201ctight box\u201d bounding the occurrence of the right whale call in a spectrogram, followed by extraction of a customized set of features for each tight box. The 1st place winning team used a multiple template matching approach, while the 2nd place winning team used a Viterbi algorithm to find the exact trajectories of frequency up-sweeps. The tight boxes make the features more consistent and robust and thus more frequency-invariant and/or time-invariant.\nBoth winning methods also designed several feature vectors from different perspectives to incorporate information from either the spectrum, the temporal dynamics of a call\u2019s frequency-modulation, and even the temporal ordering of labeling (positive or negative). The last variable, temporal ordering, emerged from the ordering and numbering of the files and labels identifying the calls in the dataset. As a result, many positive classification events appear consecutively. This temporal clustering feature in this dataset might not be something reliable that we could use in our updated automated detection system. However, this feature could be useful to discriminate between right whale up-calls, which almost always occur as individual transients, and humpback whale frequency-modulated upsweeps, which are either notes within a song or produced as a series of calls.\nMany participants applied a deep learning approach (in particular, a convolutional network) and achieved high scores (e.g. contestants ranked #3, #4, and #6). In our understanding of their deep learning approach, the spectrogram of a right whale call is treated as an image in much the same way as a handwritten digit.\nMany contestants used Python as the preferred programming language, reflecting the fact that modules of Python, such as Sci-Kit-learn, Sci-py, Num-py, have become standards in the world of data analysis. Accordingly, several classifiers, for example gradient boosting and random forest, were preferred over others by the participants.\nData integrity\nSeveral participants expressed concerns about data integrity. To some participants some of the audio clips tagged as right whale up-calls did not sound like an up-call, and vice versa. The following are two additional results we need to keep in mind for the particular dataset used in this competition:\n(i) Some audio clips had very low signal-to-noise ratio (SNR).\n(ii) An audio clip tagged as a right whale up-call might actually be a non-biological sound or a sound from a different species.\nWhen both (i) and (ii) occur simultaneously, things can get tricky. The energy from a right whale call might be much lower than the energy from the other sound object in the sound sample. On the other hand, some audio clips tagged as \u201cno-call\u201d sounded like and could appear similar to an up-call in a spectrogram. One possible explanation for this conundrum is that humpback whales, which are renown for their vocal virtuosity, are responsible for these confounding calls. However when humpbacks produce up-call like sounds, they typically produce them in a repetitive sequence. Thus, if a longer acoustic sample had been provided, instead of just the 2-sec clip, discrimination between a single call occurrence (i.e. a right whale up-call) and a sequence (i.e., a humpback song note or call sequence) might have been more obvious, thereby improving correct classification of the sound.\nFuture\nWe are going to apply the top two winning methods, along with other methods developed in the Bioacoustic Research Program, to improve our abilities to automatically detect and classify right whale calls. The suite of new methods will also include deep learning and computer-vision-based techniques. All of these methods will be a core part of our new, automated acoustic detection-classification system for large-scale analysis for endangered species, including whales, elephants and birds. One of the first technical challenges is to have the automatic detection-classification process operate on a continuous, long-duration audio stream (e.g. months to years). We\u2019re investigating methods from computer vision and image processing that will locate connected regions, as well as an efficient method for applying a sliding window, by which classification is repeatedly applied along a continuous audio stream. Presently a comprehensive performance evaluation is ongoing using an 8-day dataset. One goal in the next few months is to apply methods from this competition on a 44-month, continuous underwater sound recording. Another very important goal is to use the source code that you all have produced to improve automatic detection-classification systems that listen for whales in order reduce the chances of whales being killed by ships (e.g. right whales in the shipping lanes off Boston, USA,\u00a0www.listenforwhales.com).\nIt is very obvious from the energy and productivity of the participants in this competition that this was not just about prize money. It was about how a group of smart, motivated people, who were strangers, could work as a group of competitive altruists, to produce software that will have a real benefit for the natural world and the ocean environment, and especially for improving the chances of survival for a species that is near extinction. A huge, huge thank you to all the participants of this excellent competition.\nAnd a huge, huge thank to Kaggle and Marinexplore for enabling this to become reality.\n"}, {"url": "http://blog.kaggle.com/2013/04/29/qa-with-vlado-boza-2nd-place-winner-job-salary-prediction-competition/", "link_text": "Q&A with Vlado Boza, 2nd Place Winner, Job Salary Prediction Competition", "id": 6, "title": "Permalink to Q&A with Vlado Boza, 2nd Place Winner, Job Salary Prediction Competition", "date": "2013-04-29", "text": "\nWhat was your background prior to entering this challenge?\nI am finishing my Master\u2019s degree in computer science. I was a software engineering intern at Google working on some machine learning problems.\u00a0I've\u00a0also entered several Kaggle competitions during the last year. I am the founder of Black Swan Rational - a Slovak company specialized in predictive analytics.\nWhat made you decide to enter?\nI had some spare time, so I decided it to spend it on some Kaggle competition. At that time there were three competitions running: Job Salary Prediction, Blue Book for Bulldozers, and Whale Detection. Whale Detection already had quite impressive submissions and I\u00a0didn't\u00a0want to spent time just by tweaking a model to get a 0.001 % difference. With Blue \u00a0Book, I thought that there\u00a0would\u00a0be no significant difference between the random forest benchmark and the best submission and it would end up as a big ensemble fight. The Job salary data seemed to be pretty clean and easy to work with. And also there were a lots of possible approaches.\nWhat preprocessing and supervised learning methods did you use?\nI extracted simple binary text features from title and description and also used categorical features for location, company, and source. My whole model was just an old-school neural network with two small hidden layers trained by back propagation. Before that I used nearest neighbor model which was quite successful (got error around 4200).\nWhat was your most important insight into the data?\nDuring one point I found out that there are too many similar ads and that their salary differs on average by 2000. I used this in my nearest\u00a0neighbor\u00a0model. But neural network could handle this even better without any hacks.\nWere you surprised by any of your insights?\nAd similarity was the only thing.\nWhich tools did you use?\nI have coded all of my algorithms in C++ (I did small preprocessing in Python). I tried to use scikit-learn but it\u00a0didn't\u00a0lead to any big success.\nWhat have you taken away from this competition?\nI have to improve my coding practices.\u00a0I've\u00a0made many stupid bugs just because of this. And I also should start to use some versioning system better than \u201cdo backup sometimes\u201d.\n\u00a0\n------------------------------------------------------------------------------\n\nVlado Boza\u00a0won Second Prize in the Adzuna Job Salary Prediction Competition. He is finishing his Master's studies of computer science at Comenius University in Bratislava.\u00a0He spent two summers as Software engineering intern at Google working on machine learning problems.\u00a0His interests include building fast and effective algorithms, hard optimization problems and machine learning.\n"}, {"url": "http://blog.kaggle.com/2013/04/23/qa-with-guocong-song-3rd-prize-job-salary-prediction-competition/", "link_text": "Q&A With Guocong Song, 3rd Prize, Job Salary Prediction Competition", "id": 7, "title": "Permalink to Q&A With Guocong Song, 3rd Prize, Job Salary Prediction Competition", "date": "2013-04-23", "text": "\nWhat was your background prior to entering this challenge?\nI had been working on wireless communication and signal processing for over 10 years and was well established. I received the 2010 IEEE Stephen O. Rice Prize (best paper award for communications), and was serving as an editor for IEEE Transaction on Wireless Communications. It was my wife who told me about the Netflix prize two years ago. Since then, I'm more interested in data science. Of course, participating in Kaggle challenges gives me valuable experience.\n\u00a0What made you decide to enter?\nBen's benchmark code already established the pipeline that avoids a lot work on data IO. It was extremely attractive to me at that time since I was very exhausted with the GE flight quest. I started working on the problem two weeks before the deadline. Therefore, I would like to thank Ben for his initial work. Technically speaking, most text mining problems belong to classification; I wanted to gain some experience of regression with text mining.\nWhat preprocessing and supervised learning methods did you use?\nTypical text feature extraction techniques are applied to the raw data, such as text normalization, stop words, n-grams, TF-IDF. I tried ridge regression, SGD, random forests and also converted the regression problem into a classification one, for which I tried native Bayes, SVM, and logistic regression. Finally, I blended the SGD regression and logistic regression based predictor.\nWhat was your most important insight into the data?\nSince salaries are not distributed smoothly, some models that can explore local properties would outperform linear regression. My background in information theory also helped me discover that 4~5 bits good enough to quantize salary values, which benefits computational complexity reduction.\n\u00a0Were you surprised by any of your insights?\nNo surprise on the score of each submission made a surprise to me. Overfitting didn't bother me with most methodologies I tried. The results are very consistent in cross-validation and two leader boards.\nWhich tools did you use?\nPython, scitkit-learn.\nWhat have you taken away from this competition?\nIn this competition, there are no significant features at all. It is not surprising that the first and second winners all use neural networks. More interestingly, my model can be regarded as a neural network with a manually created hidden layer. It does help me understand neural networks / deep learning better.\n-------------------------------------------------------------------------------\n\nGuocong Song\u00a0placed third in the Adzuna Job Salary Prediction competition. He received his\u00a0PhD in Electrical and Computer Engineering from Georgia Institute of Technology MS, and his BS in Electrical Engineering from Tsinghua University Aside from data science, his expertise is in: Signal processing, stochastic optimization, wireless networks and devices He has received the IEEE Stephen O. Rice Prize Paper Award, and the best paper award in IEEE Transactions on Communications in 2010. He lives in\u00a0Cupertino, CA.\n"}, {"url": "http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/", "link_text": "Q&A with Xavier Conort", "id": 8, "title": "Permalink to Q&A with Xavier Conort", "date": "2013-04-10", "text": "\nXavier Conort is currently the number 1 ranked Kaggle data scientist and member of team \"Gxav &*\", winners of\u00a0Flight Quest.\nQ: What is your background? What did you study in school, and what has your career path been like?\nXavier Conort: I am a French actuary with more than 15 years of working experience in France, Brazil, China, and Singapore. I studied actuarial science and statistics in ENSAE Paris Tech and University Paris Denis Diderot. Before becoming a data science enthusiast, I held different roles in the insurance industry (actuary, CFO, and risk manager).\nI currently work in the Data Analytics department of I2R (Institute for Infocomm Research, a research institute under the A*STAR family in Singapore) and develop analytics techniques and solutions together with my teammates of the GE Flight Quest. Our department has around 40 data scientists and serves several major clients like Visa and Boeing. We are one of Singapore\u2019s largest R&D teams of data scientists.\nMy teammates Hong Cao, Hon Nian Chua, Clifton Phua, and Ghim Eng Yap have PhDs in various areas of data analytics. They were all trained in Singapore, except Clifton who was trained in Australia. Recently, Hon Nian completed his post-doc stints in the University of Toronto and Harvard University, and Clifton left our department and joined SAS.\nQ: How long have you been competing on Kaggle?\nI started to compete about 18 months ago but am already considered a veteran.\nQ: What other kinds of challenges have you solved for companies through Kaggle?\nThe problems I solved for companies through Kaggle were very diverse. I, with Marcin Pionnier,\u00a0detected if a car purchased at auction is a good buy or a lemon in \u201cDon\u2019t Get Kicked\"\u00a0(1st). I predicted with my teammates from DataRobot biological activities of different molecules given numerical descriptors generated from their chemical structures in the \u201cMerck Molecular Activity Challenge\"\u00a0(2nd). I forecasted monthly online sales in \u201cOnline Product Sales\"\u00a0(2nd). \u00a0I modeled the probability that somebody will experience financial distress in \u201cGive Some Credit\"\u00a0(2nd). I developed scoring engines to support the grading of student written essays in the 2 challenges hosted by the Hewlett Foundation (4th). I predicted customer retention for Allstate in \u201cWill I Stay or Will I Go?\" (4th). And I identified patients diagnosed with Type 2 Diabetes in \u201cPractice Fusion Diabetes Classification\"\u00a0(4th).\nMy teammates for GE Flight Quest have also won academic data mining competitions (outside Kaggle) together with various colleagues from I2R. They placed 1st in PAKDD 2012 Churn Prediction, ACML 2012 Fraud Detection in Mobile Advertising, and Opportunity\u2019s 2011 Mobile Activity Recognition Challenge. In addition, they have achieved top-5 positions in many other competitions.\nQ: What do you like best about these competitions? Why do you think they\u2019re successful at solving problems for businesses and other organizations?\nI like the diversity of problems to solve and I enjoy getting live feedback from the public leaderboard. It makes the fight for the best model very concrete.\nI believe that the competition framework is a win-win scenario. Competitors get access to real-world data to test their algorithms and their modeling skills. Competition hosts benefit by bringing out the best from us, obtain very strong accuracy benchmarks and get the opportunity to implement innovative solutions coming from different industries.\nQ: What skills do you think are important for a successful data scientist? Did you learn these skills in school, on the job, or on your own?\nI think that what makes a good data scientist is more of the right attitude than skills. Besides a strong background in statistics or computer science, a good data scientist is a person who loves to solve problems. (S)he is not afraid of putting is (possibly) unrecognized hard work because short cuts rarely produce good results from data. And (s)he is open-minded and is excited to learn new things.\nI personally discovered machine learning 2 years ago, thanks to Andrew Y. Ng\u2019s Coursera course and Hastie et al\u2019s book titled \u201cThe Elements of Statistical Learning,\u201d but learned to really make sense from data when I was working for the insurance industry as an actuary and CFO, and in university when I studied statistics.\nMy wife (also an actuary) tells me I don't think like a normal person (usually after I've given her a long complicated answer to what she thinks is a 30 second question), but she thinks that's mainly because I'm French.\nQ: Why do you think your algorithm/predictive model was able to improve on aviation industry benchmarks?\nIt is certainly due to the fact that many industries work in isolation. Companies like Kaggle, with its large community of data scientists and I2R (my current workplace) are changing the game by bringing new solutions for those industries.\nQ: What was your process in developing Flight Quest algorithm/predictive model?\nThe algorithms we used are very standard for Kagglers. We used Gradient Boosting Machine and Random Forest, which have proved to work very well in other competitions too.\nWe spent most of our efforts in feature engineering. Our final feature selection is a collection of flight statistics and attributes, weather information during the flights, traffic in airports and weather conditions at arrival. We were also very careful to discard features likely to expose us to the risk of over-fitting our model.\nQ: Based on the data you were given, what challenges did you encounter when developing your model? Was there anything outside of the data you had to consider?\nUnlike the usual competitions, we did not have standard structured data that we could use to produce a quick first solution. We spent a tremendous time \u00a0exploring the numerous datasets, visualizing the data, understanding which data could bring value, and elaborating a strategy to convert this insight in usable features before producing a first model.\nQ: What was the most challenging part of this data quest?\nThe timeline of the competition was our biggest challenge. The most critical deadline of the competition was just a few days after Chinese New Year. Chinese New Year is a 4-day period during which you are supposed to spend time with your family, not with data and algorithms!\nQ: What is your definition of a data scientist? What impact will data science and data scientists have on the aviation industry?\nI will consider myself a fully qualified data scientist when I am able to build a one-stop solution that produces high accuracy for very large data sets.\nProliferation of the use of sensor networks and low-cost communications generate large volumes of operational data in the aviation and other industries. This opens up tremendous opportunities for data scientists to contribute in various aspects. Our department is already working with aircraft manufacturers and suppliers to apply data science to the areas of manufacturing equipment health monitoring, fuselage integrity monitoring and engine airflow optimization.\n"}, {"url": "http://blog.kaggle.com/2013/02/25/5-lessons-learned-for-the-event-recommendation-challenge/", "link_text": "5 Lessons Learned from the Event Recommendation Challenge", "id": 9, "title": "Permalink to 5 Lessons Learned from the Event Recommendation Challenge", "date": "2013-02-25", "text": "\nCross-posted from rouli.net.\nNothing could have prepared me for the\u00a0pleasant\u00a0surprise awaiting me last Thursday's morning. Just a few hours earlier I've submitted my final predictions on\u00a0Kaggle's Event Recommendation Engine Challenge\u00a0(in short - the goal is to predict which events will be interesting to a given user). The public leader board was frozen a week earlier, and at the time I was at the 11th place, after falling six places in a couple of days. Moreover, during the board-freeze session, I didn't find any feature or modeling technique that improved significantly my score on my own validation set. I was hoping to be in the top 10%, but woke up to find that I won the third place, reaching my best ranking ever.\nI strive to learn from each competition I participate in, and this one is no different. However, my take-aways from this challenge don't involve a new\u00a0algorithm\u00a0or feature selection insights. Rather, they are lessons about handling data on Kaggle challenges and in real life :). But first, the boring stuff:\n\nI used pandas, which I really wanted to try out (great, but I'm missing some of R's data exploration methods), scikit-learn and\u00a0iPython notebook (fantastic!).\nI've treated the challenge as a classification problem, and ranked the events by their predicted probability to be classified as interesting.\nMy model is basically an average between a Random Forest and a Gradient Bosting Classifier, with a sprinkle of Naive Bayes on the events' descriptions.\nThe top feature for me (as judged by the random forest), was the time between when the user was presented with the event and the event's date, or delta for short. Next come the ratio of invited users that decided not to attend an event (surprisingly, the higher the ratio, the more interesting is the event), \u00a0the total number of attendees, and the estimated distance between the user and the event (more on this later).\n\n\nLesson One: Over Fitting is a Bitch\nAs you can see in the graph below, the public score was a very bad predictor for the private (and final) score.\n\n\n\nLuckily, I've trusted more my five-fold validation averaged score than the scores I got on the public leader board. But boy, was that frustrating. A few times during the competition I've submitted my predictions after fixing some bug or improving the reliability of some feature just to get a lower public score. Even worst, in the last couple of days before the board freeze, I was unable to improve my public score and was watching new-comers getting far better scores than mine. I was exiled from the warm comfort of the fourth place to the 11th place. It wasn't easy to ignore the public score and not to optimize against it, but luckily I did so.\n\n\n\nLesson Two: Don't Believe Random Forest's Hype\n\nIt was my first time successfully employing a random forest classifier as the main predictive mode (usually, logistic regression works better for me), and I believed all the hype about random forests being better at avoiding over fitting. However, the telltale this isn't the case here was observing that adding features to the model sometimes decreased my validation score.\nI've combated that behavior by limiting the trees in the random forest to a certain maximum depth and a minimum number of samples at each leaf, and averaging with the GB classifier (which I haven't tweaked).\nKnowing what I know now, I would have also dropped some features that I've added to the model just because they seemed relevant and I believed\u00a0Overkill Analytics' approach of adding as many features as possible to a random forest, and let it sort them out.\n\n\n\nLesson Three: There's Always Some Data Leakage\n\n\nMidway through the competition I've discovered a feature with a very strong predictive power. Turns out that in many cases you could guess whether a user in the train set is interested in a specific event just by looking at the timestamp when he observed that event. If a given user observed several events at timestamp X, and another one at timestamp Y>X (even if those are just a few seconds apart), that other \"later\" event was probably marked as interesting.\nObviously this was some sort of a bug in the train set, and once I've proved that it was exploitable by reaching the third spot, I've (foolishly, since I didn't get any \"finders fee\" :)) alerted Kaggle about it. [Kaggle note: \u00a0Much appreciated!!] Fortunately, I got back to third place without such dirty tricks.\nHowever, in the way the train and test sets were assembled, there\u00a0was some leakage that could (and should) be exploited. First, for each user we had a list of about six events, from which exactly one was marked as interesting. That is, we are given a lot of information in comparison of the classical classification problem. I've exploited that by having a feature that compared the delta of each event displayed to a user against the delta of the earliest event. This proved to have more predictive power than the number of friends a user had in the event.\nMoreover, we can assume that startup behind the competition already propose only relevant events to its users. For example, users from Indonesia were rarely presented with events happening in the US. They probably do this by examining IP address which they chose not to share in the train set.\nThis led to my first attempt and still most successful at creating a \"distance between user and event\" feature; I simply calculated the median of the locations of the events presented to the user, and for each event calculated its distance from that median, as though the median was a substitute to the user's location. This worked better than deriving the user coordinates by looking at events\u00a0occurring\u00a0at the same city as the one found in the user's profile, and averaging between them.\n\n\nLesson Four: Always (Always!) Use Random Seeds\nSome of my features are derived from graphs, such that a pagerank score for events according to their\u00a0attendance\u00a0graph (where events share an edge with a weight that depends on the Jaccard\u00a0similarity\u00a0between the users that attended them), or, like many other did, events and users clusters. Sadly, because my hardware isn't powerful enough, I had to prune the graphs and I did so by deleting edges at random.\nBig mistake. Now I cannot recreate my final submission, and after all the work I put into it, I may not see the prize money. I was smart enough to set a random seed for my random forest, but too lazy or stupid to that when creating the graph. Gah!\n\nLesson Five: Things that didn't Work\nI've tried a lot of things during this competition, most of them didn't work, or at least I couldn't prove them to be working. However, they are good things to consider in future competitions:\n\n\nI've tried using\u00a0ELO rating\u00a0as an extra event-rating technique over the classification ones. It showed some (a lot!) of promise in the public test set, but failed in my 5 fold validation sets.\nI've tried calculating the \"distance\" between a user and an event by considering a graph with edges between users according to their friendship status, and between users and events according to their\u00a0attendance. This was a complete failure.\nNaive Bayes (or more precisely, Multinomial Naive Bayes) on the event's descriptions was very disappointing. I then splited the users according to geographies and had a different NB classifier for each, which made this technique only slightly disappointing.\nPage-ranks of events (and even more so, page-ranks of the events organizers) didn't contribute much.\nI've tried filling up missing users and events locations by using an iterative approach; I guessed the location of each user by averaging the\u00a0coordinates\u00a0of the events \u00a0she attended, and the location of each event by averaging the coordinates of the users who attended it. I did this repeatedly until I failed to discover the locations of any new events or users. This actually made my performance worse.\n\nEpilogue\n\nWow, did you really read this very long blog post? If so, you may be interested in following me on\u00a0Twitter. Hope you enjoyed it, and feel free to comment, especially if you notice any grammatical or spelling mistakes :).\n"}, {"url": "http://blog.kaggle.com/2013/01/17/getting-started-with-pandas-predicting-sat-scores-for-new-york-city-schools/", "link_text": "Getting Started with Pandas - Predicting SAT Scores for New York City Schools", "id": 10, "title": "Permalink to Getting Started with Pandas - Predicting SAT Scores for New York City Schools", "date": "2013-01-17", "text": "\nCrossposted from blog.untrod.com\nAn Introduction to Pandas\nThis tutorial will get you started with Pandas - a data analysis library for Python that is great for data preparation, joining, and ultimately generating well-formed, tabular data that's easy to use in a variety of visualization tools or (as we will see here) machine learning applications. This tutorial assumes a solid understanding of core Python functionality, but nothing about machine learning or Pandas.\nGoals\n\nUsing data from NYC Open Data, build a unified, tabular dataset ready for use with machine learning algorithms to predict student SAT scores on a per school basis.\nLearn and use the Pandas data analysis package.\nLearn how data is typically prepared for machine learning algorithms (ingestion, cleaning, joining, feature generation).\n\nFirst, let's ingest the data and get the lay of the land. You can download the data sets referenced below from\u00a0NYC Open Data, or directly download a\u00a0zip file\u00a0with the relevant data.\nIn\u00a0[8]:\nimport pandas as pd\r\n# Load the data\r\ndsProgReports = pd.read_csv('C:/data/NYC_Schools/School_Progress_Reports_-_All_Schools_-_2009-10.csv')\r\ndsDistrict = pd.read_csv('C:/data/NYC_Schools/School_District_Breakdowns.csv')\r\ndsClassSize = pd.read_csv('C:/data/NYC_Schools/2009-10_Class_Size_-_School-level_Detail.csv')\r\ndsAttendEnroll = pd.read_csv('C:/data/NYC_Schools/School_Attendance_and_Enrollment_Statistics_by_District__2010-11_.csv')[:-2] #last two rows are bad\r\ndsSATs = pd.read_csv('C:/data/NYC_Schools/SAT__College_Board__2010_School_Level_Results.csv') # Dependent\r\n\nIn\u00a0[9]:\ndsSATs\nOut[9]:\nInt64Index: 460 entries, 0 to 459\r\nData columns:\r\nDBN                      460  non-null values\r\nSchool Name              460  non-null values\r\nNumber of Test Takers    460  non-null values\r\nCritical Reading Mean    460  non-null values\r\nMathematics Mean         460  non-null values\r\nWriting Mean             460  non-null values\r\ndtypes: object(6)\nOutline\nPandas has read the data files without issue. Next let's create a rough map of where we are going.\nWe have five datasets here, each with information about either schools or districts. We're going to need to join all of this information together into a tabular file, with one row for each school, joined with as much information we can gather about that school & its district, including our dependent variables, which will be the mean SAT scores for each school in 2010.\nDrilling down one level of detail, let's look at the dataset dsSATs, which contains the target variables:\nIn [10]:\ndsSATs\nOut[10]:\nInt64Index: 460 entries, 0 to 459\r\nData columns:\r\nDBN                      460  non-null values\r\nSchool Name              460  non-null values\r\nNumber of Test Takers    460  non-null values\r\nCritical Reading Mean    460  non-null values\r\nMathematics Mean         460  non-null values\r\nWriting Mean             460  non-null values\r\ndtypes: object(6)\nTarget Variable and Joining Strategy\nWe are going to build a dataset to predict Critical Reading Mean, Mathematics Mean, and Writing Mean for each school (identified by DBN).\nAfter digging around in Excel (or just taking my word for it) we identify the following join strategy (using SQL-esque pseudocode):\ndsSATS join dsClassSize on dsSATs['DBN'] = dsClassSize['SCHOOL CODE']\r\njoin dsProgReports on dsSATs['DBN'] = dsProgReports['DBN']\r\njoin dsDistrct on dsProgReports['DISTRICT'] = dsDistrict['JURISDICTION NAME']\r\njoin dsAttendEnroll on dsProgReports['DISTRICT'] = dsAttendEnroll['District']\nNow that we have the strategy identified at a high level, there are a number of details we have to identify and take care of first.\n\nPrimary Keys - Schools\nBefore we can join these three datasets together, we need to normalize their primary keys. Below we see the mismatch between the way the DBN (school id) field is represented in the different datasets. We then write code to normalize the keys and correct this problem.\nIn\u00a0[11]:\npd.DataFrame(data=[dsProgReports['DBN'].take(range(5)), dsSATs['DBN'].take(range(5)), dsClassSize['SCHOOL CODE'].take(range(5))])\nOut[11]:\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nDBN\n01M015\n01M019\n01M020\n01M034\n01M063\n\n\nDBN\n01M292\n01M448\n01M450\n01M458\n01M509\n\n\nSCHOOL CODE\nM015\nM015\nM015\nM015\nM015\n\n\n\nIn\u00a0[12]:\n#Strip the first two characters off the DBNs so we can join to School Code\r\ndsProgReports.DBN = dsProgReports.DBN.map(lambda x: x[2:])\r\ndsSATs.DBN = dsSATs.DBN.map(lambda x: x[2:])\r\n\r\n#We can now see the keys match\r\npd.DataFrame(data=[dsProgReports['DBN'].take(range(5)), dsSATs['DBN'].take(range(5)), dsClassSize['SCHOOL CODE'].take(range(5))])\r\n\nOut [12]:\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nDBN\nM015\nM019\nM020\nM034\nM063\n\n\nDBN\nM292\nM448\nM450\nM458\nM509\n\n\nSCHOOL CODE\nM015\nM015\nM015\nM015\nM015\n\n\n\nPrimary Keys - Districts\nWe have a similar story with the district foreign keys. Again, we need to normalize the keys. The only additional complexity here is that dsProgReports['DISTRICT'] is typed numerically, whereas the other two district keys are typed as string. We do some type conversions following the key munging.\nIn\u00a0[13]:\n#Show the key mismatchs\r\n#For variety's sake, using slicing ([:3]) syntax instead of .take()\r\npd.DataFrame(data=[dsProgReports['DISTRICT'][:3], dsDistrict['JURISDICTION NAME'][:3], dsAttendEnroll['District'][:3]])\nOut[13]:\n\n\n\n\n0\n1\n2\n\n\n\n\nDISTRICT\n1\n1\n1\n\n\nJURISDICTION NAME\nCSD 01 Manhattan\nCSD 02 Manhattan\nCSD 03 Manhattan\n\n\nDistrict\nDISTRICT 01\nDISTRICT 02\nDISTRICT 03\n\n\n\nIn [14]:\n#Extract well-formed district key values\r\n#Note the astype(int) at the end of these lines to coerce the column to a numeric type\r\nimport re\r\ndsDistrict['JURISDICTION NAME'] = dsDistrict['JURISDICTION NAME'].map(lambda x: re.match( r'([A-Za-z]*\\s)([0-9]*)', x).group(2)).astype(int)\r\ndsAttendEnroll.District = dsAttendEnroll.District.map(lambda x: x[-2:]).astype(int)\r\n\r\n#We can now see the keys match\r\npd.DataFrame(data=[dsProgReports['DISTRICT'][:3], dsDistrict['JURISDICTION NAME'][:3], dsAttendEnroll['District'][:3]])\r\nOut[14]:\r\n0\t1\t2\r\nDISTRICT\t1\t1\t1\r\nJURISDICTION NAME\t1\t2\t3\r\nDistrict\t1\t2\t3\r\nAdditional Cleanup\nAt this point we could do the joins, but there is messiness in the data still. First let's reindex the DataFrames so the semantics come out a bit cleaner. Pandas indexing is beyond the scope of this tutorial but suffice it to say it makes these operations easier.\nIn [15]:\n#Reindexing\r\ndsProgReports = dsProgReports.set_index('DBN')\r\ndsDistrict = dsDistrict.set_index('JURISDICTION NAME')\r\ndsClassSize = dsClassSize.set_index('SCHOOL CODE')\r\ndsAttendEnroll = dsAttendEnroll.set_index('District')\r\ndsSATs = dsSATs.set_index('DBN')\nLet's take a look at one of our target variables. Right away we see the \"s\" value, which shouldn't be there.\nWe'll filter out the rows without data.\nIn [16]:\n#We can see the bad value\r\ndsSATs['Critical Reading Mean'].take(range(5))\nOut[16]:\nDBN\r\nM292    391\r\nM448    394\r\nM450    418\r\nM458    385\r\nM509      s\r\nName: Critical Reading Mean\nIn [17]:\n#Now we filter it out\r\n\r\n#We create a boolean vector mask. Open question as to whether this semantically ideal...\r\nmask = dsSATs['Number of Test Takers'].map(lambda x: x != 's')\r\ndsSATs = dsSATs[mask]\r\n#Cast fields to integers. Ideally we should not need to be this explicit.\r\ndsSATs['Number of Test Takers'] = dsSATs['Number of Test Takers'].astype(int)\r\ndsSATs['Critical Reading Mean'] = dsSATs['Critical Reading Mean'].astype(int)\r\ndsSATs['Mathematics Mean'] = dsSATs['Mathematics Mean'].astype(int)\r\ndsSATs['Writing Mean'] = dsSATs['Writing Mean'].astype(int)\r\n\r\n#We can see those values are gone\r\ndsSATs['Critical Reading Mean'].take(range(5))\nOut[17]:\nDBN\r\nM292    391\r\nM448    394\r\nM450    418\r\nM458    385\r\nM515    314\r\nName: Critical Reading Mean\r\nFeature Construction\ndsClassSize will be a many-to-one join with dsSATs because dsClassSize contains multiple entries per school. We need to summarize and build features from this data in order to get one row per school that will join neatly to dsSATs.\nAdditionally, the data has an irregular format, consisting of a number of rows per school describing different class sizes, then a final row for that school which contains no data except for a number in the final column, SCHOOLWIDE PUPIL-TEACHER RATIO.\nWe need to extract the SCHOOLWIDE PUPIL-TEACHER RATIO rows, at which point we'll have a regular format and can build features via aggregate functions. We'll also drop any features that can't be easily summarized or aggregated and likely have no bearing on the SAT scores (like School Name).\nIn [18]:\n#The shape of the data\r\nprint dsClassSize.columns\r\nprint dsClassSize.take([0,1,10]).values\r\narray([BORO, CSD, SCHOOL NAME, GRADE , PROGRAM TYPE,\r\n       CORE SUBJECT (MS CORE and 9-12 ONLY),\r\n       CORE COURSE (MS CORE and 9-12 ONLY), SERVICE CATEGORY(K-9* ONLY),\r\n       NUMBER OF CLASSES, TOTAL REGISTER, AVERAGE CLASS SIZE,\r\n       SIZE OF SMALLEST CLASS, SIZE OF LARGEST CLASS, DATA SOURCE,\r\n       SCHOOLWIDE PUPIL-TEACHER RATIO], dtype=object)\r\n[[M 1 P.S. 015 ROBERTO CLEMENTE 0K GEN ED - - - 1.0 21.0 21.0 21.0 21.0 ATS\r\n  nan]\r\n [M 1 P.S. 015 ROBERTO CLEMENTE 0K CTT - - - 1.0 21.0 21.0 21.0 21.0 ATS\r\n  nan]\r\n [M 1 P.S. 015 ROBERTO CLEMENTE nan nan nan nan nan nan nan nan nan nan nan\r\n  8.9]]\nIn [19]:\n#Extracting the Pupil-Teacher Ratio\r\n\r\n#Take the column\r\ndsPupilTeacher = dsClassSize.filter(['SCHOOLWIDE PUPIL-TEACHER RATIO'])\r\n#And filter out blank rows\r\nmask = dsPupilTeacher['SCHOOLWIDE PUPIL-TEACHER RATIO'].map(lambda x: x > 0)\r\ndsPupilTeacher = dsPupilTeacher[mask]\r\n#Then drop from the original dataset\r\ndsClassSize = dsClassSize.drop('SCHOOLWIDE PUPIL-TEACHER RATIO', axis=1)\r\n\r\n#Drop non-numeric fields\r\ndsClassSize = dsClassSize.drop(['BORO','CSD','SCHOOL NAME','GRADE ','PROGRAM TYPE',\\\r\n'CORE SUBJECT (MS CORE and 9-12 ONLY)','CORE COURSE (MS CORE and 9-12 ONLY)',\\\r\n'SERVICE CATEGORY(K-9* ONLY)','DATA SOURCE'], axis=1)\r\n\r\n#Build features from dsClassSize\r\n#In this case, we'll take the max, min, and mean\r\n#Semantically equivalent to select min(*), max(*), mean(*) from dsClassSize group by SCHOOL NAME\r\n#Note that SCHOOL NAME is not referenced explicitly below because it is the index of the dataframe\r\ngrouped = dsClassSize.groupby(level=0)\r\ndsClassSize = grouped.aggregate(np.max).\\\r\n    join(grouped.aggregate(np.min), lsuffix=\".max\").\\\r\n    join(grouped.aggregate(np.mean), lsuffix=\".min\", rsuffix=\".mean\").\\\r\n    join(dsPupilTeacher)\r\n\r\nprint dsClassSize.columns\r\narray([NUMBER OF CLASSES.max, TOTAL REGISTER.max, AVERAGE CLASS SIZE.max,\r\n       SIZE OF SMALLEST CLASS.max, SIZE OF LARGEST CLASS.max,\r\n       NUMBER OF CLASSES.min, TOTAL REGISTER.min, AVERAGE CLASS SIZE.min,\r\n       SIZE OF SMALLEST CLASS.min, SIZE OF LARGEST CLASS.min,\r\n       NUMBER OF CLASSES.mean, TOTAL REGISTER.mean,\r\n       AVERAGE CLASS SIZE.mean, SIZE OF SMALLEST CLASS.mean,\r\n       SIZE OF LARGEST CLASS.mean, SCHOOLWIDE PUPIL-TEACHER RATIO], dtype=object)\nJoining\nOne final thing before we join - dsProgReports contains distinct rows for separate grade level blocks within one school. For instance one school (one DBN) might have two rows: one for middle school and one for high school. We'll just drop everything that isn't high school.\nAnd finally we can join our data. Note these are inner joins, so district data get joined to each school in that district.\nIn [20]:\nmask = dsProgReports['SCHOOL LEVEL*'].map(lambda x: x == 'High School')\r\ndsProgReports = dsProgReports[mask]\nIn [21]:\nfinal = dsSATs.join(dsClassSize).\\\r\njoin(dsProgReports).\\\r\nmerge(dsDistrict, left_on='DISTRICT', right_index=True).\\\r\nmerge(dsAttendEnroll, left_on='DISTRICT', right_index=True)\n(Even More) Additional Cleanup\nWe should be in a position to build a predictive model for our target variables right away but unfortunately there is still messy data floating around in the dataframe that machine learning algorithms will choke on. A pure feature matrix should have only numeric features, but we can see that isn't the case. However for many of these columns, the right approach is obvious once we've dug in.\nIn [22]:\nfinal.dtypes[final.dtypes.map(lambda x: x=='object')]\nOut[22]:\nSchool Name                      object\r\nSCHOOL                           object\r\nPRINCIPAL                        object\r\nPROGRESS REPORT TYPE             object\r\nSCHOOL LEVEL*                    object\r\n2009-2010 OVERALL GRADE          object\r\n2009-2010 ENVIRONMENT GRADE      object\r\n2009-2010 PERFORMANCE GRADE      object\r\n2009-2010 PROGRESS GRADE         object\r\n2008-09 PROGRESS REPORT GRADE    object\r\nYTD % Attendance (Avg)           object\nIn [23]:\n#Just drop string columns.\r\n#In theory we could build features out of some of these, but it is impractical here\r\nfinal = final.drop(['School Name','SCHOOL','PRINCIPAL','SCHOOL LEVEL*','PROGRESS REPORT TYPE'],axis=1)\r\n\r\n#Remove % signs and convert to float\r\nfinal['YTD % Attendance (Avg)'] = final['YTD % Attendance (Avg)'].map(lambda x: x.replace(\"%\",\"\")).astype(float)\r\n\r\n#The last few columns we still have to deal with\r\nfinal.dtypes[final.dtypes.map(lambda x: x=='object')]\nCategorical Variables\nWe can see above that the remaining non-numeric field are grades . Intuitively, they might be important so we don't want to drop them, but in order to get a pure feature matrix we need numeric values. The approach we'll use here is to explode these into multiple boolean columns. Some machine learning libraries effectively do this for you under the covers, but when the cardinality of the categorical variable is relatively low, it's nice to be explicit about it.\nIn [24]:\ngradeCols = ['2009-2010 OVERALL GRADE','2009-2010 ENVIRONMENT GRADE','2009-2010 PERFORMANCE GRADE','2009-2010 PROGRESS GRADE','2008-09 PROGRESS REPORT GRADE']\r\n\r\ngrades = np.unique(final[gradeCols].values) #[nan, A, B, C, D, F]\r\n\r\nfor c in gradeCols:\r\n    for g in grades:\r\n        final = final.join(pd.Series(data=final\r\n\r\n.map(lambda x: 1 if x is g else 0), name=c + \"_is_\" + str(g))) final = final.drop(gradeCols, axis=1) #Uncomment to generate csv files #final.drop(['Critical Reading Mean','Mathematics Mean','Writing Mean'],axis=1).to_csv('C:/data/NYC_Schools/train.csv') #final.filter(['Critical Reading Mean','Mathematics Mean','Writing Mean']).to_csv('C:/data/NYC_Schools/target.csv')\nThat's it!\nWe now have a feature matrix that's trivial to use with any number of machine learning algorithms. Feel free to stop here and run the two lines of code below to get nice CSV files written to disk that you can easily use in Excel, Tableau, etc. Or run the larger block of code to see how easy it is to build a random forest model against this data, and look at which variables are most important.\nIn [25]:\n#Uncomment to generate csv files\r\n#final.drop(['Critical Reading Mean','Mathematics Mean','Writing Mean'],axis=1).to_csv('C:/data/NYC_Schools/train.csv')\r\n#final.filter(['Critical Reading Mean','Mathematics Mean','Writing Mean']).to_csv('C:/data/NYC_Schools/target.csv')\nIn [26]:\nfrom sklearn.ensemble import RandomForestRegressor\r\n\r\ntarget = final.filter(['Critical Reading Mean'])\r\n#We drop all three dependent variables because we don't want them used when trying to make a prediction.\r\ntrain = final.drop(['Critical Reading Mean','Writing Mean','Mathematics Mean'],axis=1)\r\nmodel = RandomForestRegressor(n_estimators=100, n_jobs=-1, compute_importances = True)\r\nmodel.fit(train, target)\r\n\r\npredictions = np.array(model.predict(train))\r\nrmse = math.sqrt(np.mean((np.array(target.values) - predictions)**2))\r\nimp = sorted(zip(train.columns, model.feature_importances_), key=lambda tup: tup[1], reverse=True)\r\n\r\nprint \"RMSE: \" + str(rmse)\r\nprint \"10 Most Important Variables:\" + str(imp[:10])\r\nRMSE: 80.13105688\r\n10 Most Important Variables:[('PEER INDEX*', 0.81424747874371173), ('TOTAL REGISTER.min', 0.060086333792196724), ('2009-2010 ENVIRONMENT CATEGORY SCORE', 0.023810405565050669), ('2009-2010 ADDITIONAL CREDIT', 0.021788425210174274), ('2009-2010 OVERALL SCORE', 0.019046860376900468), ('AVERAGE CLASS SIZE.mean', 0.0094882658926829649), ('2009-2010 PROGRESS CATEGORY SCORE', 0.0094678349064146652), ('AVERAGE CLASS SIZE.min', 0.0063723026953534942), ('2009-2010 OVERALL GRADE_is_nan', 0.0057710237481254567), ('Number of Test Takers', 0.0053660239780210584)]\nphoto by Stefan on Flickr\n"}, {"url": "http://blog.kaggle.com/2013/01/14/webapps-for-data-scientists-building-your-first-crud/", "link_text": "Webapps for Data Scientists - Building your first CRUD", "id": 11, "title": "Permalink to Webapps for Data Scientists - Building your first CRUD", "date": "2013-01-14", "text": "\nUPDATE: \u00a0Part III of web tutorial now available\nThis is the first time I\u2019ve been excited about programming in years.\nThe first time the tools feel like they\u2019re doing what they ought to be doing.\nSince I started coding 30 years ago, now and again I find something transformative, something that makes programming fun and natural, and this is one of those times. I\u2019m referring to Google\u2019s AngularJS framework.\nAs the chief scientist of data science company, you might wonder why I am so excited to be writing about a webapp framework, as opposed to the hottest new machine learning library. \u00a0I feel that modern web frameworks have quietly taken functional programming research, brought it to the mainstream, and let you build something very complete and holistic around it. \u00a0Whether it's building an internal viewer app to let other people in your organization test out your model, or bringing a full data product to market (Kaggle Startup Program is still accepting applications) AngularJS is a great tool to have in your data scientist kit. \u00a0Why AngularJS in particular? Well, it has a really clever foundation around \u2018directives\u2019 - which allow you to extend HTML to work in entirely new ways - \u00a0and \u2018scope\u2019 - which allows you to tie your rich data structures directly to your HTML templates, using 2-way bindings. \u00a0It\u2019s expressive and very opinionated, but opinionated in all the right ways.\nThe only difficulty with learning AngularJS was that (until now) that wasn\u2019t really any complete end-to-end run-throughs of how to create a full working web application.\nTo help others get started, I\u2019ve put together a series of tutorials on how to build a working webapp up in less than one hour. \u00a0You can find the step-by step instructions and screenshots here on my blog. \u00a0I\u2019ve also made a series of videos from the Kaggle Lunch Talks I\u2019ve been giving on this subject. \u00a0You can find the first two in the series here and here, with more to come in the very near future. The tutorials have examples in both C# and Python, although they should be easy enough to translate to whatever language you prefer.\nSo, let\u2019s begin shall we...\n\nImage via Flikr, 'sand mandala detail' by\u00a0ginnerobot\n"}, {"url": "http://blog.kaggle.com/2012/12/19/1st-place-observing-dark-worlds/", "link_text": "1st Place: Observing Dark Worlds", "id": 12, "title": "Permalink to 1st Place: Observing Dark Worlds", "date": "2012-12-19", "text": "\nCross-posted from Tim Salimans on Data Analysis. \u00a0He'll\u00a0post the Matlab code for his solution sometime later this week\nKaggle recently ran another\u00a0great competition, which I was very fortunate to win. The goal of this competition: detect clouds of dark matter floating around the universe through their effect on the light emitted by background galaxies.\nFrom the competition website:\nThere is more to the Universe than meets the eye. Out in the cosmos exists a\u00a0form of matter that outnumbers the stuff we can see by almost 7 to 1, and we don\u2019t know what it is. What we do know is that it does not emit or absorb light, so we call it\u00a0Dark Matter.\u00a0Such a vast amount of aggregated matter does not go unnoticed. In fact we observe that this stuff aggregates and forms massive structures called\u00a0Dark Matter Halos.\u00a0Although dark, it warps and bends spacetime such that any light from a background galaxy which passes close to the\u00a0Dark Matter\u00a0will\u00a0have its path altered and changed.\u00a0This bending causes the galaxy to appear as an ellipse in the sky.\nThe task is then to use this \u201cbending of light\u201d to estimate where in the sky this dark matter is located.\nAlthough the description makes this sound like a physics problem, it is really one of statistics: given the noisy data (the elliptical galaxies) recover the model and parameters (position and mass of the dark matter) that generated them. After recovering these dark matter halos, their positions could then be uploaded to the Kaggle website where a complicated loss function was used to calculate the accuracy of our estimates.\nBayesian analysis provided the winning recipe for solving this problem:\n\nConstruct a prior distribution for the halo positions\u00a0, i.e. formulate our expectations about the halo positions before looking at the data.\nConstruct a probabilistic model for the data (observed ellipticities of the galaxies) given the positions of the dark matter halos:\u00a0.\nUse Bayes\u2019 rule to get the posterior distribution of the halo positions:\u00a0, i.e. use to the data to guess where the dark matter halos might be.\nMinimize the expected loss with respect to the posterior distribution over the predictions for the halo positions:\u00a0, i.e. tune our predictions to be as good as possible for the given error metric.\n\nFor step 1. I simply assumed that the dark matter halos were distributed uniformly at random across the sky. Step 2 is more complicated. Fortunately the competition organizers provided us with a set of training skies for which the positions of the dark matter halos was known, as well as a summary of the physics behind it all. After reading through the tutorials and forum posts it became clear that the following model should be reasonable:\n,\nwhere\u00a0\u00a0denotes the normal distribution,\u00a0\u00a0is the\u00a0tangential direction, i.e. the direction in which halo\u00a0\u00a0bends the light of galaxy\u00a0,\u00a0\u00a0is the mass of halo\u00a0, and\u00a0\u00a0is a decreasing function in the euclidean distance\u00a0\u00a0between galaxy\u00a0\u00a0and halo\u00a0.\nAfter looking at the data I fixed the variance of the Gaussian distribution\u00a0\u00a0at 0.05. Like most competitors I also noticed that all skies seemed to have a single large halo, and that the other halos were much smaller. For the large halo I assigned the halo mass\u00a0\u00a0a log-uniform distribution between 40 and 180, and I set\u00a0. For the small halos I fixed the mass at 20, and I used\u00a0. The resulting model is likely to be overly simplistic but it seems to capture most of the signal that is present in the data. In addition, keeping the model simple protected me against overfitting the data. Note that I assumed that the galaxy positions were independent of the halo positions, although it turns out this may not have been completely accurate.\nAfter completing step 1 and 2, step 3 and 4 are simply a matter of implementation: I choose to use a simple random-walk Metropolis Hastings sampler to approximate the posterior distribution in step 3. The optimization in step 4 was done using standard gradient-based optimization, with random restarts to avoid local minima.\nLike I remarked in the competition forums, the outcome of this competition was more noisy than is usual: final prediction accuracy was judged on a set of only 90 cases, with an evaluation metric that is very sensitive to small (angular) perturbations of the predictions. The public leaderboard standings were even more random, being based on only 30 cases. In fact, the 1.05 public score of my winning submission was only about average on the public leaderboard. All of this means I was very lucky indeed to win this competition. Nevertheless, the runner-up seems to have taken a\u00a0very similar approach, suggesting there is at least something to be said for looking at this kind of problem from a Bayesian perspective.\nFinally, I would like to thank the organizers Dave and Tom, and sponsor Winton Capital for organizing a great competition. Looking at a problem different from the standard regression/classification problems was very refreshing.\nPhoto Credit: Dark Matter Visualization for LBC Dataset,\u00a0NPACI Visualization Services -\u00a0Amit Chourasia,\u00a0Steve Cutchin. \u00a0San Diego Supercomputer Center, ENZO Early Universe Simulation\n\n"}, {"url": "http://blog.kaggle.com/2012/12/19/a-bayesian-approach-to-observing-dark-worlds/", "link_text": "A Bayesian approach to Observing Dark Worlds", "id": 13, "title": "Permalink to A Bayesian approach to Observing Dark Worlds", "date": "2012-12-19", "text": "\nCross-posted from Iain Murray's homepage. He's also sharing his code.\nIn December 2012 I entered the\u00a0Kaggle/Winton Observing Dark Worlds\u00a0competition. My\u00a0predictions placed 2nd out of 357 teams. This short note describes my approach.\nModeling the data, and making predictions\nI took the \u201cobvious\u201d Bayesian approach to this challenge, although didn\u2019t implement it quite as carefully as possible. This style of solution involves three steps: 1)\u00a0Build a probabilistic model of the data; 2)\u00a0Infer underlying explanations of the test data; 3)\u00a0Fit the \u2018best\u2019 estimates that minimize the expected error (measured by the metric used on the leaderboard).\n1) The model:\u00a0The observed data were positions of galaxies and their measured ellipticities. The ellipticities were clearly Gaussian-distributed. The dark matter halos exert a \u201cforce\u201d on the galaxies, locally shifting the mean of the distribution over ellipticities. The example maximum likelihood code, provided by the organizers, gave me the equations to describe how the ellipticities are shifted by a force. That only left me to model the force as a function of displacement from a halo center.\nBased on a quick visualization, I picked a force term of\u00a0m/max(r,r0), where\u00a0m\u00a0is proportional to the halo mass,\u00a0r\u00a0is the radial distance from the halo center, and\u00a0r0\u00a0is a core radius inside which the force doesn\u2019t increase. I doubted that this model was quite right, so I also increased the variance of the ellipticities inside the halo core, hoping to increase the robustness of the model.\n(I was a little negligent: I should have considered more flexible models of the forces, with more free parameters, and learned whether and how to use them from the data. I didn\u2019t make time to do that, or even test properly whether increasing the within-core variance was a good idea. I\u2019d be more careful in a real collaboration.)\n2) Inference:\u00a0Given the model, I applied a standard method to simulate plausible explanations of the test skies. I used Slice Sampling, an easy-to-use Markov chain Monte Carlo (MCMC) method, and didn\u2019t have to tune any tweak parameters. My results on the training skies seemed to be similar if I initialized the dark matter halos at their true locations, or randomly, so I assumed that the sampler was working well enough for the competition. The inference stage was by far the easiest: very little coding or computer time, and no tweaking required.\nIf you would like to try out slice sampling, you could play with my\u00a0Octave/Matlab-based MCMC tutorial exercise. The\u00a0original paper (Neal, 2003)\u00a0is excellent, or for a shorter introduction, see Chapter\u00a029 ofDavid MacKay's book.\nSampling gives me a bag of samples: a set of plausible dark matter halo positions and masses. If I showed you a movie, stepping through the different plausible halo locations, you would probably understand roughly what we can and can\u2019t know. You might say something like \u201cI can see that there\u2019s definitely a halo in the top right, but the other one could be nearly anywhere\u201d. Unfortunately I\u2019m not able to cram a bag of samples, or your verbal explanation, into the\u00a0.csv\u00a0submission format defined by the competition rules. There are a plethora of possible ways to make a single guess, including: picking a random sample, using the most probable sample, or using the mean of the samples. But these methods shouldn\u2019t be used without justification.\n3) Making a leaderboard submission:\u00a0A sensible way to make a guess, when forced to, is to minimize the expected size of your mistakes. In this competition, the size of a mistake was measured in a complicated way: the \u201cDark Worlds Metric\u201d. I\u2019d like to say that I minimized the expected value of this metric, but it was hard to deal with. The best prediction for a sky depends on the mistakes made in other skies, and I didn\u2019t know which skies would be in the final test set. Rather than considering possible test set splits (and possibly inferring the split!), I used a pragmatic hack. I optimized the prediction for each sky using the Dark Worlds Metric applied to a fictitious test set containing multiple copies of the same sky with different halo locations given by the MCMC samples. I hoped that this procedure was close enough to the ideal procedure that I would do well.\nDiscussion\nGiven the setup of this competition, I doubt that it is possible to do much better than the style of approach described here. Tim Salimans, who won, followed\u00a0a very similar approach. He might have solved the optimization problem better (I should have used gradient-based optimization, but didn\u2019t). Tim fixed his\u00a0r0\u00a0parameters, based on the training data, whereas I left them uncertain. I didn\u2019t work hard enough to notice that these parameters were always close to one of two fixed values (an artifact of the competition if true). Hierarchical learning of the distribution of such tweak parameters is important in real problems, and would be the way to go in future.\nIf the organizers ran MCMC on the model that they actually used to generate the data, that would find the best reasonable performance: what could be achieved without getting lucky, or fitting to the test data with multiple submissions. I have\u00a0described in a separate Forum posthow it would also be possible for the organizers to tell whether entrants to the competition were just lucky, by testing their expected performance over all reasonable explanations of the test set, rather than just one.\nI know from the\u00a0Forum\u00a0that some of the competitors near the top of the leaderboard didn't use Bayesian approaches: there are other ways to get good predictions, and my approaches to machine learning are not always Bayesian either. However, in this problem, the Bayesian approach was natural and straightforward. Unlike some methods discussed on the Forum, no careful tuning, special consideration of edge effects, or choosing of grids was required. The result of inference (given enough samples) is a complete description of what is known about the sky. The resulting samples aren\u2019t tied to one evaluation metric, but can be used to find an estimate that will work well under any.\nThanks very much to the organizers: Tom Kitching, David Harvey, Kaggle, and Winton for a fun competition. Congratulations\u00a0to Tim Salimans for coming first!\nI will release my code. Check back later if you want it. Email me if it isn't here by January 2013.\n\u00a0\nCode is now available here.\nHeader Image: \u00a0Iain's 'Research in a Nutshell' video\n"}, {"url": "http://blog.kaggle.com/2012/11/05/team-takes-3rd-in-the-merck-challenge/", "link_text": "Team '.' takes 3rd in the Merck Challenge", "id": 14, "title": "Permalink to Team '.' takes 3rd in the Merck Challenge", "date": "2012-11-05", "text": "\nSo, what's with the punctuation mark for a team name?\nEu Jin Lok: Apologies for the team name, I know it\u2019s annoying. If you were wondering, I chose it for its functionality: (1) It\u2019s hard for people to notice; (2) It\u2019s hard for people to click (if they want to find out our names).\nWhat was your background prior to entering this challenge?\nZach Mayer: I've got an undergraduate degree in biology, and a professional background in applied statistics and predictive modeling. \u00a0I currently work for management consulting firm AlixPartners.\nEu Jin Lok: I majored in Marketing and Econometrics in University, and like Zach, I'm currently working for Deloitte in the data analytics unit as a senior consultant.\nAlexander Larko: \u00a0I have a Master's degree in computer science - from South Russian State Technical University (Novocherkassk Polytechnic Institute). I started my career as an engineer with Scientific Research Institute of the city of Donetsk and worked there for three years. After that, I left to join a manufacturing firm and spent the next 25 years of my career as researcher, IT - engineer and senior manager for the firm. Now, I'm working for a small IT company as a technical director\nWhat made you decide to enter?\nAL: I liked the challenge, because it's an interesting set of data.\nZM: Well it seemed like an easy regression problem at first, at least until we realized how much the size of the data sets varied. \u00a0Self-evaluation on this problem was especially challenging.\nEJ: Zach and I have been working together for a while on the HHP contest. On day, Zach mentioned the competition to me in passing so I thought why not give it a go. its a pretty interesting problem to solve...and also not to mention the prize was attractive.\nHow did you guys meet each other?\nZM: Eu Jin and I met when we started collaborating on the Heritage prize, and we liked working together, so we entered a couple other competitions.\nHow did you decide to start working together on the Merck comp?\nEJ:\u00a0One day, all of a sudden, there was a raft of new competitions on Kaggle and they were all really interesting, but deadline is so close together, like the MERCK and US Census. So I decided that the best strategy is to work with Zach whom I'm already working with on the HHP contest. We entered the MERCK and US Census competition together, doing a tag team, swamping and changing as we get new ideas. Half way through the contest, our work/career ate into our Kaggle time, so I invited Alex to join us as I've seen him competing for 2 years now and thought it would be nice to work together with him.\nWhat preprocessing and supervised learning methods did you use?\nEJ: I used SVD to reduce features which was used as training data. For models, I tried everything from GBMs to PLS but it came down to just SVM and Random Forest.\nAL: The key success factor was selecting the significant variables, and for this I used a gradient increase as a feature selector. I used SVMs, gradient increases and neural networks to build several models which was subsequently put together to create the final submission.\nZM: I created a glmnet model that used sparse matrix representations of each data set. Unfortunately, my approach did not crystalise to a strong solution. So, for the rest of the time, I helped Eu Jin with SVD and PCA when his laptop ran out of RAM!\nWhat was your most important insight into the data?\nZM: Alex discovered that a GBM run on a sample of the data could be used to select features and greatly speed up the full model.\nEJ:I was surprised by Alex's approach wherein he ran a GBM on a sample of the data which he then used to select features. Did not expect that to work well but it did so that was an insight for me.\nAL: Yea me too! But also the temporal effect of the dataset, which was prevalent in the activity of the molecules.\nWere you surprised by any of your insights?\nZM: Not particularly. \u00a0A big portion of this competition was the technical challenge of pre-processing and modeling on large data sets.\nAL:\u00a0I was surprised by the coincidence of errors in different parts of the test suite (public error, private error).\nEJ: I didn't really have any insights, couldn't be more surprised.....\nWhich tools did you use?\nTogether: R.\nAL: And open office too.\nEJ: And excel too, for graphs.\nWhat have you taken away from this competition?\nEJ: Alot of what we have learnt on the MERCK contest, I will take it to the US Census and the HHP. On a serious note, try everything and don't give up. Every tiny effort you put in will bring you closer to the top.\nZM: RAM is cheap and you should have a lot on your prototyping machine! \u00a0I personally couldn't afford to keep an m2.4xlarge EC2 instance running for a month or 2...\nAL: The benefits of multiple approaches.\n"}, {"url": "http://blog.kaggle.com/2012/11/04/team-datarobot-merck-2nd-place-interview/", "link_text": "Team DataRobot: Merck 2nd place Interview", "id": 15, "title": "Permalink to Team DataRobot: Merck 2nd place Interview", "date": "2012-11-04", "text": "\nTeam DataRobot explains how to take on the Merck Molecular Activity Challenge using smoke alarms and airplanes.\nWhat was your background prior to entering this challenge?\nXavier: I run a consultancy Gear Analytics specialized in predictive analytics in Singapore. Previously, I worked in France, Brazil, China and Singapore holding different roles (actuary, CFO, risk manager) in the life and non-life insurance industry.\nJeremy and Tom: We met while we were both studying Math and Physics at the University of Massachusetts at Lowell and have been friends and colleagues ever since. We both have 7+ years experience doing applied predictive analytics. Most recently we were both Directors of Research and Modeling at Travelers Insurance (Tom on the Business Insurance side and Jeremy on the Personal Insurance side). Earlier this year, we decided to quit our jobs to start our own Data Science company (DataRobot). In our previous three kaggle competitions, we placed 3rd (private), 1st (Bio) and 4th (Diabetes).\nWhat made you decide to enter?\nXavier: I was looking for a competition to team up with my buddies Tom and Jeremy, who I met through Kaggle. We previously teamed up with Sergey Yurgenson for the \"Practice Fusion Diabetes Classification\" competition. We got good results but failed to finish in the top 3 (4th place). We also knew that we had good chance to win the Merck competition as we did quite well for the \"Biological Response\" (1st and 5th place).\nJeremy and Tom: We were looking for a competition to team up with our buddy Xavier, who we met through Kaggle, and we thought we\u2019d be able to leverage what we had learned during the BioResponse competition which we placed 1st in. Also, because we quit our jobs earlier this year, we were hoping to place in the top 3 and win some money to pay for a few more months of ramen noodles and canned tuna fish. If we placed 1st or 2nd we thought we might even be able to turn our internet back on--if our neighbor changes his wifi password on us again, we are screwed.\nWhat preprocessing and supervised learning methods did you use?\nMethodologies explored for various roles include Random Forests, PCA, GBM, KNN, Neural Nets, Elastic Net, GAM, and SVM.\nWhat was your most important insight into the data?\nFor most problems, GBM and SVM had similar predictive power and produced similar predictions. However, for problem 5, SVM's predictions deviated significantly from GBM's predictions and scored badly on the public leaderboard. This confirmed the importance of having at least 2 very different models in one's toolkit. We also improved the accuracy slightly by capping the predictions of a few problems.\nWere you surprised by any of your insights?\nThe most surprising thing was that almost all attempts to use subject matter knowledge or insights drawn from data visualization led to drastically worse results. We actually arranged a 2 hour whiteboard lecture from a very talented biochemist and came up with some ideas based on what we learned, but none of them worked out. Also, the visualizations that were shared as part of the visualization part of the competition were incredible (thanks to all who contributed!). We drew much insight from them which led us to try some new approaches that we were absolutely sure would work. However, most of these approaches failed to improve results, and many of them drastically decreased our public leaderboard scores. The visualizations did make us take a second look at capping which helped a bit.\nWe were also surprised to see how well our internal CV scores correlated with the public (and private) leaderboard scores. This was unexpected because of all the evidence suggesting the test sets were very different from the training sets for some problems. Only for problem 4 did the public leaderboard give us faulty feedback, but since problem 4 was so small, we knew to include a submission that ignored the public leaderboard feedback and included our best CV model for problem 4.\nWhich tools did you use?\nWe used R, Python, and a lot of computing power. We really didn\u2019t start working on the problem until around 2 weeks before the deadline, so we had to cram lots of cpu cycles into a short amount of time. \u00a0We used our 9 Ubuntu servers, Amazon, plus Xaviers magic macbook which he somehow gets to perform like it is a 32 core machine with 256GB of RAM. I (Jeremy) was sure the thing would combust at any moment, so I made sure all the smoke alarms in the house had fresh batteries.\nWe also used airplanes--Xavier came to the US and worked with us in person for 4 days which allowed us to get a good head start on the problem and make a solid two week plan (which we stuck to for the most part).\nWhat have you taken away from this competition?\nWe have some new techniques we need to learn if we want to compete for first place in future competitions.\nKaggle is a great place to meet people with the same interest and great results can come from this: friendship + powerful models!\n\n\n"}, {"url": "http://blog.kaggle.com/2012/11/02/t-distributed-stochastic-neighbor-embedding-wins-merck-viz-challenge/", "link_text": "t-Distributed Stochastic Neighbor Embedding Wins Merck Viz Challenge", "id": 16, "title": "Permalink to t-Distributed Stochastic Neighbor Embedding Wins Merck Viz Challenge", "date": "2012-11-02", "text": "\nWe spoke with the\u00a0Merck Visualization Challenge\u00a0winner about his technique. \u00a0All algorithms and visualizations were produced using Matlab R2011a. Implementations of t-SNE (in Matlab, Python, R, and C) are available from\u00a0the t-SNE website.\nWhat was your background prior to entering this challenge?\nI am a post-doctoral researcher at Delft University of Technology (The Netherlands), working on various topics in machine learning and computer vision. In particular, I focus on developing new techniques for dimensionality reduction, embedding, structured prediction, regularization, face recognition, and object tracking.\nWhat made you decide to enter?\nI entered the visualization challenge to test the effectiveness of an embedding technique, called t-Distributed Stochastic Neighbor Embedding (t-SNE), that Geoffrey Hinton and I developed a few years ago (building on earlier work by Geoffrey Hinton and Sam Roweis).\nWhat preprocessing or data munging methods did you use?\n\nThe main ingredient of my visualization approach is formed by t-SNE (L.J.P. van der Maaten and G.E. Hinton. Visualizing Data using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008). T-SNE represents each object by a point in a two-dimensional scatter plot, and arranges the points in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points. When you construct such a map using t-SNE, you typically get much better results than when you construct the map using something like principal components analysis or classical multidimensional scaling, because (1) t-SNE mainly focuses on appropriately modeling small pairwise distances, i.e. local structure, in the map and (2) because t-SNE has a way to correct for the enormous difference in volume of a high-dimensional feature space and a two-dimensional map. As a result of these two characteristics, t-SNE generally produces maps that provide much clearer insight into the underlying (cluster) structure of the data than alternative techniques.\nTo produce the visualizations I submitted to the challenge, I ran t-SNE on the raw data and I plotted the resulting two-dimensional map as a scatter plot, coloring the points according to either their index in the data set or according to their activity value. The first coloring provides insight into how the data distributions changes over time (using index as a surrogate for time), whereas the second coloring provides insight into how well the activity values may be predicted from the raw data. I also constructed the similar plots in which I also included the test data in the t-SNE analysis and colored the test points in a neutral gray color, to obtain insight in the difference between the training and the test distributions.\nWhat was your most important insight into the data?\n\nOne of the key insights that my visualizations give into the data distribution is that it changes enormously over time. When coloring the points according to their index, for many data sets, distinct colored clusters can be identified that suggest the data comprises batches of very different measurements. Maps that include the test data (depicted in a neutral gray color) reveal that the test distribution is very different from the training distribution for many data sets. I confirmed this finding using a simple experiment: I trained logistic regressors to discriminate the training from the test data (as is often done in importance-weighting approaches to covariate shift), and found that these logistic regressor have zero error for almost all data sets. This suggests the support of the training and test distribution are almost completely disjoint.\nWere you surprised by any of your insights?\n\nThe enormous difference between the training and test distributions was quite surprising: the difference is so large, that standard importance-weighting techniques for covariate shift will completely fail (because nearly all training points obtain an infinitesimal weight). I am curious to see how the contestants in the prediction challenge have dealt with this problem. I am also interested to know what the underlying phenomenon is that leads to the enormous shift in the data distribution (perhaps such knowledge suggests a preprocessing of the data that would reduce the shift).\nAnother surprising result was that the individual data sets appear to have quite different structure. This suggests that different data sets may be best modeled by different prediction models.\nWhat have you taken away from this competition?\n\nAlways visualize your data first, before you start to train predictors on the data! Oftentimes, visualizations such as the ones I made provide insight into the data distribution that may help you in determining what types of prediction models to try.\nBrief Bio\nI studied computer science at Maastricht University (The Netherlands), and obtained my Ph.D. from Tilburg University (The Netherlands) in 2009 for a thesis that used machine learning and computer vision techniques to analyze archaeological data. As a Ph.D. student, I became interested in using dimensionality reduction to visualize high-dimensional data and whilst visiting Geoffrey Hinton's lab at University of Toronto, Geoffrey and I developed t-SNE. After being doctored, I became a post-doctoral researcher at University of California San Diego, where I studied new algorithms for structured prediction and online learning, and where I worked on machine-learning applications in software engineering and face analysis. At present, I am a post-doctoral researcher at Delft University of Technology (The Netherlands), where I work on a range of topics including embedding, structured prediction, regularization, face recognition, and object tracking.\nI decided the enter the competition to test out the effectiveness of t-SNE on the challenging Merck data sets. Because I do not have any prior knowledge of the underlying process that generated the data, I had no means of \"helping\" t-SNE to produce an appropriate map of the data: visualizing this data really was a \"blind\" test of t-SNE. I am happy to see that t-SNE was effective on the Merck data in that it was helpful in building an intuition for the underlying data distribution.\nI like the Kaggle platform a lot because it provides a very fair way to compare different learning approaches. This makes the platform a very valuable addition to the experimental evaluations that are done in machine-learning papers (in such papers, experimental conditions may have been used that favor the approach developed by the authors of those papers; on Kaggle, such subtle ways of \"cheating\" are impossible). A downside of many Kaggle competitions is that the best performance is typically obtained by an ensemble of a large number of blended predictors. This makes it hard for individual machine-learning researchers to be very competitive.\n"}, {"url": "http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/", "link_text": "Deep Learning How I Did It: Merck 1st place interview", "id": 17, "title": "Permalink to Deep Learning How I Did It: Merck 1st place interview", "date": "2012-11-01", "text": "\nWhat was your background prior to entering this challenge?\nWe are a team of computer science and statistics academics. Ruslan Salakhutdinov and Geoff Hinton are professors at the University of Toronto. George Dahl and Navdeep Jaitly are Ph.D. students working with Professor Hinton. Christopher \"Gomez\" Jordan-Squire is in the mathematics Ph.D. program at the University of Washington, studying (constrained) optimization applied to statistics and machine learning.\nWith the exception of Chris, whose research interests are somewhat different, we are highly active researchers in the burgeoning subfield of machine learning known as deep learning, a sub-field revived by Professor Hinton in 2006. George and Navdeep, along with collaborators in academia and industry, brought deep learning techniques to automatic speech recognition. Systems using these techniques are being commercialized by companies around the world, including Microsoft, IBM, and Google.\nWhat made you decide to enter?\nWe wanted to show the Kaggle community the effectiveness of neural networks that use the latest techniques from the academic machine learning community, even when used on problems with relatively scarce data, such as the one from this competition. Neural nets similar to the ones we used have recently demonstrated a lot of success in computer vision, speech recognition, and other application domains.\nWhat preprocessing and supervised learning methods did you use?\nSince our goal was to demonstrate the power of our models, we did no feature engineering and only minimal preprocessing. The only preprocessing we did was occasionally, for some models, to log-transform each individual input feature/covariate. Whenever possible, we prefer to learn features rather than engineer them. This preference probably gives us a disadvantage relative to other Kaggle competitors who have more practice doing effective feature engineering. In this case, however, it worked out well. We probably should have explored more feature engineering and preprocessing possibilities since they might have given us a better solution.\nAs far as supervised learning goes, our solution had three essential components: single-task neural networks, multi-task neural networks, and Gaussian process regression. The neural nets typically had multiple hidden layers, used rectified linear hidden units, and used \"dropout\" to prevent overfitting. No random forests were harmed (or used) in the creation of our solution. We used simple, greedy, equally-weighted averaging of these three basic model types. At the very end we began experimenting with gradient boosted decision-tree ensembles to hedge our solution against what we believed other competitors would be using and improve our averages a bit. We didn't have a lot of time to explore these models, but they seemed to make very different predictions from our other models and were thus more useful in our averages than their often weaker individual performances would suggest. For similar reasons, we suspect that averaging our models with the models from other top teams could improve performance quite a bit.\nWhat was your most important insight into the data?\nOur single most important insight was that the similarity between the fifteen tasks could be exploited well by a neural network using all inputs from all tasks and with an output layer with fifteen different output units. This architecture allows the network to reuse features it has learned in multiple tasks and share statistical strength between tasks. Since we can only assume that Merck is interested in even more than the fifteen molecular targets in the competition data, it should be possible to gain even more benefits from combining more and more targets.\nWere you surprised by any of your insights?\nWe were somewhat surprised that using ridge regression for model averaging did not provide any detectable improvement over simple equally-weighted averaging.\nWhich tools did you use?\nWe used Matlab code released by Carl Rassmussen and Chris Williams to accompany their Gaussian processes book. For the neural nets we used a lot of our own research code (in python) and wrote some new neural net code specifically for the competition. Our research code is designed to run on GPUs using CUDA. The GPU component uses Tijmen Tieleman's gnumpy library. Gnumpy runs on top of Volodymyr Mnih's cudamat library. We also used scikits.learn for a variety of utility functions, our last minute experiments with gradient boosted decision trees, and our ill-fated attempts at more sophisticated model averaging.\nWhat have you taken away from this competition?\nOur experience has confirmed our opinion that training procedures for deep neural networks have now reached a stage where they can outperform other methods on a variety of tasks, not just speech and vision. In the Netflix competition, the Toronto group publicized their novel use of restricted Boltzmann machines for collaborative filtering, and the winners used this method to create several of the models that were averaged to produce the winning solution. In this competition we decided not to share our neural network methods before the close of the competition, which may have helped us win.\n"}, {"url": "http://blog.kaggle.com/2012/10/18/tuzzeg-the-troll-hunter-impermium-2nd-place-interview/", "link_text": "Tuzzeg the Troll-hunter: Impermium 2nd place Interview", "id": 18, "title": "Permalink to Tuzzeg the Troll-hunter: Impermium 2nd place Interview", "date": "2012-10-18", "text": "\nWe check in with the 2nd place winner of the Impermium \"Troll-dar\" Competition. \u00a0He's also published his\u00a0code\u00a0and a more detailed\u00a0explanation\u00a0of his approach on github.\nWhat was your background prior to entering this challenge?\nI used to work in Yandex (Russian N1 search engine) on text classification\nproblems. I also finished great online courses: ML class by Andrew Ng and\nNLP class by Manning and Jurafsky. Actually I am not a strong ML hacker, I\nthink my advantage was in variety in extracted features and text processing\nmethods.\nWhat made you decide to enter?\nI recognized this Kaggle competition as an opportunity to experiment with\ntext processing tasks and to learn more about machine learning techniques.\nWhat preprocessing and supervised learning methods did you use?\nI used stemming and dependency parsing in preprocessing. I also used\nlanguage model code which I wrote during studying at the NLP class. As for\nlearning methods - I used logistic regression for basic classifiers and\nrandom forest for the final ensemble.\nWhat was your most important insight into the data?\nSentence level features. After examining classification errors I realized\nthat many insulting posts were one sentence posts, and in bigger posts\nthere were one insulting sentence.\nWere you surprised by any of your insights?\nOne of the most surprising thing for me was that the simple stem-based\nfeatures (subsequnces and ngrams) work much better in the final ensemble\nthan complex features based on parser results and POS tags.\nSyntax features (features build around dependency parser results) alone\ngave me pretty great AUC, but got very low feature importance in the final\nensemble.\nWhich tools did you use?\nStanford POS tagger and parser for preprocessing. scikit-learn for learning.\nWhat have you taken away from this competition?\nI learned how to build ensembles using stacking (I said - I am not a ML\nhacker ;). Also got some insight how to use different NLP features.\nThis competition definitely gave me great opportunity to stretch my\nknowledge about ML and NLP. Eager to participate in next competition, just\nneed make up for the lost sleep  \nPhoto Credit:\u00a0Howard Dickins\n"}, {"url": "http://blog.kaggle.com/2012/10/15/make-for-data-scientists/", "link_text": "Make for Data Scientists", "id": 19, "title": "Permalink to Make for Data Scientists", "date": "2012-10-15", "text": "\nCross-posted from bitaesthetics.com\n(I'm replying re: a conversation started on the disqus thread on Engineering Practices in Data Science)\n\nAny reasonably complicated data analysis or visualization project will involve a number of stages. Typically, the data starts in some raw form and must be extracted and cleaned. Then there are a few transformation stages to get the data in the right shape, merge it with secondary data sources, or run it against a model. Finally, the results get converted into the output format desired.\nWhen I first started working with data, I did the transformations on the data sequentially, using a variety of tools and scripts. When a client came back with more source data, I would have to manually run it through the steps from beginning to end. As my projects got more complex, I noticed I was spending less time coding and more time manually managing the data pipeline. Worse still, I found myself returning to old projects with all the scripts and data intact, only to realize that I hadn't documented the pipeline and had to figure out from clues how to re-run it. I realized I needed to start automating the process to make my projects manageable.\nFortunately, I'm not the first person to want to automate a computation pipeline. Way back in 1977, Stuart Feldman at Bell Labs realized that the tasks required to compile a program could be abstracted out into a general-purpose pipeline tool. The command-line tool, called Make, is still widely used for building software, and it works just as well for data processing jobs.\nMake is easy to obtain, with free versions available for all common operating systems. In this post I will be discussing\u00a0GNU Make, which is included with\u00a0XCode\u00a0on Mac OS and either pre-installed or easy to obtain on most Linux systems. Windows users will want to look intoNmake, which is included with the\u00a0Windows SDK.\nTo use make, all you need is a file in the base directory of your project, typically namedMakefile\u00a0with no file extension. This file is plain text and will describe all the data transformation stages in your project. For example, suppose your project uses data from aHive\u00a0data warehouse and a JSON file available through a web API. You could create two rules for fetching the data by putting the following text in your makefile.\nsome_data.tsv :\u00a0\r\n hive -e \"select * from my_data\" > some_data.tsv\u00a0\r\nother_data.json :\u00a0\r\n curl example.com/some_data_file.json > other_data.json\nYou can think of makefile rules as stages of computation. The first line of a rule shows the output file (called a target) of the computation, followed by a colon. The second line is indented by a tab character (make\u00a0doesn't like spaces) and indicates the command used to generate the target file. This is run through your system shell, so you can use shell features like redirecting output into files as shown above.\nNow if you run\u00a0make some_data.tsv, it will run the Hive query and store the result insome_data.tsv. Likewise, you can run\u00a0make other_data.json\u00a0to fetch the JSON file usingcurl, a command-line tool for downloading files from the web. If you then run either command again,\u00a0make\u00a0will simply tell you that nothing needs to be done. That's because\u00a0makesees that the target file already exists.\nNow suppose you want to convert the\u00a0JSON\u00a0and\u00a0TSV\u00a0files to\u00a0CSV, and you have python scripts in your\u00a0src/\u00a0folder to do this. I'm using\u00a0src/\u00a0as an example; make is totally agnostic towards directory structure. You could add these rules anywhere in your makefile.\nsome_data.csv : some_data.tsv src/tsv_to_csv.py\u00a0\r\n src/tsv_to_csv.py\u00a0some_data.tsv some_data.csv\u00a0\r\nother_data.csv : other_data.json src/json_to_csv.py\u00a0\r\n src/json_to_csv.py other_data.json other_data.csv \nThis looks similar to the rules above, but notice now that we have two filenames after each colon. These are called components and tell make what files that stage of computation depends on. Now if you run\u00a0make some_data.csv, it will run the conversion scripttsv_to_csv.py\u00a0with the argument list given in the rule.\nHere's the cool part: now you delete\u00a0some_data.tsv\u00a0and run\u00a0make some_data.csv\u00a0again. Instead of failing because a required component (some_data.tsv) doesn't exist, make runs the rule for\u00a0some_data.tsv\u00a0first and then runs the rule for\u00a0some_data.csv\u00a0once the requirement is satisfied.\nIt's not just data files, either. If we change\u00a0src/json_to_csv.py\u00a0to, say, fix a bug, Make will know that\u00a0other_data.csv\u00a0is out of date and recreate it with the new version of the script. Make doesn't do anything magical, it just traces the path of the data and figures out what's missing or out of date based on the \"Last Modified\" attribute of the file. Note thatsrc/json_to_csv.py\u00a0is listed as a component of\u00a0other_data.csv\u00a0in the rule above.\nMakefiles are easy to use but scale well to a more complicated workflow. For example, I've used make to automate the entire flow of data from a raw source to a polished visualization, from gathering and transforming the data to rendering and compositing layers of the image. In general, make will support any workflow as long as it doesn't require writing to the same file from multiple stages.\nOne limitation of this approach is that intermediate data (the data made available from one stage to the next) can't be stored in an external database. This is because make relies on the existence and age of files to know what needs to be recomputed, and database tables don't always map to individual files stored in predictable locations. You can get around this by using an embedded database like SQLite and storing the database file within the data directory, as long as you create the database in one stage and restrict yourself to read-only access afterwards.\nA\u00a0make\u00a0workflow can play nicely with version control systems like Git. My habit is to keep data files (both source and derived) out of the repository and instead add rules to fetch them directly from their source. This not only reduces the amount of data in the repo, it creates implicit documentation of the entire build process from source to final product. If you're dealing with collaborators, you can use environment variables to deal with the fact that different collaborators may have slightly different build environments.\nMake may not be the best pipeline tool for every situation, but I've yet to find a tool that beats it on simplicity and versatility.\n\n\n\n\n\nphoto credit\u00a0Tomasz Stasiuk\n\n"}, {"url": "http://blog.kaggle.com/2012/10/12/observing-dark-worlds-a-beginners-guide-to-dark-matter-how-to-find-it/", "link_text": "Observing Dark Worlds: A Beginners Guide to Dark Matter & How to Find It", "id": 20, "title": "Permalink to Observing Dark Worlds: A Beginners Guide to Dark Matter & How to Find It", "date": "2012-10-12", "text": "\nHere at Kaggle we are very excited to launch a brand new Kaggle Recruit competition: Observing Dark Worlds (ODW). Being an Astrophysicist as well as a great lover of everything weird and wonderful such a competition really gets my motors going.\nThe subject of Dark Matter is commonly grouped with similar abstract concepts such as aliens, black holes, supernovae and the big bang, assumed to be incomprehensible and inaccessible. However, speaking from personal experience, grasping Dark Matter needn't require more than a wine glass and a candle. One of the main aims of ODW is to open up the metaphorical Dark Matter doors to all, engaging every type of data scientist, with the added hope of helping to explain the Universe!\nIn order to grasp the concept of Observing Dark Worlds, it would be good to get an understanding of what Dark Matter actually is, why we are interested in it and what we hope to achieve.\nWhat is Dark Matter?\nDark Matter is a particle (or composed of particles), like electrons, quarks, protons, but fundamentally different in the way it interacts. What type of particle it is, is yet to be seen, but evidence suggests that it could be part of some completely new family of particles.\nIt is thought that in the beginning the distribution of Dark Matter throughout the Universe was pretty uniform, however as small bits drew themselves together through the force of gravity, it started to clump and aggregate. As larger bodies coalesced it started to form huge structures leading to the present day where it exists as a cosmic spider's web, with strand like filaments funneling matter down to connecting points where you find huge clumps called halos. More abundant than everything we can see by a factor of 7:1, Dark Matter acts like a cosmological scaffolding. It provides the basic framework for all visible matter to form on. Its gravitational pull determines the positions of the galaxies and stars in the Universe, and in the environments of the most massive halos we see lots of galaxies all bound together orbiting one another.\nObserved distribution of galaxies in the sky. Each dot is a galaxy. It can be seen how the galaxies reveal the web like structure in the Universe with filaments and halos. Credit: M. Blanton & SDSS Collaboration, www.sdss.org\nSo how do we go about finding Dark Matter? As you may quite reasonable ask, how do we see something that is dark? And that's the crux of it: the problem with Dark Matter is that, well, its dark and we can't see it. Fortunately something so vast and massive does not go unnoticed. As Dark Matter halos become increasingly large they actually bend the fabric of spacetime, just as when you put a golf ball on a sheet of rubber it bends and distorts the rubber. If you add more golf balls the rubber sheet will become increasingly distorted. The result is that anything that passes close enough to the golf ball will 'fall' in, and become affected by the distorted space it is travelling in. If the object is massive enough it will fall in and be consumed by the object, i.e. a black hole, however if there is only a slight bending of spacetime the paths of particles moving past it will only be slightly bent. If we imagine a scenario in which such a passing particle is in fact a light particle (or photon), as it experiences the deformed spacetime it will roll down slightly and come out of the distortion at a different angle to when it went in. This results in the object which emitted the photon appearing different to us than it would in the absence of the deformation. In space the emitting objects of interest are galaxies and since there are a lot of galaxies behind these halos of dark matter that are causing spacetime distortions, all the photons and hence the shapes of the galaxies will appear different in a way that reflects the position of the Dark Matter.\nLight from a background galaxy is being bent by foreground Dark Matter halo. Credit: NASA, ESA & L. Cal\u00e7ada\nWhy do we need you guys to help us find these pieces of Dark Matter?\nDark Matter has proven to be very elusive. We have been trying to smash stuff together to find it (LHC CERN), we have tanks of xenon in abandoned mines trying to detect a signal of one interaction a year, yet still we have no definitive evidence for dark matter on Earth. The only place that we currently have direct evidence for Dark Matter is from the motion and behaviour of the galaxies and stars in the Universe (e.g The Bullet Cluster). Therefore the only way we can get an insight into the properties of Dark Matter is through the study of the wider Universe. If we can pin down what Dark Matter is from our observations maybe we can help with focus of Earth studies.\nA huge Dark Matter halo has pulled together a group of galaxies. The vast amount of Dark Matter has caused the image of a background galaxy to look like a smeared arc across the image. Credit: NASA, ESA, J. Richard (CRAL) and J.-P. Kneib (LAM). Acknowledgement: Marc Postman (STScI)\nUnderstanding the properties of Dark Matter requires accurate estimates of its peak position. If we can nail down the peak of the density profile for a halo we can explore its properties. However estimating this both accurately and precisely is extremely difficult.\nThe aim of ODW is to develop an algorithm that can pin point the positions of Dark Matter halos extremely precisely and without directional bias . If we can do this then we can fully exploit future space missions such as  Euclid.\nHow the competition is structured\nSo the aim of ODW is to develop an algorithm that can predict the position of Dark Matter halos. What we have done is simulate a number of skies filled with galaxies. Each sky will contain between 300 and 700 galaxies with a x and y sky coordinate between 0 and 4200. We will then place either 1, 2 or 3 Dark Matter halos in the sky between the galaxies and us such that the galaxies are distorted. We will then tell you what the ellipticities of the galaxies are. Ellipticity can be split into two components: e1 and e2. e1 describes the elongation of a galaxy in the x direction (positive e1) and the y direction (negative e1). e2 describes the elongation of a galaxy in the 45 degree angle. So positive e2 is a galaxy elongated on the 45 degree line and negative e2 is elongation in the 135 angle. For more on ellipticity please see the  Introduction to Ellipticity page. So the the test_sky and train_sky data will look for example like this (except in csv format):\n\n\n\nGalaxyId\nx\ny\ne1\ne2\n\n\nGalaxy1\n1234.56\n4000.21\n0.1422\n-0.03212\n\n\nGalaxy2\n3214.23\n1232.32\n0.3444\n0.0233\n\n\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\n\n\nWe will then provide you with the answers to the training data, which will look like this (again in csv format)\n\n\n\nSkyId\nN\nx_ref\ny_ref\nhalo_x1\nhalo_y1\nhalo_x2\nhalo_y2\nhalo_y3\nhalo_y3\n\n\nSky1\n1\n1086.8\n1114.61\n1086.8\n1114.61\n0\n0\n0\n0\n\n\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\n\nSky142\n2\n3477.71\n1907.33\n3477.71\n1907.33\n232.32\n4001.11\n0\n0\n\n\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\n\nSky223\n3\n2315.78\n1081.95\n2315.78\n1081.95\n2312.32\n2981.24\n198.23\n3889.01\n\n\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\n\n\nWhere the first column is the ID of the sky and allows you to cross reference the data file in train_skies and the answer in Training_halos.csv, then the number of halos in that sky, a x and y reference point which is used to calculate the metric, and then the true x and y coordinate of each halo. In the case when a halo is not present (i.e. there is only one or two halos in the sky) the position of the halo will be 0. Note: In the file the header for column 'N' will in fact be named numberHalos. We will also give you a file called 'testhalosCount.csv' which will have the information of the number of halos in each sky in the test set.\nObserving Dark Worlds: Getting you started\nThe data page presents two benchmarks in python which have been used to calculate the positions of halos. Although similar they are in fact calculating two different things. One is model dependant, i.e. it assumes some kind of relationship between the force of gravity and the distance the galaxy is from the peak of the Dark Matter density, the other just calculates the signal in a gridded image. Here are some snippets of code to help you on your way to calculating the positions of a Dark Matter halos.\nBenchmark #1: Creating a Signal Map\n\r\ndef dark_matter_finder( x_galaxy, y_galaxy, e1, e2, x_halo, y_halo)\r\n\"\"\"Function to calculate the Dark Matter signal around a proposed position\r\nArguments :\r\n  x_galaxy, y_galaxy: Vectors containing the x and y coordinate of each galaxy in the sky\r\n  e1, e2: The 2 components of ellipticity for each galaxy in the sky\r\n  x_halo, y_halo: The estimated coordinates of the halo\r\nReturns :\r\n  signal : Scalar value of the total signal given the proposed halo\r\n\"\"\"\r\n\r\n# Find out the angle each galaxy is at with respects to my guessed position of the halo\r\n  angle_wrt_halo = arctan((y_galaxy-y_halo)/(x_galaxy-x_halo))\r\n\r\n# Calculate the total signal for a halo at my guessed position\r\n  signal = sum( -(e1*np.cos(2.0*angle_wrt_halo) + \\ e2*np.sin(2.0*angle_wrt_halo)) )\r\n\r\n  return signal\r\n\nOnce you have calculated this you can search the image for the spots with the highest signal\n\r\nif __name__ == \"__main__\":\r\n  \"\"\" Main program to determine the position of a halo \"\"\"\r\n  # Read in the data from the Sky test file\r\n  x_galaxy, y_galaxy, e1, e2 = loadtxt('Test_Sky1.csv', usecols=(1, 2, 3, 4),\\ skiprows=1, unpack=True)\r\n\r\n  #I want to search the sky in a grid like fashion, so I want to split # the skyup and find the signal at each point in the grid\r\n  Number_of_bins = 10\r\n  Sky_size = 4200.0\r\n\r\n  #It is square in all cases\r\n  Binwidth = Sky_size/float(Number_of_bins)\r\n  gridded_map= zeros([Number_of_bins, Number_of_bins], float)\r\n  for i xrange(Number_of_bins):\r\n    for j in xrange(Number_of_bins):\r\n      x_halo = i*binwidth # Proposed x position of the halo\r\n      y_halo = j*binwidth # Proposed y position of the halo\r\n\r\n      gridded_map[i,j] = dark_matter_finder(x_galaxy, y_galaxy, e1, e2,\\ x_halo, y_halo)\r\n\r\n  estimated_x_position_halo=where(signal == max(gridded_map))[0][0]*binwidth\r\n  estimated_y_position_halo=where(signal == max(gridded_map))[1][0]*binwidth\r\n\nThis code would work reasonably well in the case of one halo. It would need to be extended if there were more than one.\nBenchmark #2: Maximum Likelihood \nSo another option, instead of finding the signal at a given point, it is possible to assume a profile of Dark Matter halo and then try to fit this model to the data. From this find the most likely position of the halo. So one such model could be that the distortion caused by a Dark Matter halo has a 1/r drop off, where r is the distance from the center of the halo. This code finds the likelihood of a halo at a particular position and then assumes that the position with the maximum likelihood is the position of the halo. So using the same main function as before but re-defining the dark_matter_finder function:\n\r\ndef dark_matter_finder( x_galaxy, y_galaxy, e1, e2, x_halo, y_halo) :\r\n  \"\"\"Function to calculate the likelihood of a dark matter halo given a proposed position\r\n  Arguments:\r\n    x_galaxy, y_galaxy: Vectors containing the x and y coordinate of each galaxy in the sky\r\n    e1, e2: The 2 components of ellipticity for each galaxy in the sky\r\n    x_halo, y_halo: The estimated coordinates of the halo\r\n  Returns :\r\n    likelihood : Scalar value of the likelihood given the proposed halo\r\n   \"\"\"\r\n\r\n  # Find out the radial distance and angle each galaxy is at with respects to my guessed position of the halo\r\n  radial_distance_galaxy_from_halo = sqrt( (x_galaxy-x_halo)**2 +\\ (y_galaxy-y_halo)^2 )\r\n  angle_wrt_halo = arctan((y_galaxy-y_halo)/(x_galaxy-x_halo))\r\n\r\n  #We assume that the ellipticity caused by this is 1/r\r\ntotal_ellipticity = 1.0/radial_distancee_galaxy_from_halo\r\n\r\n  #Then convert this into the two components of ellipticity, e1 and e2\r\n  e1_model = -total_ellipticity*cos(2.0*angle_wrt_halo)\r\n  e2_model = -total_ellipticity*sin(2.0*angle_wrt_halo)\r\n\r\n  #Now work out the chi-square fit of the model with compared to the data\r\n  chi_square_fit = sum( (e1_model - e1)**2 + (e2_model - e2)**2 )\r\n\r\n  #Convert to likelihood\r\n  likelihood = exp((-chi_square_fit/2.0))\r\n\r\n  return likelihood\r\n\nOnce again this snippet of code will only work for one halo. It is possible to simultaneously fit two halos (or three) to the sky so that you are fitting more parameters, for example by adding the effects of multiple halos.\nAlthough we have supplied you with two introductory methods please feel free to explore alternative routes (in fact we encourage it!) or use online code. One benchmark we have provided is a public code called LENSTOOL. LENSTOOL fits realistic models of dark matter halos to data using a clever sampling technique. It is one of the leading algorithms in this work you are more than welcome to build upon it!\nThe Observing Dark Worlds Metric: How are we testing you and how are you going to win?!\nSo the aim of the competition is to create an algorithm that can recognise the features of a Dark Matter halo in a field of galaxies. In future experiments we want to exploit entire data sets and therefore it is imperative that there is no residual bias in the algorithm, however we also don't want to lose sight of the main goal which is estimating the positions as accurately as possible. To this affect we have created a metric which will sum the average distance your estimates are away from the true position and the average angular vector of your position. (These will be weighted such they are both dimensionless and similar orders of magnitude.) For more details on the metric please visit the evaluation page of the competition.\nIs my algorithm really going to change the Universe?\nThe business of Gravitational Lensing (the bending of light due to matter) is a relatively new one, and therefore an opportunity exists to really change the way we approach this problem. We are looking for innovative, new and thought provoking methods that can solve this issue in a manner that the astrophysics community had never thought of.\nThe Wider Picture\nObserving Dark Worlds\nObserving Dark Worlds (ODW) will be an advert to not only astronomers but all investment banks and financial institutions. It will not only provide new ways to reconstruct the positions of Dark Matter but it will show how you can use abstract concepts as a method for recruitment filtering. Companies spend billions each year on finding the best of the best. Interviews, assessment centres and recruitment days, all of which cost money. Kaggle recruit allow companies to already filter out the best from the rest. By implementing difficult astronomical questions you are testing a participant\u2019s ability to apply their knowledge to different situations and scenarios. It allows those scientists out there who may have credentials that would normally get filtered out at the first hurdle to be considered for jobs. Kaggle have already helped major companies recruit new staff, who they admit, would never have considered purely on their academic background. Winton Capital, one of the largest hedge funds in the world, is sponsoring the ODW competition. They are looking to offer a budding data scientist a potential career by putting up the prize money. This is a great example of how the money to sponsor competitions doesn\u2019t always have to come from within the community.. They are looking for the brightest minds the world has to offer and all you have to do is grab their attention by solving the problems of the Universe. Good Luck and Happy Hunting!\nDavid Harvey\nDirector of Astrophysics, Kaggle\nTitle Background Image: Credit NASA; ESA; L. Bradley (Johns Hopkins University); R. Bouwens (University of California, Santa Cruz); H. Ford (Johns Hopkins University); and G. Illingworth (University of California, Santa Cruz)\n"}, {"url": "http://blog.kaggle.com/2012/10/05/tournament-vs-table-play-strategy-for-kaggle-comps/", "link_text": "Tournament vs. Table Play:  Strategy for Kaggle Comps", "id": 21, "title": "Permalink to Tournament vs. Table Play:  Strategy for Kaggle Comps", "date": "2012-10-05", "text": "\nCross-posted from Machined Learnings. \u00a0Paul discusses the differences between doing ML in an industrial vs a competition setting.\nI recently entered into a private\u00a0Kaggle\u00a0competition for the first time. Overall it was positive experience and I recommend it to anyone interested in applied machine learning.\nSince it was a private competition, I can only discuss generalities, but fortunately there are many. The experience validated all of the\u00a0machine learning folk wisdom\u00a0championed by Pedro Domingos, although the application of these principles is modified by the top-heavy metric-driven nature of the payouts. As in poker, where strategic adjustments are required between tournament and table play, machine learning competitions incent techniques that would be improper in a normal industrial setting.\nEVALUATION\nFor the competition we started by understanding the evaluation metric. In real-life (aka ``table play''), the goal of a machine learning system is often under-specified (e.g., ``improve relevance'' or``detect analomies'') or intractable to optimize directly due to aggregate impact and uncontrolled external factors (e.g., ``improve long-term customer retention and satisfaction''). Under these conditions proxy metrics are used, but hopefully nobody takes them too seriously because everybody understands the proxy metric was chosen for convenience.\nIn a competition (aka ``tournament play'') the evaluation metric determines how much you get paid. Ergo, it needs to be taken very seriously. The first step is to attempt to reproduce the evaluation metric using a simple model (e.g., the constant model), which is typically a known function applied to an unknown data set. Methods for assessing generalization error (k-fold cross-validation, out-of-bag estimation) can be used but it is important to remember that you are not trying to generalize to a future data set drawn from the same distribution, but rather a particular fixed data set used for evaluation. Note there is absolutely no problem with over-fitting the evaluation data set if possible. Whether this is possible depends upon how many intermediate submissions you are allotted and how submissions are scored, and whether you can see the evaluation features.\nIn this competition there was no twinning of a particular variable between training and evaluation sets, so this needed to be respected when doing cross-validation on the training set to estimate the final score. However, submissions were scored on a fixed (unknown) subset of the evaluation data which was a random line-by-line split of the evaluation set. Therefore submissions were a high quality method for probing the evaluation metric, and we used them as much as possible (subject to our submission limit). For instance, since the evaluation set was known (but not the labels!), we tried using covariate shift to reweight the training data and improve the models. Estimating the impact of this using the training data alone would have required implementing sub-crossfolding, and might have been misleading. Instead we prepared a pair of submissions that differed only in this respect and quickly learned this was not a promising line of attack. As another example, we decided between two different settings for a regularization parameter by preparing a pair of submissions and observing the resulting scores. (We used admittedly boring experimental design strategies to probe the evaluation set; maybe there are better ways to leverage submissions.)\nFEATURE ENGINEERING\nHere tournament and table play machine learning are more aligned, as feature engineering plays a critical role in both. However since the margin of victory in competitions is very slim, tournament feature engineering is more frantic. Put simply, if it appears to improve the model at all, it stays in: issues of interpretability, implementation complexity, maintainability, or evaluation time are unimportant in tournaments, except to the extent that they will cause problems for the modeler during the duration of the competition. In table play features are subject to a more nuanced cost-benefit analysis which accounts for these other factors.\nSince feature engineering is model selection, it is important to monitor the evaluation metric. During the feature engineering phase we used\u00a0vee-dub\u00a0exclusively in order to turn models around as quickly as possible.\n\n\n\n\n\n\nTime to un-pimp your features.\n\n\n\nIt's worth investing some time during the competition improving the speed at which you can turn around an evaluation estimate. I made a few, but I missed a big one which I only realized after the contest ended: I was doing 10-fold cross-fold validation by invoking vee-dub 10 times with different inputs, but this is best done internally in vee-dub via the reduction interface in order to amortize the I/O. Hopefully I can get that implemented in the near future.\nData shaping was also very important and relatively inefficient. By data shaping, I mean choosing how to represent a particular feature to the learning algorithm (e.g., try taking the log of this thing and setting some spline knots here, here, and here). Being able to specify this kind of data shaping directly to vee-dub would facilitate fast experimentation and parameter sweeping, again by amortizing I/O.\nFinally, there was certain ``features'' we found in the data that were clearly artifacts of how the competition data was collected and unlikely to ``truly generalize.'' For tournament play, the issue is simply whether they would generalize to the evaluation set, so we prepared paired submissions to assess. In table play, these features would be not be utilized at all unless the predictive lift was significant, and even then they would be cautiously assessed with additional data due to the clear lack of face validity.\nBy the way, we did discover (apparently) real features as well which should generate insight for the contest promoter. While the incentives associated with competitions are not perfect, mostly it still boils down to doing good work.\nMODEL AVERAGING\nOur winning submission consisted of several machine learned models trained using completely different procedures and averaged together. This is now standard tournament technique, although I've never used it explicitly in table play (implicitly yes, via boosting and bagging; but averaging a tree and a neural network is not something I've ever done in a commercial setting). I was absolutely shocked how effective this was: average a model with another model which is demonstrably worse, and the result is better then either one, thanks to a\u00a0bias-variance tradeoff. Getting something for nothing quickly becomes an addictive habit, but the best results come from combining models that were trained using completely different methods (i.e., models that do not covary), and it becomes harder to find completely different methods that generate competitive results (e.g., we tried metric-based methods but these were so poor that we did not use them in the final ensemble).\nGETTIN' REAL\nThe best part about tournament play is the intensity of competition. In table play, you are typically working cooperatively with other machine learning experts to produce a good model that will benefit the business. Although you might be competing with other businesses also leveraging machine learning technology, the relationship between the fortunes of your employer and the quality of your machine learning model is obscured. In tournament play, the teams doing better than you on the leaderboard directly indicate the suboptimality of your current technique; and if you do happen to find yourself at the top of the heap, the minuscule and ephemeral nature of your lead will drive you to find further improvements and insights. I found myself working far more than I originally anticipated.\nHappy Kaggling!\n"}, {"url": "http://blog.kaggle.com/2012/10/04/how-we-did-it-cprod-1st-place-interview/", "link_text": "How We Did It: CPROD 1st place interview", "id": 22, "title": "Permalink to How We Did It: CPROD 1st place interview", "date": "2012-10-04", "text": "\nWe catch up with the team of undergrads who took 1st place in the CPROD (Consumer Products) Challenge. \u00a0They'll be presenting their results this December at the\u00a0ICDM-2012 conference.\nWhat was your background prior to entering this competition?\nWe are undergraduate students from Tsinghua University, China. Before entering the competition, we have some experience about developing software and applications using techniques from machine learning and nature language processing. What\u2019s more, we attended KDD Cup 2012 Track 1 with the same team name \u201cISSSID\u201d and ranked 8th finally.\nWhat made you decide to enter?\nWe found that the problem was both challenging and research-oriented. In addition, the competition is a part of the ICDM 2012 conference.\nWhat preprocessing and supervise learning methods did you use?\nPreprocessing: (1) JSON format to plain text format (2) cleaning the data by deleting all useless characters and symbols. (3) Change all uppercase for few products to lowercase\nSupervision learning: We employed \u201cConditional Random Field\u201d Model. We choose this algorithm because it converges faster and is easy to implement. We used tool MALLET for this purpose.\nWhat was your most important insight into the data?\nThe specific characters of products naming (Example: iPhone \u2013 mixture of both uppercase and lowercase) and human sematic behavior analysis (Example: my iPhone or <action> by <company name>) are the most important insight that helped us to improve the precision overall. We finally took voting approach (to find which category the product belongs to) based on our experimental results.\nWere you surprised by any of your insights or any key features?\nWhen we merged the Conditional Random Field Model to other two models (Standard and Rule Template) we have, the performance we achieved significantly increased. We got approximately 3% improvement in F1 score.\nNotably combinations of CRF models achieved the highest score in the private leaderboard, but not in the public leaderboard.\nWhich tools did you use?\nLanguages: C++, Python, Perl\nTool: Mallet\nWhat have you taken away from this competition?\nReal Life problem challenges because of the following reason:\n1)\u00a0\u00a0\u00a0 Data is from heterogeneous dataset.\n2)\u00a0\u00a0\u00a0 Generally entity resolution is quite difficult task.\n3)\u00a0\u00a0\u00a0 Huge product name list.\n4)\u00a0\u00a0\u00a0 Semantics used in the forum environment poses challenge.\nEven we entered the competition very late, we have devised several approaches and ran quiet good amount of experiments to move in the right direction. We learnt working on real life problem poses lot many challenges. Working on this problem improved our approach, creativity and knowledge. Now we look forward to work more on such real life dataset problems. Finally we are glad to win the competition in a popular conference, and of course bucks!\n"}, {"url": "http://blog.kaggle.com/2012/10/03/practice-fusion-diabetes-classification-interviews-with-winners/", "link_text": "Practice Fusion Diabetes Classification - Interviews with Winners", "id": 23, "title": "Permalink to Practice Fusion Diabetes Classification - Interviews with Winners", "date": "2012-10-03", "text": "\nWe check in with the 1st, 2nd, and 3rd place teams in the Practice Fusion Diabetes Classification Challenge\u00a0( based on\u00a0Shea Parkes' top voted\u00a0submission in the Prospect round). \u00a0As an experiment, we've decided to group all the winners interviews together in one post to really highlight the diversity of backgrounds among successful data scientists.\nWhat are your backgrounds prior to entering this competition?\n1st place:\u00a0Jose Antonio Guerrero\u00a0aka 'blind ape',\u00a0Sevilla, Spain:\u00a0My degrees are in mathematics, statistics and operations research. I\u2019m worked in the public health sector for 25 years as researcher, IT technician and senior manager. \u00a0A year ago, when I turned 50, decided it was a good age for return at\u00a0my professional\u00a0origin, so I went to Virgen del Rocio Universitary\u00a0Hospital, the flagship hospital in the region.\u00a0I'm working with large size (8 figures) clinic records databases, grouping\u00a0clinical cases and with quality and research issues\n2nd:\u00a0Matt Berseth\u00a0aka 'mtb', Jacksonville, FL, USA:\u00a0I have a Bachelor's degree in computer science and a Master's degree in software engineering - both from North Dakota State University. I started my career as an intern with Microsoft and worked there full time for three years. Since leaving Microsoft, I have been working as a full stack developer with primarily Microsoft technologies for the last ten years. I have been fortunate to work in a variety of interesting areas including: automotive marketing, transportation management / logistics and healthcare IT.\n3rd:\u00a0Shashi Godbole\u00a0aka 'An apple a day',\u00a0Mumbai, India: Data mining has been the focus of my work for the past six odd years. Earlier this year, I started a consulting firm with a friend from my alma mater. I had worked on a few other Kaggle competitions in the past (including the Heritage Health Prize which is still going on) but had done it mostly for fun. These earlier competitions allowed me to brush up my skills and learn the latest advances in machine learning. I also turned to R as my primary tool for analysis.\nWhat made you decide to enter?\nJose:\u00a0My experience in health sector. I\u2019m used to clinical databases.\nMatt:\u00a0I studied machine learning as an undergraduate, but that was over ten years ago. So last fall when Coursera launched their machine learning course I decided to take the opportunity to get back up to speed. I enjoyed the course and took Daphane Koller's graphical models course this spring as a follow-up. With all of that theory under my belt, I decided I should apply it to something tangible like one of the Kaggle competitions.\nShashi:\u00a0Healthcare is one of the core focus areas of my team at the consulting firm we are building. The problem statement of this particular competition resonated strongly with the kind of problems we are looking to solve for healthcare providers, payers and other stakeholders.\nWhat preprocessing and supervised learning methods did you use?\nJose:\u00a0Main work was data cleaning and feature creation. Grouping diagnostics and actives principles was crucial.\u00a0As an advance of my solution, I based it in a hard preprocessing and\u00a0feature creation work:\n\nTranslating each medication to its active principles, route of\u00a0administration.\nGrouping principles\u00a0active by chemical families / clinical indication. In some cases, as statins,\u00a0adjusting dose equivalences. Choosing for each group between number of\u00a0prescriptions, dose or binary flag for feature creation.\nGrouping the diagnoses in base CCS and my personal experience.\nAnd much\u00a0more...\n\nAfter, the methods used were the well known gbm and randomforest and later stacking in a generalized additive model.\nMatt:\u00a0I spent a fair amount of time generating features from the ICD9, NDC and lab data. I used wikipedia heavily to learn more about diabetes and create features from the diagnoses and treatments that are related to diabetes.\nAll of the models I selected for my final submissions were boosted trees. I used anywhere from 5 to 13 different models and blended/combined their predictions to create my submissions.\nShashi:\u00a0The preprocessing was limited to missing value imputation for a few fields in the data tables. I used random forests, gradient boosting and neural networks to build several models which were finally stacked together to generate a final solution.\nWhat was your most important insight into the data?\nJose:\u00a0The great numbers of comorbidities and symptoms associated with diabetes.\nMatt:\u00a0The ICD9 data is rich. There is information in the hierarchy of the codes (i.e. what level a specific code belongs to), information regarding the health of the individuals family (i.e. any of the 'family history of' codes) and information regarding the individuals behavior (i.e. any of the 'history of non-compliance' codes). And of course the conditions that are associated with each of the codes.\nFor the first month or so of the competition I focused almost solely on feature generation. And a majority of these features were derived from the ICD9 codes.\nShashi:\u00a0One big insight was that the formats for some key fields were different in train and test datasets. This was causing a much larger error on test data than that on training data. I could not think of any explanation for this for quite a while. Fixing the formatting discrepancy resolved this issue and made the train and test errors consistent.\nWhere you surprised by any of your insights or any key features?\nJose:\u00a0I\u2019m surprised by gender overall impact. In the data, diabetes was much more prevalent in male (15%) than female (11%) but when fitting the model, the gender influence fell to 0.17%. Probably other comorbidities associated to gender would explain this reduction.\nMatt:\u00a0I was most surprised by the area's that I did not find interesting features. I did not find the lab data very useful and I thought the drug information would be more useful than it was. That surprised me. I would be interested in seeing what features the other competitors found in this area.\nShashi:\u00a0I created several new features by taking ratios of different pairs of features. I was surprised by the extent to which these features improved my model. I created these features towards the very end of the competition. It helped me jump up a couple of places on the leaderboard.\nWhich tools did you use?\nJose:\u00a0R\nMatt:\u00a0\n\n.Net / C# for writing the logic that generates the features\nSQL Server for storing the data as well as basic analysis (just sql queries from management studio)\nPython and scikit-learn for training, testing and evaluating the models\nR for graphical analysis (ggplot2)\n\nShashi: \u00a0I used only R to do all the data processing and the modeling. Excel was used just a little bit to do some quick plots on the data.\nWhat have you taken away from this competition?\nJose:\u00a0I learned a lot about active principles and their interactions with diabetes.\nMatt:\u00a0Trust your cross validation scores and use the public leaderboard as a measurement of competitiveness. When I selected my final models for submission, I picked the models with the lowest cross-validation scores, not the ones with the lowest public leaderboard scores. This worked well for me in this competition, using this formula, I ended up picking my five best models.\nShashi:\u00a0The most important learning was that feature engineering is of utmost importance. Perhaps even more than any fine-tuning of modeling algorithms. Allocating a lot of time to just review the data and create useful features can result in significant performance improvement.\n"}, {"url": "http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/", "link_text": "Troll Detection with Scikit-Learn", "id": 24, "title": "Permalink to Troll Detection with Scikit-Learn", "date": "2012-09-26", "text": "\nCross-post from Peekaboo,\u00a0Andreas Mueller's computer vision and machine learning blog. \u00a0This post documents his experience in the Impermium Detecting Insults in Social Commentary competition, but rest of the blog is well worth a read, especially for those interested in computer vision and Python scikit-learn and -image.\nRecently I entered my first\u00a0kaggle\u00a0competition - for those who don't know it, it is a site running machine learning competitions. A data set and time frame is provided and the best submission gets a money prize, often something between 5000$ and 50000$.\nI found the approach quite interesting and could definitely use a new laptop, so I entered\u00a0Detecting Insults in Social Commentary.\u00a0\nMy weapon of choice was Python with\u00a0scikit-learn\u00a0- for those who haven't read my blog before: I am one of the core devs of the project and never shut up about it.\nDuring the competition I was visiting Microsoft Reseach, so this is where most of my time and energy went, in particular in the end of the competition, as it was also the end of my internship. And there was also the\u00a0scikit-learn release\u00a0in between. Maybe I can spent a bit more time on the next competition.\nThe Task\nThe task was to classify forum posts / comments into \"insult\" and \"not insult\".\nThe original data set was very\u00a0 small, ~3500 comments, each usually between 1 and 5 sentences.\nOne week before the deadline, another ~3500 data points where released (the story is a bit more complicated but doesn't matter so much). Some data points had timestamps (mostly missing in training but available in the second set and the final validation).\nThe Result (Spoiler alert)\nI made 6th place.\u00a0Vivek Sharma\u00a0won.\nFrom some mail exchanges, comments in my blog and a\u00a0thread I opened in the competition forum, I know that at least places 1, 2, 4, 5 and 6 (me) used\u00a0scikit-learn\u00a0for classification and / or feature extraction. This seems like a huge success for the project! I haven't heard from the third place, yet, btw.\nEnough blabla, now to the interesting part:\nFirst\u00a0my code on github. Probably not so easy to run. Try my \"working\" branch of sklearn\u00a0if you are interested.\nThings That worked\nMy two best performing models are actually quite simple, so I'll just paste them here.\nThe first uses character n-grams, some handcrafted features (in BadWordCounter), chi squared and logistic regression (output had to be probabilities):\n\r\nselect = SelectPercentile(score_func=chi2, percentile=18)\r\nclf = LogisticRegression(tol=1e-8, penalty='l2', C=7)\r\ncountvect_char = TfidfVectorizer(ngram_range=(1, 5), analyzer=\"char\", binary=False)\r\nbadwords = BadWordCounter()\r\nft = FeatureStacker([(\"badwords\", badwords), (\"chars\", countvect_char), ])\r\nchar_model = Pipeline([('vect', ft), ('select', select), ('logr', clf)])\r\n\nThe the second is very similar, but also used word-ngrams and actually preformed a little better on the final evaluation:\n\r\nselect = SelectPercentile(score_func=chi2, percentile=16)\r\n\r\nclf = LogisticRegression(tol=1e-8, penalty='l2', C=4)\r\ncountvect_char = TfidfVectorizer(ngram_range=(1, 5), analyzer=\"char\", binary=False)\r\ncountvect_word = TfidfVectorizer(ngram_range=(1, 3), analyzer=\"word\", binary=False, min_df=3)\r\nbadwords = BadWordCounter()\r\n\r\nft = FeatureStacker([(\"badwords\", badwords), (\"chars\", countvect_char),(\"words\", countvect_word)])\r\nchar_word_model = Pipeline([('vect', ft), ('select', select), ('logr', clf)])\r\n\nMy final submission contained two more models and also the combination of all four. As expected, the combination performed better than any single model, but the improvement over char_model_word was not large (0.82590 AUC vs 0.82988 AUC, the winner had 0.84249).\nBasically all parameters here are crazily cross-validated, but many are quite robust (C= 12 and percentile=4 will give about the same results).\nSome of the magic happens obviously in BadWordCounter. You can see the implementation\u00a0here, but I think the most significant features are \"number of words in a badlist\", \"ratio of words that is in badlist\", \"ratio of words in ALL CAPS\".\nHere is a visualization of the largest coefficients of three of my model. Blue means positive sign (insult), red negative (not insult):\n\nFull sized image\nMost of the used features are quite intuitive, which I guess is a nice result (bad_ratio is the fraction of \"bad\" words, n_bad is the number).\nBut in particular the character plot looks pretty redundant, with most of the high positives detecting whether someone is a moron or idiot or maybe retarded...\nStill it performs quite well (and of course these are only 100 of over 10,000 used features).\nFor the list of bad words, I used one that allegedly is also used by google.\nAs this will include \"motherfucker\" but not \"idiot\" or \"moron\" (two VERY important words in the training / leaderboard set), I extended the list with these and whatever the thesaurus said was \"stupid\".\nInterestingly in some models, the word \"fuck\" had a very large negative weight.\nI speculate this is caused by n_bad (the number of bad words) having a high weight and \"fuck\" not actually indicating insults.\nAs a side note: for the parameter selection, I used the ShuffleSplit (as\u00a0Olivier\u00a0suggested), as StratifiedKFold didn't seem to be very stable. I have no idea why.\nI discovered very close to the end that there were some duplicates in the training set (I think one comment was present 5 times), which might have been messing with the cross-validation.\nThings that didn't work\nFeature selection:\nI tried L1 features selection with logistic regression followed by L2 penalized Logistic regression, though it was worse than univariate selection in all cases.\nI also tried RFE, but didn't really get it to work. I am not so familiar with it and didn't know how to adjust the step-size to work in reasonable time with so many features.\nI also gave the randomized logistic regression feature selection a shot (only briefly though), also without much success.\nClassifiers:\nOne of my submissions used elastic net penalized SGD, but that also turned out to be a bit worse than Logistic Regression.\nI also tried Bernoulli naive Bayes, KNN, and random forests (after L1 feature selection) to no avail.\nWhat surprised me most was that I couldn't get SVC (LibSVM) to work.\nThe logistic regression I used (from LibLinear) was a lot better than the LibSVM with Platt-scaling. Therefore I didn't really try any fancy kernels.\nFeatures:\nI tried to use features from PCA and K-Means (distance to centers).\nI also tried to use the chi squared kernel approximation in RandomizedChi2,\nas this often worked very well for bag of visual words, but didn't see any improvement.\nI also played with\u00a0jellyfish, which does some word stemming and standardization, but couldn't see an improvement.\nA long complicated pipeline:\nI also tried to put more effort into handcrafting the features and parsing the text.\nI used sentence and word tokenizers from\u00a0nltk, used collocations, extracted features\u00a0using regex, even tried to count and correct spelling mistakes.\nI briefly used part-of-speech tag histograms, but gave up on POS-tagging as it was very slow.\nYou can look up the details of what I tried\u00a0here.\nThe model using these features was by far the worst. I didn't use any character features, but many many handcrafted ones. And it didn't really overfit.\nIt was also pretty bad on the cross-validation on which I designed the features.\nApparently I didn't really find the features I was missing.\nI also used a database of positive and negative connotated words.\nI should probably have tried to combine each of these features with the other classifiers, though I wanted to avoid building to similar models (as I wanted to average them). Also I didn't really invest enough time to do that (my internship was more important to me).\nThings I implemented\nI made several additions to scikit-learn particularly for this competition.\nThey basically focused on text feature extraction, parameter selection with grid search and feature selection.\nThese are:\nMerged\n\nEnable grid searches using Recursive Feature Elimination. (PR)\nAdd minimum document frequency option to CountVectorizer (n-gram based text feature extraction) (PR)\nSparse Matrix support in Recursive Feature Elimination. (PR)\nSparse Matrix support in Univariate Feature Selection. (PR)\nEnhanced grid search for n-gram extraction. (PR)\nAdd AUC scoring function. (PR)\nMinMaxScaler: Scale data feature-wise between given values (i.e. 0-1). (PR)\n\nNot merged (yet)\n\nFeatureUnion: use several feature extraction methods and concatenate features. (PR)\nSparse matrix support in randomized logistic regression (PR).\nEnhanced visualization and analysis of grid searches. (PR)\nAllow grid search using AUC scores. (PR)\n\nThings I learned\nI learned a lot about how to process text. I never worked with any text data before and I think now I have a pretty good grip on the general idea. The data was quite small for this kind of application but still I think I got a little feel.\nAlso, it seems to me that the simplest model worked best, feature selection and feature extraction are very important, though hand-crafting features is very non-trivial.\nTo recap: my best single model was the \"char_word_model\",\u00a0 which can be constructed in 7 lines of sklearn stuff,\u00a0 together with 30 lines for custom feature extraction. I think if I had added also the date, I might have had a good chance.\nThings that worked for others\nMost contestants used similar models as I did, i.e. linear classifiers,\nword and character n-grams and some form of counting swearwords.\nVivek, who won, found that SVMs worked better for him than logistic regression. Chris Brew, who came in fourth, only used character n-grams\nand a customized SGD classifier. So even with very simple features, you can\nget very far.\nIt seems most people didn't use feature selection, which I tried a lot.\nThe most commonly used software was scikit-learn, as I said above, R, and\u00a0software from the Stanford NLP\u00a0group.\nFor details on what others used, see the discussion in the\u00a0kaggle forum.\nFinal Comments\nAfter the first version of this blog-post (which I now shamelessly rewrote), I got a huge amount (relatively speaking) of feedback from other competitors.\nThanks to everybody who shared there methods - in the comments, at kaggle, and at the\u00a0scikit-learn mailing list\u00a0- and even\u00a0their code!\nI feel it is great that even though this is a competition and money is involved, we can openly discuss what we use and what works. I think this will help push the \"data science\" community and also will help us create better tools.\nThere where several thing that seemed a bit weird about the competition.\nI know the competitions are generally still somewhat in a beta, phase, but there are some things that could be improved:\nThe scores from the leader board dropped significantly, from\u00a0\u00a0around 91 AUC\u00a0to\u00a0around 83 AUC\u00a0on the final evaluation. I'm pretty sure I did not overfit (in particular the leader board score was always close to my cross validation score and I only scored on the leader board 4 times). Some discussion about this is\u00a0here. Generally speaking, some sanity tests on the data sets would be great.\nI was a bit disappointed during the competition as cross-validation seemed very noisy and my standard deviation captured the scores of the first 15 places.\nThat also made it hard to see which changes actually helped.\nAlso, there seemed to be a high amount of label noise.\nFor example most of my models had this false positive:\nAre you retarded faggot lol If you are blind and dont use widgets then that doesnt mean everyone else does n't use them Widgets is one of the reasons people like android and prefer it agains iOS You can have any types of widgets for weather seeing your twitter and stuff and on ios you scroll like an idiot like a minute and when you finally found the apps you still have to click a couple of times before you see what you need Android 2:0 iOS ; ]\u00a0\nHope you enjoyed this lengthy post  \n"}, {"url": "http://blog.kaggle.com/2012/09/25/datalanche-pf-open-challenge-1st-place/", "link_text": "Datalanche: PF Open Challenge 1st place", "id": 25, "title": "Permalink to Datalanche: PF Open Challenge 1st place", "date": "2012-09-25", "text": "\nFor the final entry in our How I Did It series on the\u00a0\u00a0Practice Fusion Open Challenge, we spoke with the winner, Ryan Pedela, the CEO and co-founder of medical info search engine Datalanche ( currently in Private Beta, but you can check it out with the login info provided in his contest submission)\nWhat was your background prior to entering this competition?\nOur team at Datalanche has experience and expertise in computer science, computer graphics, gaming, and data science.\n\nWhat made you decide to enter?\n\nWe had already started work on a medical information search engine powered by de-identified patient records. The competition and Practice Fusion\u2019s data set aligned perfectly with our goals. The competition gave us an opportunity to build and test our search engine using a high-quality, real-world data set of 10,000 de-identified patient records.\n\nWhat preprocessing methods did you use to study the data?\n\nOne of our goals is to give users a dynamic, near real-time experience based on their input. This allows them to quickly see how a patient\u2019s demographics, medical history, etc affect any given medical statistic. Practice Fusion\u2019s data set was re-organized so that we could quickly compute statistics based on the user\u2019s input.\nWe use MedlinePlus Connect, an API from the National Library of Medicine, to provide our users encyclopedia information for every medical topic in our database. The API returns encyclopedia information formatted as HTML, but we needed a different HTML formatting than the one provided. We wrote Python scripts to reformat the encyclopedia information.\n\nHow did you decide what aspects of the data to use?\n\nAccording to a 2010 study by the Pew Research Center [1], the most commonly searched medical topics are symptoms, medical conditions, and treatments. We decided to focus on medical conditions and medications since they are commonly searched and a significant percentage of Practice Fusion\u2019s data set was devoted to those medical topics.\nreferences:\n1.\u00a0http://pewinternet.org/Reports/2011/HealthTopics.aspx\n\nWere you surprised by any of your insights or any key features?\n\nWe were surprised by the breadth of medical conditions and medications represented in Practice Fusion\u2019s data set given its relatively small size. With only 10,000 patients in the data set, 21 patients had been diagnosed with multiple sclerosis, a rare disease, and some had been treated with interferon, a medication prescribed for treatment of multiple sclerosis. Several other relatively rare medical conditions and medications are also represented in the data set. To us, that shows Practice Fusion\u2019s data set has great coverage for medical conditions and medications.\n\nWhich tools did you use?\n\nThe website is built with Javascript, HTML5, CSS3 on the client. On the server, we use Node.js for the web server, our relational database is MySQL, and Apache Solr for search. Data preprocessing scripts are written in Python.\n\nWhat have you taken away from this analysis?\n\nWe believe improving everyone's access to reliable medical information will improve health care. Our mission is to find medical correlations, trends, facts or \"insights\" which can only be found by analyzing large amounts of anonymous medical data, then intuitively showcase those insights. This competition showed us that the medical data necessary to accomplish our mission is available.\nPhoto Credit:\u00a0sgillies\n"}, {"url": "http://blog.kaggle.com/2012/09/25/ehr-with-r-practice-fusion-open-challenge-2nd-place-visualizations/", "link_text": "EHR with R - Practice Fusion Open Challenge 2nd place Visualizations", "id": 26, "title": "Permalink to EHR with R - Practice Fusion Open Challenge 2nd place Visualizations", "date": "2012-09-25", "text": "\nFirst time Kaggler Yasmin Lucero aka Yolio took home 2nd place in the Practice Fusion Open Challenge\u00a0by combining Electronic Health Records with general population data. \u00a0Also, lots of good tips on using R for visualizations ( Go ggplot2! )\nWhat was your background prior to entering this competition?\nI earned my PhD doing mathematical biology and statistics in the field of\u00a0marine fisheries science. I have done analytical work on a variety of\u00a0problems in environmental science, mostly working for NOAA (National\u00a0Oceanographic and Atmospheric Administration).\n\nWhat made you decide to enter?\nIt was a chance for me to build my portfolio. I recently decided that I\u00a0want to move into doing data analytics work for a tech company in the\u00a0health and wellness sphere. I need opportunities to demonstrate how my\u00a0skills transfer to this new area and how I add value. This project was\u00a0perfect for that.\n\nWhat preprocessing methods did you use to study the data?\nI work primarily in R. I used a package called RSQLite to access the SQL\u00a0database. I did this partly because I thought I would be doing a lot of\u00a0joins, but I found that practice fusion had already provided tables with\u00a0almost everything I wanted (the data was in great shape). I did eventually\u00a0write a few SQLite queries of my own: I wanted the number of doctor visits,\u00a0medications and diagnoses for each patient. You can sort of do this in R\u00a0with the aggregate function, but the database was large enough for it to be\u00a0quite slow. SQL is really good at doing that sort of thing fast.\nOnce the data was in R, I did exploratory analysis using the functions str\u00a0and table. Then, I made many histograms and other plots. I spent lots of\u00a0time studying the metadata/schema that was provided in a pdf file. I did a\u00a0bit of recoding. I recoded dmIndicator to a logical variate (TRUE/FALSE). I\u00a0recoded the NIST smoking codes. That was tricky; I had to dig around the\u00a0internet quite a bit to make sense of the metadata that was provided. I\u00a0eventually was able to recode the NIST codes into a binary logical variate\u00a0called Smoker. I also came across some obvious measurement error: there\u00a0were several weight measurements for greater than a 1000 pounds and height\u00a0measurements for people greater than 9 feet tall. I ended up cutting out\u00a0lots of unrealistic weight/height measures. I converted Year of Birth to\u00a0age. I ended up removing all of the data for Puerto Rico, since I couldn't\u00a0get comparative general population data.\nHow did you decide what aspects of the data to use?\nI wanted to find out how representative this medical population was of the\u00a0general U.S. population. So, I was interested in any data that I could get\u00a0both for the EHR and the general population, via the census or other\u00a0surveys. I searched the internet for general population data for anything\u00a0that was also in this data. This boiled down: obesity rates, smoking\u00a0status, diabetes status, state of residence, age and gender.\n\nWere you surprised by any of your insights or any key features?\nYes, I thought that the spatial distribution of the patient population\u00a0would drive the overall population characteristics, but this didn't happen.\u00a0It seemed that the EHR population represented a specific demographic slice\u00a0that was the same regardless of which state they were in.\n\nWhich tools did you use?\nThe graphics are all made with the ggplot2 package in R. I made a lot of\u00a0use of the reshape2 package as well, to prepare the data for plotting. And\u00a0I used RSQLite to get the data into R, and to implement a few queries. And\u00a0I used the knitr package to generate the markdown report.\n\nWhat have you taken away from this analysis?\nI think that my favorite result was the age/bmi distribution for diabetics\u00a0vs non-diabetics. Most diabetics are older, but young diabetics are much\u00a0more prone to be very overweight. Older diabetics have a weight\u00a0distribution that isn't that different from older non-diabetics. I also\u00a0thought it was interesting that while there is a strong relationship\u00a0between diabetes and BMI, most overweight people are not diabetic. Even at\u00a0extremely high BMI, 2/3 of people are not diabetic.\n"}, {"url": "http://blog.kaggle.com/2012/09/25/word-tornado-practice-fusion-open-challenge-3rd-place-interview/", "link_text": "Word Tornado - Practice Fusion Open Challenge 3rd Place Interview", "id": 27, "title": "Permalink to Word Tornado - Practice Fusion Open Challenge 3rd Place Interview", "date": "2012-09-25", "text": "\nWe catch up with Indy Actuary Shea Parkes\u00a0on his prize-winning Word Tornado entry to the Practice Fusion Open Challenge. \u00a0Shea also had the winning entry to the prospect phase of the predictive challenge, which was the source of the Practice Fusion Diabetes Classification contest (in which he placed 5th with NSchneider). \u00a0These dudes know their healthcare data.\nWhat was your background prior to entering this competition?\nI'm a health actuary with Milliman, Inc. I do some traditional services\u00a0like pricing and reserving, but I also focus on applied statistics and\u00a0statistical graphing. I have worked with EHR data for some client projects\u00a0before. Neil Schneider and I have teamed up on many of the Predictive\u00a0Modeling contests on Kaggle over the last couple years. You'll find us\u00a0consistently just outside the money and loitering in the forums.\nWhat made you decide to enter?\nI enjoy making visualizations. I don't have a lot of experience with\u00a0data-heavy dashboards, but I constantly make graphs to tell stories. As a\u00a0part of any Predictive Modeling contest I enter, I create mountains of\u00a0visualizations. I don't really believe a result until I can see the\u00a0result. Given this, I enjoyed the novelty of a contest just about\u00a0visualization.\nWhat preprocessing methods did you use to study the data?\nMostly I dealt with flattening the data to one observation per patient. I\u00a0focused on Age/Gender, Diagnosis, Medications, Smoking Status and some of\u00a0the Biometrics. For the Diagnosis and Medication prevalence I did some\u00a0Bayesian shrinkage since some patients were seen so rarely I did not\u00a0believe their complete health status had been captured. I used non-linear\u00a0expansions of the continuous variables since linear assumptions are tenuous\u00a0at best with real data. My entry goes into more details about the\u00a0particular form of dimensionality reduction I applied next. Lastly, I did\u00a0a few supervised learning steps to make the Data.gov mash-up.\nHow did you decide what aspects of the data to use?\nI focused on the portions that could tell a good story of redundancy. I\u00a0also avoided the portions that would require heavy imputation. I initially\u00a0chased down physician specialty, but there was too little variation to be\u00a0interesting in an unsupervised environment. For the Data.gov portion I\u00a0focused mainly on state-level information since that was the most likely\u00a0shared key. EHR data is commonly sparse in socioeconomic information, so I\u00a0chose income data.\nWere you surprised by any of your insights or any key features?\nI was surprised at how strong the Smoking Status was in the principal\u00a0component analysis. I expected Age/Gender to drive the major data\u00a0divisions, but Smoking Status was as important if not more so. Smokers\u00a0just utilize a very different set of services than non-smokers.\nWhich tools did you use?\nI did all of my analysis in R (http://cran.r-project.org/). I cite all of\u00a0the great packages I used in my report. It was the first time I had tried\u00a0flattening a relational database via R, and it went much smoother than I\u00a0expected. It was also the first time using the Markdown language, which\u00a0was also surprisingly easy.\nWhat have you taken away from this analysis\nA much better understanding of the Markdown language and the RStudio /Knitr implementation in particular. I will definitely be using that in my\u00a0consulting work going forward.\n"}, {"url": "http://blog.kaggle.com/2012/09/21/overkill-analytics-wordpress-winner-describes-his-method/", "link_text": "Overkill Analytics: WordPress Winner Describes His Method", "id": 28, "title": "Permalink to Overkill Analytics: WordPress Winner Describes His Method", "date": "2012-09-21", "text": "\nCrossposted from Overkill Analytics, the newly launched extra-curricular data science blog by Gigaom-Wordpress Challenge winner Carter S.\u00a0 You can also read more about his 'overkill' philosophy on Gigaom.\nI\u2019d like to start this blog by discussing my first\u00a0Kaggle\u00a0data science competition \u2013 specifically, the \u201cGigaOM WordPress Challenge\u201d. \u00a0\u00a0This was a competition to design a recommendation engine for WordPress blog users; i.e. predict which blog posts a WordPress user would \u2018like\u2019 based on prior user activity and blog content.\u00a0\u00a0 This \u00a0post will focus on how my engine used the WordPress social graph to find candidate blogs that were not in the user\u2019s direct \u2018like history\u2019 but were central in their \u2018like network.\u2019\nMy Approach\nMy general approach \u2013 consistent with my\u00a0overkill analytics\u00a0philosophy \u2013 was to abandon any notions of elegance and instead blindly throw multiple tactics at the problem. \u00a0 In practical terms, this means I hastily\u00a0wrote ugly Python scripts to create data features, and I used oversized RAM and CPU from an Amazon EC2 spot instance to avoid any memory or performance issues from inefficient code.\u00a0\u00a0 I then tossed all of the resulting features into a glm and a random forest, averaged the results, and hoped for the best. \u00a0 It wasn\u2019t subtle, but it was effective.\u00a0(Full code can be found\u00a0here\u00a0if interested.)\nThe WordPress Social Graph\nFrom my brief review of other winning entries, I believe one unique quality of my submission was its limited use of the WordPress social graph.\u00a0\u00a0 (Fair warning:\u00a0 I may abuse the term \u2018social graph,\u2019 as it is not something I have worked with previously.) \u00a0 Specifically, a user \u2018liking\u2019 a blog post creates a link (or edge) between user nodes and blog nodes, and these links construct a graph connecting users to blogs outside their current reading list:\n\nDefining user-blog relationships by this \u2018like graph\u2019 opens up a host of available tools and metrics used for social networking and other graph-related problems.\nNode Distance, a.k.a. Two Degrees of Separation\nThe simplest of these graph metrics is the concept of\u00a0node distance\u00a0within graphs.\u00a0 In this case, node distance is the smallest number of likes required to traverse between a particular user node and a particular blog node.\u00a0\u00a0 In the diagram above, for example, User A and Blog 4 have a node distance of 3, while User C and Blog 5 have a distance of 5.\nThe chart below breaks down likes from the last week of the final competition training data (week 5) by the node distance between the user and the liked blog within their prior \u2018like graph\u2019 (training data weeks 1-4):\n\nAs you can see, nearly 50% of all new likes are from blogs one \u2018edge\u2019 from the user \u2013 i.e., blogs the user had already liked in the prior four weeks.\u00a0\u00a0\u00a0 These \u2018like history\u2019 blogs are a small, easily manageable population for a recommendation engine, and there are many relevant features that can be extracted based on the user\u2019s history with the blog.\u00a0\u00a0 Therefore, the like history set was the primary focus of most contest submissions (including mine).\nHowever, expanding the search for candidates one more level \u2013 to a distance of 3 edges/likes traversed \u2013 encompasses 90% of all new likes.\u00a0\u00a0 A \u2018distance 3\u2019 blog wold be a blog that is not in the subject\u2019s immediate like history but that is in the history of another user who had liked at least one blog in common with the subject.\u00a0\u00a0 This is admittedly a large space (see below), but I think it significant that >90% of a user\u2019s liked blogs in a given week can be found by traversing through just one common reader in the WordPress graph.\u00a0\u00a0 Finding the key common readers and common blogs, therefore, is a promising avenue for finding recommendation candidates that are new to the subject user.\nNode Centrality, a.k.a. Finding The Common Thread\nAs referenced above, the main problem with using the distance 3 blogs as recommendation candidates is that the search space is far too large \u2013 most users have tens of thousands of blogs in their distance 3 sets:\n\nAs seen from the above chart, while a significant portion of users (~20%) have a manageable distance 3 blog set (1,000 to 2,000 blogs), the vast majority have tens of thousands of blogs within\u00a0that\u00a0distance.\u00a0\u00a0 (Some post-competition inspection shows that many of these larger networks are caused by a few \u2018hyper-active\u2019 users in the distance 3 paths.\u00a0 Eliminating these outliers could be a reasonable way to create a more compact distance 3 search set.)\nOne could just ignore the volume issues and run thousands of distance 3 blog candidates per user through the recommendation engine.\u00a0 However, calculating the features and training the models for this many candidate blogs would be computationally intractable (even given my inclination for overkill).\u00a0 To get a manageable search space, one needs to find a basic, easily calculable feature that identifies the most probable liked blogs in the set.\nThe metric I used was one designed to represent\u00a0node centrality, a measure of how important a node is within a social graph.\u00a0 There are many\u00a0sophisticated, theoretically sound ways to measure node centrality, but implementing them would have required minutes of exhaustive wikipedia reading.\u00a0\u00a0 Instead, I applied a highly simplified calculation designed to measure a blog node\u2019s three-step centrality within a specific user\u2019s social graph:\n\nStep 1(a):\u00a0 Calculate all three-step paths from the subject user in the graph (counting multiple likes between a user and blog and multiple possible paths);\nStep 1(b): Aggregate the paths by the end-point blog; and\nStep 1(c): Divide the aggregated paths by the total paths in step 1(a).\nStep 2: ???\nStep 3: Profit.\n\nThe metric is equivalent to the probability of reaching a blog in three steps from the subject user, assuming that at each outbound like/edge has an equal probability of being followed.\u00a0\u00a0 It is akin to Google\u2019s original\u00a0PageRank, except only the starting user node receives an initial \u2018score\u2019 and only three steps are allowed when traversing the graph.\nI don\u2019t know if this is correct or theoretically sound, but it worked reasonably well for me \u2013 substantially lifting the number of likes found when selecting candidates from the distance 3 graph:\n\nAs shown above, if you examine the first 500 distance 3 blogs by this node centrality metric, you can find over 20% of all the likes in the distance 3 blog set.\u00a0\u00a0 If you selected 500 candidates by random sample, however, only 3% of the likes from this population would be found.\u00a0\u00a0 While I am certain this metric could be improved greatly by using more sophisticated centrality calculations, the algorithm above serves as a very useful first cut.\nSome Very Simple Code\nI\u2019d feel remiss not putting any code in this post. \u00a0 Unfortunately, there was a lot of bulky data handling code I used in this competition to get to the point where I could run the analysis above, so posting the code that produced this data would require a lot of extra files. \u00a0I\u2019d happily send it all to anyone interested, of course, just e-mail me.\nHowever, in the interest of providing something, below is a quick node distance algorithm in Python that I used after the fact to calculate node distances in the like graph. \u00a0This is just a basic breadth-first search implemented in Python, with the input graph represented as a dictionary with node names as keys and sets of connected node names as values:\n\r\n\r\ndef distance(graph, start, end):\r\n\r\n  # return codes for cases where either the start point\r\n  # or end point are not in the graph at all\r\n   if start not in graph: return -2\r\n   if end not in graph: return -3\r\n\r\n# set up a marked dictionary to identify nodes already searched\r\n   marked = dict((k, False) for k in graph)\r\n   marked[start] = True\r\n\r\n# set a FIFO queue (just a python list) of (node, distance) tuples\r\n   queue = [(start, 0)]\r\n\r\n# as long as the queue is full...\r\n  while len(queue):\r\n     node = queue.pop(0)\r\n\r\n    # if the next candidate is a match, return the candidate's distance\r\n     if node[0] == end:\r\n        return node[1]\r\n\r\n    # otherwise, add all the nodes connected to the candidate if not already searched\r\n    # mark them as searched (added to queue) and associate them with candidate distance + 1\r\n     else:\r\n        nextnodes = [nn for nn in graph.get(node[0], set()) if marked[nn] == False]\r\n        queue.extend((nn, node[1]+1) for nn in nextnodes)\r\n        marked.update(dict((nn, True) for nn in nextnodes))\r\n\r\n  # if you fall through, return a code to show no connection\r\n  return -1\r\n\r\n\nConclusion\nIn the end, this node centrality calculation served as a feature in my recommendation engine\u2019s ranking and \u2013 more importantly \u2013 as a method of identifying the candidates to be fed into the engine.\u00a0\u00a0 I have not done the work to see how much this feature and selection method added to my score, but I know as a feature it added 2%-3% to my validation score, a larger jump than many other features.\u00a0\u00a0 Moreover, my brief review of the other winner\u2019s code leads me to think this may have been a unique aspect of my entry \u2013 many of the other features I used were closely correlated elements in the other\u2019s code.\nMore crucially, for actual implementation by Automattic the \u2018like graph\u2019 is a more practical avenue for a recommendation engine, and is probably what the company uses to recommend new posts.\u00a0\u00a0 Most of the work we did in the competition\u00a0differentiated\u00a0between posts from blogs the user was already reading \u2013 useful, but not a huge value-add to the WordPress user.\u00a0\u00a0 Finding\u00a0unseen\u00a0blog posts in which a user may have interest would be a more relevant and valuable tool, and finding new blogs for the user with high centrality in their social graph is a reasonable way to find them.\u00a0\u00a0 From my work on the competition, I believe these methods would be more promising avenues than NLP and topic-oriented methods.\nThe above posts covers all of my thinking in applying network concepts to the WordPress challenge problem, but I am certain\u00a0 I only scratched the surface.\u00a0 \u00a0There are a host of other metrics that make sense to apply (such as\u00a0eccentricity,\u00a0closeness\u00a0and\u00a0betweenness).\u00a0\u00a0 If you work for WordPress/Automattic (and that is the only conceivable reason you made it this far), I\u2019d be happy to discuss additional ideas, either on this blog or in person.\nPhoto Credit:\u00a0karindalziel\n"}, {"url": "http://blog.kaggle.com/2012/08/28/getting-started-with-data-science-linux/", "link_text": "Getting Started with Data Science Linux", "id": 29, "title": "Permalink to Getting Started with Data Science Linux", "date": "2012-08-28", "text": "\nCross-posted from Data Science Linux. \u00a0WARNING: This was not intended to be a copy-paste example. \u00a0Please use the code on github.\nI get many people interested in doing data science, yet, have no clue where to start. Fear no more! \u00a0This blog post will cover what to do when someone slaps you in the face with some data.\nWARNING (shameless plug): like the ACM hackathon running on Kaggle right now, jus sayin\u2019\nPrerequisites:\n\nSign up for an AWS account here:\u00a0http://aws.amazon.com/\nLaunch an Instance of Data Science Linux:\u00a0http://bit.ly/NV4PYm\nCode:\u00a0https://github.com/koooee/BigDataR_Examples/tree/master/ACM_comp\n\u00a0\nFor those of you tweaking out:\u00a0 (do this on a fresh launch of Data Science Linux)\n1. \u00a0git clone git://github.com/koooee/BigDataR_Examples.git \u00a0Examples\n2. pushd Examples; ./runme.sh\n3. You\u2019re done\n\u00a0\nStep1: \u00a0Load the data into postgres\ncreate\u00a0table\u00a0big_data_train\u00a0(\nuserid\u00a0varchar(100)\n,sku\u00a0varchar(50)\n,category\u00a0varchar(20)\n,query\u00a0varchar(2000)\n,click_time\u00a0timestamp\u00a0without\u00a0time\u00a0zone\n,query_time\u00a0timestamp\u00a0without\u00a0time\u00a0zone\n);\ncreate\u00a0table\u00a0big_data_test\u00a0(\nuserid\u00a0varchar(100)\n,category\u00a0varchar(20)\n,query\u00a0varchar(2000)\n,click_time\u00a0timestamp\u00a0without\u00a0time\u00a0zone\n,query_time\u00a0timestamp\u00a0without\u00a0time\u00a0zone\n);\nCOPY\u00a0big_data_train\u00a0(userid,\u00a0sku,\u00a0category,\u00a0query,\u00a0click_time,\u00a0query_time)\nFROM\u00a0'/mnt/big_data/train_big.csv'\nWITH\u00a0CSV;\nCOPY\u00a0big_data_test\u00a0(userid,\u00a0category,\u00a0query,\u00a0click_time,\u00a0query_time)\nFROM\u00a0'/mnt/big_data/test_big.csv'\nWITH\u00a0CSV;\nStep2:\u00a0convert raw id\u2019s to integer ids\n(makes many things easier, working with matrices, joins, searches .. etc..)\n-- Create table with all unique userIDs\ncreate\u00a0table\u00a0big_data_userid_mapping\u00a0as\nselect\u00a0distinct\u00a0userid\u00a0from\u00a0(select\u00a0userid\u00a0from\u00a0big_data_train\u00a0UNION\u00a0select\u00a0userid\u00a0from\u00a0big_data_test)\u00a0a;\n-- Create a table with all unique skus\ncreate\u00a0table\u00a0big_data_sku_mapping\u00a0as\u00a0select\u00a0distinct\u00a0sku\u00a0from\u00a0big_data_train;\n-- Create a table with all unique categories\ncreate\u00a0table\u00a0big_data_category_mapping\u00a0as\nselect\u00a0distinct\u00a0category\u00a0from\u00a0(select\u00a0category\u00a0from\u00a0big_data_train\u00a0UNION\u00a0select\u00a0category\u00a0from\u00a0big_data_test)\u00a0b;\n-- Create a table with all unique queries\ncreate\u00a0table\u00a0big_data_query_mapping\u00a0as\u00a0select\u00a0distinct\u00a0query\u00a0from\u00a0(select\u00a0query\u00a0from\u00a0big_data_train\u00a0UNION\u00a0select\u00a0query\u00a0from\u00a0big_data_test)\u00a0b;\n-- Create a sequential id mapping for each column \u00a0(sequential means it just increments by 1 for every value)\nalter\u00a0table\u00a0big_data_category_mapping\u00a0add\u00a0category_id serial;\nalter\u00a0table\u00a0big_data_query_mapping\u00a0add\u00a0query_id serial;\nalter\u00a0table\u00a0big_data_sku_mapping\u00a0add\u00a0sku_id serial;\nalter\u00a0table\u00a0big_data_userid_mapping\u00a0add\u00a0userid_id serial;\n\n\nStep3: Create a basic benchmark\n-- Top Sku\u2019s by category (so we can recommend these skus to that category in the test set)\ndrop\u00a0table\u00a0if\u00a0exists\u00a0category_counts;\n-- create a temporary table to store all counts by sku and category so we can filter later\ncreate\u00a0table\u00a0category_counts\u00a0as\nselect\ncategory\n,sku\n,count(*)\nfrom\u00a0big_data_train\u00a0a\ngroup\u00a0by\ncategory\n,sku\norder\u00a0by\u00a0count(*);\n-- create a table that has the top 5 skus in each category\ndrop\u00a0table\u00a0if\u00a0exists\u00a0top_5_skus_by_category;\ncreate\u00a0table\u00a0top_5_skus_by_category\u00a0as\nselect\na.category\n,a.sku\nfrom\u00a0category_counts\u00a0a\nwhere\u00a0a.sku\u00a0in\u00a0(\n-- filter out only the top 5 skus\nselect\u00a0b.sku\nfrom\u00a0category_counts b\nwhere\u00a0a.category=b.category\norder\u00a0by\u00a0b.count\u00a0desc\nlimit\u00a05);\n\u00a0\nStep4: \u00a0(insanely) Naive approach to using ALS with query data\n4.a) Extract a query matrix\ndrop\u00a0table\u00a0if\u00a0exists\u00a0query_matrix;\ncreate\u00a0table\u00a0query_matrix\u00a0as\nselect\nquery_id\n,sku_id\n,count(*)\nfrom\u00a0big_data_train_ids\ngroup\u00a0by\u00a0query_id,\u00a0sku_id;\n-- save to the local file system\nCOPY\u00a0query_matrix\u00a0to\u00a0'/mnt/query_matrix'\u00a0with\u00a0csv;\n\u00a0\n4.b) \u00a0Convert to matrix market format\n# use a data science linux helper to convert the query matrix to matrix market format\n~/Examples/helpers/convert_to_matrix_market_format.sh\u00a0/mnt/query_matrix\n\u00a0\n4.c) Use Graphlab to compute ALS matrix factorization\nDeeper description of this command here:\u00a0http://bit.ly/PMSSmx\npmf /mnt/query_matrix.matrix.market 0 \\\n--scheduler=\"round_robin(max_iterations=10,block_size=1)\" --matrixmarket=true \\ --lambda=0.065 --ncpus $(cat /proc/cpuinfo | grep -c processor)\n\u00a0\n4.d) Now that we have U and V we can use those to compute recommendations for queries\n\u00a0\n# graphlab will look for a second file with \u2018e\u2019 at the end that matches the input file\nln -s /mnt/*.U /mnt/output;\nln -s /mnt/*.V /mnt/outpute;\n\u00a0\n# run mode 8 is the recommendations feature\nglcluster /mnt/output 8 5 0 --matrixmarket=true \\ --training_ref='/mnt/query_matrix.matrix.market' \\\n--ncpus=$(cat /proc/cpuinfo | grep -c processor)\n\u00a0\nStep5: Blend it all together\n5.helpers)\n# lookup a real sku from its sku id\ndef\u00a0sku_lookup(s):\ntry:\nreturn\u00a0str(sku_id_lookup[str(s)])\nexcept:\nprint\u00a0\"ERROR: this sku {0} does not exist!\".format(s)\nsys.exit(0)\n\u00a0\n# category top 5 lookup\ncategories\u00a0=\u00a0dict(list())\ncategory_file\u00a0=\u00a0csv.reader(open('/mnt/top_5_skus_by_category'))\nfor\u00a0line\u00a0in\u00a0category_file:\ntry:\ncategories[line[0]].append(line[1])\nexcept\u00a0KeyError:\ncategories[line[0]]\u00a0=\u00a0list()\ncategories[line[0]].append(line[1])\n\u00a0\n5.a) Set up a query recommendations lookup\n# query predictions\nquery_recs\u00a0=\u00a0open('/mnt/output5.recommended-items.mtx')\n# burn the first 4 since this is the matrix market header\nquery_recs_mapping\u00a0=\u00a0[v.strip().split(' ')\u00a0for\u00a0k,v\u00a0in\u00a0enumerate(query_recs.read().split('\\n'))\u00a0if\u00a0k\u00a0>\u00a03]\nquery_recs_mapping\u00a0=\u00a0[[item.strip()\u00a0for\u00a0item\u00a0in\u00a0row]\u00a0for\u00a0row\u00a0in\u00a0query_recs_mapping]\n\u00a0\n5.b) If we haven\u2019t seen the query before, recommend from the category benchmark.\n# open the test file\nf\u00a0=\u00a0open('/mnt/big_data_test_file',\u00a0'r')\nfor\u00a0line\u00a0in\u00a0f.readlines():\nline_a\u00a0=\u00a0line.strip().split(\",\")\n# we \u00a0have this query so use the query recommender\nif\u00a0line_a[1]\u00a0!=\u00a0'':\npredictions.write(\" \".join(map(sku_lookup,\u00a0query_recs_mapping[int(line_a[1])]))\u00a0+\u00a0\"\\n\")\n# otherwise, we can use the category recommender\nelse:\npredictions.write(\" \".join(categories[line_a[2]])\u00a0+\u00a0\"\\n\")\n\u00a0\n6. DONE\npredictions will be written to /mnt/predictions and are ready for submission\n\u00a0\n"}, {"url": "http://blog.kaggle.com/2012/08/06/hackathon-how-i-did-it-shanda-innovations/", "link_text": "Hackathon How I Did It - Shanda Innovations", "id": 30, "title": "Permalink to Hackathon How I Did It - Shanda Innovations", "date": "2012-08-06", "text": "\nWhat\u00a0was\u00a0your\u00a0background\u00a0prior\u00a0to\u00a0entering\u00a0this\u00a0competition?\nWe\u00a0are\u00a0a\u00a0team\u00a0focused\u00a0on\u00a0data\u00a0mining\u00a0from\u00a0Shanda\u00a0Innovations,\u00a0a\u00a0tech\u00a0incubator\u00a0of\u00a0Shanda\u00a0Corporation\u00a0from\u00a0China.\u00a0It\u2019s\u00a0a\u00a0global\u00a0leading\u00a0interactive\u00a0entertainment\u00a0media\u00a0group\u00a0.\u00a0We\u00a0all\u00a0graduated\u00a0from\u00a0top\u00a0tier\u00a0universities\u00a0in\u00a0China,\u00a0majored\u00a0in\u00a0Computer\u00a0Science,\u00a0and\u00a0then\u00a0started\u00a0our\u00a0career\u00a0in\u00a0Chinese\u00a0IT\u00a0companies.\u00a0Right\u00a0before\u00a0the\u00a0EMI\u00a0Competition,\u00a0we\u00a0was\u00a0awarded\u00a0second\u00a0place\u00a0in\u00a0ACM\u00a0KDD-Cup\u00a02012.\n\nWas your strategy any different for competing in a 24-hour hackathon vs. the longer running KDD cup? \u00a0Any advice for future hackathon participants on how to win the 'sprint' rather than the 'marathon'?\n\nOur strategy for the longer running KDD-Cup and the 24-hour hackathon was very different. The 24 hour hackathon is a very intensive competition, so that what the participants need to do is to find the key features in a much faster way. This means the participants need to take simpler and more effective methods for preprocessing and post-processing. The hackathon is more demanding with getting quick reaction and making the right priorities. Given a chance, we will give the above advice for future participants.\n\nThe KDD-Cup lasting for several months is more demanding with the continuous concentration as well as better endurance of participants. Better skills in time management skill as well as project management are also necessary in this longer competition. Last but not least, participants also need to take care of each other\u2019s motivation for people tend to lose their eagerness to participate when the time goes by and it is hard to always keep a strong motivation.\n\nWhat\u00a0made\u00a0you\u00a0decide\u00a0to\u00a0enter?\nWe\u00a0would\u00a0love\u00a0to\u00a0put\u00a0ourselves\u00a0on\u00a0the\u00a0international\u00a0stage\u00a0and\u00a0to\u00a0compete\u00a0as\u00a0well\u00a0as\u00a0to\u00a0share\u00a0our\u00a0knowledge\u00a0of\u00a0data\u00a0mining\u00a0with\u00a0peers\u00a0from\u00a0all\u00a0over\u00a0the\u00a0world.\u00a0We\u00a0believe\u00a0participating\u00a0in\u00a0this\u00a0competition\u00a0would\u00a0be\u00a0a\u00a0precious\u00a0opportunity\u00a0to\u00a0\u201cmeet\u201d\u00a0all\u00a0the\u00a0talents\u00a0in\u00a0this\u00a0field.\u00a0In\u00a0addition,\u00a0the\u00a0competition\u00a0was\u00a0very\u00a0exciting\u00a0and\u00a0challenging.\u00a0Given\u00a0a\u00a0tight\u00a0time\u00a0constraint\u00a0of\u00a0the\u00a0competition,\u00a0we\u00a0viewed\u00a0it\u00a0as\u00a0a\u00a0chance\u00a0for\u00a0overnight\u00a0team\u00a0building.\nWhat\u00a0preprocessing\u00a0and\u00a0machine\u00a0learning\u00a0methods\u00a0did\u00a0you\u00a0use?\u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0\nSome\u00a0preprocessing\u00a0was\u00a0given\u00a0to\u00a0Words.txt.\u00a0We\u00a0mapped\u00a0the\u00a0words\u00a0that\u00a0users\u00a0chose\u00a0to\u00a0describe\u00a0artists\u00a0to\u00a0some\u00a0keyword\u00a0IDs\u00a0and\u00a0used\u00a0these\u00a0IDs\u00a0in\u00a0the\u00a0logistic\u00a0regression\u00a0model,\u00a0which\u00a0greatly\u00a0improved\u00a0the\u00a0performance.\nThe\u00a0main\u00a0machine\u00a0learning\u00a0methods\u00a0were\u00a0SVD++\u00a0and\u00a0Logistic\u00a0Regression.\u00a0\nWhat\u00a0was\u00a0your\u00a0most\u00a0important\u00a0insight\u00a0into\u00a0the\u00a0data?\nFor\u00a0most\u00a0users\u00a0the\u00a0training\u00a0data\u00a0was\u00a0very\u00a0sparse.\u00a0Therefore,\u00a0we\u00a0should\u00a0integrate\u00a0more\u00a0features\u00a0from\u00a0other\u00a0aspects.\u00a0For\u00a0instance,\u00a0the\u00a0user\u00a0profiles,\u00a0words\u00a0they\u00a0chose,\u00a0and\u00a0survey\u00a0results\u00a0would\u00a0be\u00a0very\u00a0valuable.\nSometimes\u00a0it\u00a0is\u00a0easy\u00a0for\u00a0us\u00a0to\u00a0trap\u00a0in\u00a0a\u00a0fixed\u00a0mindset\u00a0and\u00a0may\u00a0ignore\u00a0some\u00a0potential\u00a0important\u00a0indicators.\u00a0Keeping\u00a0our\u00a0mind\u00a0open\u00a0is\u00a0easier\u00a0said\u00a0than\u00a0done.\nWere\u00a0you\u00a0surprised\u00a0by\u00a0any\u00a0of\u00a0your\u00a0insights?\nWe\u00a0were\u00a0very\u00a0surprised\u00a0to\u00a0find\u00a0that\u00a0the\u00a0variation\u00a0of\u00a0the\u00a0track\u00a0scores\u00a0given\u00a0by\u00a0different\u00a0people\u00a0was\u00a0a\u00a0lot\u00a0more\u00a0than\u00a0we\u00a0expected.\u00a0For\u00a0instance,\u00a0User\u00a0ID\u00a041072\u00a0scored\u00a0100\u00a0to\u00a0track\u00a0156\u00a0whereas\u00a0User\u00a0ID\u00a041286\u00a0gave\u00a0merely\u00a04\u00a0to\u00a0the\u00a0same\u00a0track!\u00a0It\u00a0was\u00a0very\u00a0interesting\u00a0to\u00a0find\u00a0that\u00a0people\u00a0were\u00a0so\u00a0different\u00a0in\u00a0music\u00a0preference\u00a0and\u00a0we\u00a0believed\u00a0that\u00a0was\u00a0why\u00a0so\u00a0many\u00a0different\u00a0types\u00a0of\u00a0music\u00a0existed.\u00a0By\u00a0making\u00a0a\u00a0further\u00a0in-depth\u00a0data\u00a0analysis\u00a0we\u00a0may\u00a0discover\u00a0more\u00a0on\u00a0the\u00a0music\u00a0interests\u00a0of\u00a0people.\u00a0\nWhich\u00a0tools\u00a0and\u00a0programming\u00a0language\u00a0did\u00a0you\u00a0use?\nThe\u00a0programming\u00a0languages\u00a0include\u00a0C++\u00a0and\u00a0Python.\u00a0We\u00a0would\u00a0like\u00a0to\u00a0express\u00a0our\u00a0gratitude\u00a0to\u00a0APEX\u00a0team,\u00a0the\u00a0author\u00a0of\u00a0an\u00a0open\u00a0source\u00a0toolkit\u00a0called\u00a0SVDFeature\u00a0used\u00a0in\u00a0our\u00a0solution.\nWhat\u00a0have\u00a0you\u00a0taken\u00a0away\u00a0from\u00a0this\u00a0competition?\nWe\u00a0had\u00a0a\u00a0lot\u00a0of\u00a0fun\u00a0and\u00a0our\u00a0team\u00a0became\u00a0more\u00a0united\u00a0.\u00a0What\u00a0is\u00a0more\u00a0important\u00a0is\u00a0that\u00a0we\u00a0came\u00a0to\u00a0think\u00a0about\u00a0a\u00a0broader\u00a0area\u00a0of\u00a0data\u00a0mining\u00a0application.\u00a0It\u00a0is\u00a0interesting\u00a0to\u00a0see\u00a0our\u00a0technology\u00a0being\u00a0used\u00a0in\u00a0other\u00a0industries,\u00a0for\u00a0example\u00a0the\u00a0music\u00a0and\u00a0entertainment\u00a0industry\u00a0this\u00a0time.\u00a0This\u00a0is\u00a0an\u00a0eye-opening\u00a0experience\u00a0that\u00a0brought\u00a0a\u00a0lot\u00a0of\u00a0sparkles\u00a0to\u00a0our\u00a0routine\u00a0work.\u00a0\n"}, {"url": "http://blog.kaggle.com/2012/07/17/getting-started-with-the-wordpress-competition/", "link_text": "Getting Started with the WordPress Competition", "id": 31, "title": "Permalink to Getting Started with the WordPress Competition", "date": "2012-07-17", "text": "\nHey everyone,\nI hope you've had a chance to take a look at the WordPress competition! It's a really neat problem, asking you to predict which blog posts people have liked based on which posts they've liked in the past, and carries a $20,000 purse. I've literally lost sleep over this.\nThe WordPress data is a little bit tricky to work with, however, so to help you get up and running, in this tutorial I'll show and explain the python code I used to make the \"favorite blogs + LDA\" benchmark. Feel free to use the code as a starting point for your own submissions! The major code snippets used in this tutorial are excerpted and slightly modified from code available here.\n\nThere are two main challenges of working with the WordPress data set. First of all, the data files are very large, totaling 6.1 GB uncompressed. Unless you have exceptional hardware at your disposal, you'll need to be careful to choose an algorithm that is both computationally feasible and memory-friendly. The code I'll show you below was run on one core of my laptop over about 6 hours.\nThe second challenge is that the blog posts you're given have very few features you can immediately use in a model. Aside from these few features, (like the blog the post comes from, its author, or when it was posted), you'll need to generate the rest of them yourself, a natural language processing (NLP) problem. The code I'll show uses a Latent Dirichlet Allocation (LDA) model to estimate which \"topics\" a post is about.\nThe decisions I've made in my benchmark code were guided by these two considerations. Before getting into the details, I'll describe what the benchmark code does. In broad outline, the benchmark code estimates which posts a user will like by first considering posts from blogs they have liked before. Since there are usually more than 100 such posts, (and we are required to recommend 100 posts), we choose the 100 that are semantically the most similar to the posts the user has liked before and come from the blogs the user has liked the most.\nAnd now for the details: First of all, since the posts are given to us in json, and the post content is given in html, we need some functions for parsing these formats. I found a nice html-stripper here:\nfrom HTMLParser import HTMLParser\r\n\r\n# Tools for stripping html\r\nclass MLStripper(HTMLParser):\r\n    def __init__(self):\r\n        self.reset()\r\n        self.fed = []\r\n    def handle_data(self, d):\r\n        self.fed.append(d)\r\n    def get_data(self):\r\n        return ''.join(self.fed)\r\n\r\ndef strip_tags(html):\r\n    s = MLStripper()\r\n    s.feed(html)\r\n    return s.get_data()\nYou can use the html stripper like this:\n>>> strip_tags(' <h1>foo <a href=\"www.bar.com\">baz</a></h1> ')\r\n'foo baz'\nPython comes pre-equipped with a great json parser, which is as easy to use as:\n>>> import json \r\n>>> f = open(\"kaggle-stats-blogs-20111123-20120423.json\")\r\n>>> json.loads(f.readline())\r\n{u'blog_id': 4, u'num_likes': 781, u'num_posts': 204}\nHaving figured out how to parse our data formats, we turn our attention to the LDA. Briefly, Latent Dirichlet Allocation is an unsupervised semantics model that takes a corpus of documents--in this case, the blog posts--and estimates what \"topics\" they are about. A \"topic\" is a set of word frequencies, and a document is assumed to be composed of a mixture of topics. (Check out wikipedia for more detailed information). LDA often produces very intuitive results; in this case, for example, one of the topics was on the Trayvon Martin shooting, and another on Christianity.\nI use the LDA implementation from the python gensim module. This implementation supports an \"online\" estimation of the topics, which means that we can feed the model chunks of blog posts at a time, (and don't need to load all of the posts into memory at once). To take advantage of online LDA, I build my own classes Files and Corp, which are used to iterate over posts, yielding the parsed content of thoses posts as a list of words:\nfrom gensim import corpora\r\nimport string\r\n\r\n# An object to read and parse files without\r\n# loading them entirely into memory\r\nclass Files():\r\n    def __init__(self, files):\r\n        self.files = files\r\n    def __enter__(self):\r\n        return self\r\n    \r\n    def __exit__(self, exc_type, exc_value, traceback):\r\n        self.close()\r\n\r\n    # Read only one line at a time from the \r\n    # text files, to be memory friendly\r\n    def __iter__(self):\r\n        for f in self.files:\r\n            # Reset the file pointer before a new iteration\r\n            f.seek(0)\r\n            for line in f:\r\n                post = json.loads(line)\r\n                content = post[\"content\"]\r\n                doc_words = []\r\n                # parse and split the content up into\r\n                # a list of lower-case words\r\n                try: \r\n                    doc_words = strip_tags(content).encode('ascii',\r\n                    'ignore').translate(string.maketrans(\"\",\"\"), \r\n                    string.punctuation).lower().split()\r\n                except: # Fails on some nasty unicode\r\n                    doc_words = []\r\n                yield doc_words\r\n    def __len__(self):\r\n        n = 0\r\n        for f in self.files:\r\n            f.seek(0)\r\n            for line in f:\r\n                n += 1\r\n        return n\r\n    def close(self):\r\n        for f in self.files:\r\n            f.close()\r\n\r\n# A helper class, for use in \r\n# gensim's LDA implementation\r\nclass Corp():\r\n    def __init__(self, files, dic):\r\n        self.files = files\r\n        self.dic = dic\r\n    def __iter__(self):\r\n        for doc in self.files:\r\n            yield self.dic.doc2bow(doc)\r\n    def __len__(self):\r\n        return len(self.files)\nOur final step before actually beginning to work on our data is to define some stop words, that is, words that are so common that they don't help us distinguish between topics. We will not consider these words when training the LDA model.\n# These are words that will be removed from posts, due to their \r\n# frequency and poor utility in distinguishing between topics\r\nstop_words = [\"a\",\"able\",\"about\",\"across\",\"after\",\"all\",\"almost\",\r\n\"also\",\"am\",\"among\",\"an\",\"and\",\"any\",\"are\",\"as\",\"at\",\"be\",\"because\",\r\n\"been\",\"but\",\"by\",\"can\",\"cannot\",\"could\",\"did\",\"do\",\"does\",\"either\",\r\n\"else\",\"ever\",\"every\",\"for\",\"from\",\"get\",\"got\",\"had\",\"has\",\"have\",\r\n\"he\",\"her\",\"hers\",\"him\",\"his\",\"how\",\"however\",\"i\",\"if\",\"in\",\"into\",\r\n\"is\",\"it\",\"its\",\"just\",\"least\",\"let\",\"like\",\"may\",\"me\",\"might\",\"most\",\r\n\"must\",\"my\",\"neither\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"often\",\"on\",\"only\",\r\n\"or\",\"other\",\"our\",\"own\",\"rather\",\"said\",\"say\",\"says\",\"she\",\"should\",\r\n\"since\",\"so\",\"some\",\"than\",\"that\",\"the\",\"their\",\"them\",\"then\",\"there\",\r\n\"these\",\"they\",\"this\",\"to\",\"too\",\"us\",\"wants\",\"was\",\"we\",\"were\",\r\n\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\r\n\"with\",\"would\",\"yet\",\"you\",\"your\"]\r\n\nWe will use our first pass over the data to generate the dictionary of \"words\" used in the blog posts. Because of imperfect parsing, some of these \"words\" are things like \"www\" that aren't truly words, but are nonetheless useful for determining what a blog post is about. At this point, the code begins to take a while to run, so we occasionally save our work and attempt to load it in the try/except blocks.\nfrom __future__ import division\r\nfrom collections import defaultdict\r\nfrom Corp import stop_words, Files, Corp\r\nfrom gensim import corpora, models, similarities\r\nimport logging\r\nimport json\r\nimport cPickle\r\nimport random\r\n\r\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\r\n\r\n# First, we make a dictionary of words used in the posts\r\nwith Files([open(\"../trainPosts.json\"), open(\"../testPosts.json\")]) as myFiles:\r\n    try: \r\n        dictionary = corpora.dictionary.Dictionary.load(\"dictionary.saved\")\r\n    except:\r\n        dictionary = corpora.Dictionary(doc for doc in myFiles)\r\n        stop_ids = [dictionary.token2id[stopword] for stopword in stop_words if stopword in dictionary.token2id]\r\n        infreq_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq < 50]\r\n        dictionary.filter_tokens(stop_ids + infreq_ids) # remove stop words and words that appear infrequently\r\n        dictionary.compactify() # remove gaps in id sequence after words that were removed\r\n\r\n        dictionary.save(\"dictionary.saved\")\nNext, we train the LDA model with the blog posts, estimating 100 topics.\ntry:\r\n        lda = models.ldamodel.LdaModel.load(\"lda.saved\") \r\n    except:\r\n        lda = models.ldamodel.LdaModel(corpus=Corp(myFiles, dictionary), id2word=dictionary, num_topics=100, update_every=1, chunksize=10000, passes=1)\r\n\r\n        lda.save(\"lda.saved\")\nNow, we do some quick preliminary work to determine which blogs have which posts, and to map post_id's to a zero-based index, or vice versa\ntrainPostIndices = {}\r\nblogTrainPosts = defaultdict(list)\r\nwith open(\"../trainPostsThin.json\") as f:\r\n    for i, line in enumerate(f):\r\n        post = json.loads(line)\r\n        blog_id = post[\"blog\"]\r\n        post_id = post[\"post_id\"]\r\n        trainPostIndices[post_id] = i\r\n        blogTrainPosts[blog_id].append(post_id)\r\n\r\nlogging.info(\"Done doing preliminary training data processing\")\r\n\r\ntestPostIds = []\r\ntestPostIndices = {}\r\nblogTestPosts = defaultdict(list)\r\nwith open(\"../testPostsThin.json\") as f:\r\n    for i, line in enumerate(f):\r\n        post = json.loads(line)\r\n        blog_id = post[\"blog\"]\r\n        post_id = post[\"post_id\"]\r\n        testPostIds.append(post_id)\r\n        testPostIndices[post_id] = i\r\n        blogTestPosts[blog_id].append(post_id)\r\n\r\nlogging.info(\"Done doing preliminary test data processing\")\n\u00a0\nWe now estimate the test post topic distributions. This distribution is represented by a 100 dimensional (sparse) vector, one for each post, which indicates the likelihood that a word from a given post will belong to a given topic. We then construct a lookup-index of these test post vectors, for quick answers to questions about what test posts are similar to a given training post. The similarity measure between two posts is defined to be the cosine of the angle between their topic distribution vectors, like the correlation except that we do not subtract the mean. Since the similiarity measure is the cosine of an angle, it is always between -1 and 1.\nlogging.info(\"Making the test lookup index...\")\r\n\r\ntry:\r\n    testVecs = cPickle.load(open(\"TestVecs.saved\", \"r\"))\r\n    testIndex = similarities.Similarity.load(\"TestIndex.saved\")\r\nexcept:\r\n    with Files([open(\"../testPosts.json\")]) as myFilesTest:\r\n        myCorpTest = Corp(myFilesTest, dictionary)\r\n        testVecs = [vec for vec in lda[myCorpTest]]\r\n        testIndex = similarities.Similarity(\"./simDump/\", testVecs, num_features=100)\r\n        testIndex.num_best = 100\r\n    cPickle.dump(testVecs, open(\"TestVecs.saved\", \"w\"))\r\n    testIndex.save(\"TestIndex.saved\")\r\n\r\nlogging.info(\"Done making the test lookup index\")\n\u00a0\nWe estimate the training topic vectors, which we can hold in memory since they are sparsely coded in gensim. This is purely a matter of convenience; if this were too onerous a requirement on memory, we could estimate the training topics on the fly.\nlogging.info(\"Estimating the training topics...\")\r\n\r\ntry:\r\n    TrainVecs = cPickle.load(open(\"TrainVecs.saved\", \"r\"))\r\nexcept:\r\n    with Files([open(\"../trainPosts.json\")]) as myFilesTrain:\r\n        myCorpTrain = Corp(myFilesTrain, dictionary)\r\n        trainVecs = [vec for vec in lda[myCorpTrain]]\r\n    cPickle.dump(trainVecs, open(\"TrainVecs.saved\", \"w\"))\r\n\r\nlogging.info(\"Done estimating the training topics\")\n\u00a0\nFinally, we begin making submissions. As you'll recall, we only consider posts from blogs the user has liked before. To rank the test posts from these blogs, we score them as follows: Every post gets a score that is the sum of a \"blog score\" and a \"semantic score\". The blog score is equal to the fraction of posts the user liked in the training set from a given blog out of all the posts that blog published, weighted by a \"blog_weight\", in this case 2.0. The semantic score is equal to the greatest semantic similarity between the post in question and the posts the user liked in the train period.\nAs an example, suppose we wished to score this blog post for a given user. Suppose the user had liked 8 out of 13 blog posts from the \"Kaggle\" blog in the test period, and that the closest semantic similarity between this post and any of the 15 posts the user liked in the training period was 0.93. Then this post would be scored as 2.0 * 8/13 + 0.93 = 2.16.\nlogging.info(\"Beginning to make submissions\")\r\nwith open(\"../trainUsers.json\", \"r\") as users, open(\"submissions.csv\", \"w\") as submissions:\r\n    submissions.write(\"\\\"posts\\\"\\n\")\r\n    for user_total, line in enumerate(users):\r\n        user = json.loads(line)\r\n        if not user[\"inTestSet\"]:\r\n            continue\r\n\r\n        blog_weight = 2.0\r\n        posts = defaultdict(int) # The potential posts to recommend and their scores\r\n\r\n        liked_blogs = [like[\"blog\"] for like in user[\"likes\"]]\r\n        for blog_id in liked_blogs:\r\n            for post_id in blogTestPosts[blog_id]:\r\n                posts[post_id] += blog_weight / len(blogTestPosts[blog_id])\r\n        # After this, posts[post_id] = (# times blog of post_id was liked by user in training) / (# posts from blog of post_id in training)\r\n        posts_indices = [testPostIndices[post_id] for post_id in posts]\r\n        posts_vecs = [testVecs[i] for i in posts_indices]\r\n\r\n        liked_post_indices = []\r\n        for like in user[\"likes\"]:\r\n            try: # For whatever reason, there is a slight mismatch between posts liked by users in trainUsers.json, and posts appearing in trainPosts.json\r\n                liked_post_indices.append(trainPostIndices[like[\"post_id\"]])\r\n            except:\r\n                logging.warning(\"Bad index!\")\r\n\r\n        total_likes = len(liked_post_indices)\r\n        sample_size = min(10, total_likes)\r\n        liked_post_indices = random.sample(liked_post_indices, sample_size) # to cut down computation time\r\n        liked_post_vecs = [trainVecs[i] for i in liked_post_indices]\r\n        likedPostIndex = similarities.SparseMatrixSimilarity(liked_post_vecs, num_terms=100)\r\n\r\n        for posts_index, similar in zip(posts_indices, likedPostIndex[posts_vecs]):\r\n            posts[testPostIds[posts_index]] += max([rho for rho in similar])\r\n        # ie, posts[post_id] += max(semantic similarities to sample of previously liked posts)\n\u00a0\nIf there are less than 100 test posts from blogs the user has previously liked, we fill up remaining spaces with posts semantically similar to previously liked posts, (almost always from different blogs).\nif len(posts) < 100:\r\n  similar_posts_ids  = [(testPostIds[i], rho) for similar100 in testIndex[liked_post_vecs] for i, rho in similar100]\r\n    for post_id, rho in similar_posts_ids:\r\n    posts[post_id] += rho / sample_size\r\n    # dividing by the sample size ensures that the biggest additional score a post could get from this is 1.0\n\u00a0\nFinally, we pick the top 100 blogs, (or less if that's the case), and write them to our submissions file!\nrecommendedPosts = list(sorted(posts, key=posts.__getitem__, reverse=True))\r\n  output = \" \".join(recommendedPosts[:100]) + \"\\n\"\r\n  submissions.write(output)\r\n\r\n  if user_total % 100 == 0:\r\n    logging.info(\"User \" + str(user_total) + \" out of 16262\")\n\u00a0\nWell, you've seen it all, now. As a result, you've no doubt seen that there's plenty of room for improvement! Maybe we should include more blog posts than just those from blogs the user has previously liked, or actually parse the html instead of stripping the tags, (at present, for example, image tags are removed), or come up with a more sophisticated recommendation system than the primitive scoring I've used. Whatever you do, I can't wait to see it!\nWishing you the best of luck,\nNaftali Harris\nIntern Commander-In-Chief\nP.S. Thanks to Martin O'Leary for help on cleaning up my code!\n"}, {"url": "http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/", "link_text": "The Dangers of Overfitting or How to Drop 50 spots in 1 minute", "id": 32, "title": "Permalink to The Dangers of Overfitting or How to Drop 50 spots in 1 minute", "date": "2012-07-06", "text": "\nThis post was originally published on Gregory Park's blog.\u00a0 Reprinted with permission from the author (thanks Gregory!)\nOver the last month and a half, the Online Privacy Foundation hosted a Kaggle competition, in which competitors attempted to predict psychopathy scores based on abstracted Twitter activity from a couple thousand users. One of the goals of the competition is to determine how much information about one\u2019s personality can be extracted from Twitter, and by hosting the competition on Kaggle, the Online Privacy Foundation can sit back and watch competitors squeeze every bit of predictive ability out of the data, trying to predict the psychopathy scores of 1,172 Twitter users. Competitors can submit two sets of predictions each day, and each submission is scored from 0 (worst) to 1 (best) using a metric known as \u201caverage precision\u201c. Essentially, a submission that predicts the correct ranking of psychopathy scores across all Twitter accounts will receive a score of 1.\nOver the course of the contest, I made 42 submissions, making it my biggest Kaggle effort yet. Each submission is instantly scored and competitors are ranked on a public leaderboard according to their best submission. However, the public leaderboard score isn\u2019t actually the \u201ctrue\u201d score \u2013 it is only an estimate based on a small portion of the submission. When the competition ends, five submissions from each competitor are compared to the full set of test data (all 1,172 Twitter accounts), and the highest scoring submission from each user is used to calculate the final score. By the end of the contest, I had slowly worked my way up to 2nd place on the public leaderboard, shown below.\n\n\nTop of the public leaderboard. The public leaderboard scores are calculated during the competition by comparing users\u2019 predictions to a small subset of the test data.\nI held this spot for the last week and felt confident that I would maintain a decent spot on the private or \u201ctrue\u201d leaderboard. Soon after the competition closed, the private leaderboard was revealed. Here\u2019s what I saw at the top:\n\nTop of the private leaderboard. The private leaderboard is the \u201creal\u201d leaderboard, revealed after the contest is closed. Scores are calculated by comparing users\u2019 predictions to the full set of test data.\nWhere\u2019d I go? I scrolled down the leaderboard\u2026. further\u2026 and further\u2026 and finally found my name:\n\nMy place on the private leaderboard. I dropped from 2nd place on the public leaderboard to 52nd on the private leaderboard. Notice I placed below the random forest benchmark!\nSomehow I managed to fall from 2nd all the way down to 52nd! I wasn\u2019t the only one who took a big fall: the top five users on the public leaderboard ended up in 64th, 52nd, 58th, 16th, and 57th on the private leaderboard, respectively. I even placed below the\u00a0random forest benchmark, a publicly solution available from the start of the competition.\nWhat happened?\nAfter getting over the initial shock of dropping 50 places, I began sifting through the ashes to figure out what went so wrong. I think those with more experience already know, but one clue is in the screenshot of the pack leaders on the public leaderboard. Notice that the top five users, including myself, have\u00a0a lot of submissions. For context, the median number of submissions in this contest was six. Contrast this with the (real) leaders on the private leaderboard \u2013 most have less than 12 submissions. Below, I\u2019ve plotted the number of entries from each user against their final standing on the public and private leaderboards and added a trend line to each plot.\n\nOn the public leaderboard, more submissions are consistently related to a better standing. \u00a0It could be that the public leaderboard actually reflects the amount of brute force from a competitor rather than predictive accuracy. If you throw enough mud at a wall, eventually some of it will start to stick. The problem is that submissions that score well using this approach probably will not generalize to the full set of test data when the competition closes. It\u2019s possible to overfit the portion of test data used to calculate the public leaderboard, and it looks like that\u2019s exactly what I did.\nCompare that trend in the public leaderboard to the U-shaped curve in the plot of the private leaderboard and number of submissions. After about 25 submissions or so, private leaderboard standings get worse with the number of submissions.\nPoor judgments under uncertainty\nOverfitting the public leaderboard is not unheard of, and I knew that it was a possibility all along. So why did I continue to hammer away at competition with so many submissions, knowing that I could be slowly overfitting to the leaderboard?\nMany competitors with estimate the quality of their submissions prior to uploading them using cross-validation. Because the public leaderboard is only based on a small portion of the test data, it is only a rough estimate of the true quality of a submission, and cross-validation gives a sort of second opinion of a submission\u2019s quality. For most of my submissions, I used 10-fold cross-validation to estimate the average precision. So throughout the contest, I could observe both the public leaderboard score and my own estimated score from cross-validation. After the contest closes, Kaggle reveals the private or \u201ctrue\u201d score of each submission. Below, I\u2019ve plotted the public, private, and cv-estimated score of each submission by date.\n\nScores of my 42 submissions to the Psychopathy Prediction contest over time. Each submission has three corresponding scores: a public score, a private score, and a score estimated using 10-fold cross validation (cv). The public score of a submission is calculated instantly upon submission by comparing a subset of the predictions to a corresponding subset of the test data. The private score is the \u201ctrue\u201d score of a submission (based on the entire set of test data) and is not observable until the contest is finished. Every submission has a public and private score, but a few submissions are missing an CV-estimated score. The dotted line is the private score of the winning submission.\nThere are a few things worth pointing out here:\n\nMy cross-validation (CV) estimated scores (the orange line) gradually improve over time. So, as far as I know, my submissions are actually getting better as I go.\nThe private or \u201ctrue\u201d scores actually get\u00a0worse\u00a0over time. In fact, my first two submissions to the contest turned out to be my best (and I did not chose them for my final five)\nThe public scores are reach a peak and then slowly get worse at the end of the contest.\nIt is very difficult to see a relationship between any of these trends.\n\n\u00a0\nBelow, I\u2019ve replaced the choppy lines with smoothed lines to show the general trends.\n\nAn alternate plot of the submission scores over time using smoothers to depict some general trends.\nBased on my experience with past contests, I knew that the public leaderboard could not be trusted fully, and this is why I used cross-validation. I assumed that there was a stronger relationship between the cross-validation estimates and the private leaderboard than between the public and private leaderboard. Below, I\u2019ve created scatterplots to show the relationships between each pair of score types.\n\nScatterplots of three types of scores for each submission: 10-fold CV estimated score, public leaderboard score, and private or \u201ctrue\u201d score.\nThe scatterplots tell a different story. It turned out that my cross-validation estimates were not related to the private scores at all (notice the horizontal linear trends in those scatterplots), and the public leaderboard wasn\u2019t any better. \u00a0I already guessed that the public leaderboard would be a poor estimate of the true score, but why didn\u2019t cross-validation do any better?\nI suspect this is because as the competition went on, I began to use much more feature selection and preprocessing. However, I made the classic mistake in my cross-validation method by not including this in the cross-validation folds (for more on this mistake, see this short description\u00a0or section 7.10.2 in The Elements of Statistical Learning). \u00a0This lead to increasingly optimistic cross-validation estimates. I should have known better, but under such uncertainty, I fooled myself into accepting the most self-serving description of my current state. Even worse, I knew not to trust the public leaderboard, but when I started to edge towards to the top, I began to trust it again!\nLessons learned\nIn the end, my slow climb up the leaderboard was due mostly to luck. I chose my five final submissions based on cross-validation estimates, which turned out to be a poor predictor of true score. Ultimately, I did not include my best submissions in the final five, which would have brought me up to 33rd place \u2013 not all that much better than 52nd. All said, this was my most educational Kaggle contest yet. Here are some things I\u2019ll take into the next contest:\n\nIt is easier to overfit the public leaderboard than previously thought. Be more selective with submissions.\nOn a related note, perform cross-validation the right way: include all training (feature selection, preprocessing, etc.) in each fold.\nTry to ignore the public leaderboard, even when it is telling you nice things about yourself.\n\nSome sample code\nOne of my best submissions (average precision = .86294) was actually one of my own benchmarks that took very little thought. By stacking this with two other models (random forests and elastic net), I was able to get it up to .86334. Since the single model is pretty simple, I\u2019ve included the code. After imputing missing values in the training and test set with medians, I used the gbm package in R to fit a boosting model using every column in the data as predictor. The hyperparameters were not tuned at all, just some reasonable starting values. I used the internal cross-validation feature of gbm to choose the number of trees. The full code from start to finish is below:\n\n\nlibrary(gbm)\r\n\r\n# impute.NA is a little function that fills in NAs with either means or medians\r\nimpute.NA <- function(x, fill=\"mean\"){\r\n  if (fill==\"mean\")\r\n  {\r\n    x.complete <- ifelse(is.na(x), mean(x, na.rm=TRUE), x)\r\n  }\r\n\r\n  if (fill==\"median\")\r\n  {\r\n    x.complete <- ifelse(is.na(x), median(x, na.rm=TRUE), x)\r\n  }\r\n\r\n  return(x.complete)\r\n}\r\n\r\ndata <- read.table(\"Psychopath_Trainingset_v1.csv\", header=T, sep=\",\")\r\ntestdata <- read.table(\"Psychopath_Testset_v1.csv\", header=T, sep=\",\")\r\n\r\n# Median impute all missing values\r\n# Missing values are in columns 3-339\r\nfulldata <- apply(data[,3:339], 2, FUN=impute.NA, fill=\"median\")\r\ndata[,3:339] <- fulldata\r\n\r\nfulltestdata <- apply(testdata[,3:339], 2, FUN=impute.NA, fill=\"median\")\r\ntestdata[,3:339] <- fulltestdata\r\n\r\n# Fit a generalized boosting model\r\n\r\n# Create a formula that specifies that psychopathy is to be predicted using\r\n# all other variables (columns 3-339) in the dataframe\r\n\r\ngbm.psych.form <- as.formula(paste(\"psychopathy ~\",\r\n                                   paste(names(data)[c(3:339)], collapse=\" + \")))\r\n\r\n# Fit the model by supplying gbm with the formula from above.\r\n# Including the train.fraction and cv.folds argument will perform\r\n# cross-validation \r\n\r\ngbm.psych.bm.1 <- gbm(gbm.psych.form, n.trees=5000, data=data,\r\n                      distribution=\"gaussian\", interaction.depth=6,\r\n                      train.fraction=.8, cv.folds=5)\r\n\r\n# gbm.perf will return the optimal number of trees to use based on\r\n# cross-validation. Although I grew 5,000 trees, cross-validation suggests that\r\n# the optimal number of trees is about 4,332.\r\n\r\nbest.cv.iter <- gbm.perf(gbm.psych.bm.1, method=\"cv\") # 4332\r\n\r\n# Use the trained model to predict psychopathy from the test data. \r\n\r\ngbm.psych.1.preds <- predict(gbm.psych.bm.1, newdata=testdata, best.cv.iter)\r\n\r\n# Package it in a dataframe and write it to a .csv file for uploading.\r\n\r\ngbm.psych.1.bm.preds <- data.frame(cbind(myID=testdata$myID,\r\n                                         psychopathy=gbm.psych.1.preds))\r\n\r\nwrite.table(gbm.psych.1.bm.preds, \"gbmbm1.csv\", sep=\",\", row.names=FALSE)\n\n\nCreated by Pretty R at inside-R.org\n"}, {"url": "http://blog.kaggle.com/2012/07/05/1st-place-interview-for-boehringer-ingelheim-biological-response/", "link_text": "1st place interview for Boehringer Ingelheim Biological Response", "id": 33, "title": "Permalink to 1st place interview for Boehringer Ingelheim Biological Response", "date": "2012-07-05", "text": "\n3 top competitors, who met during Kaggle's first ever private competition, teamed up to win the public Boehringer Ingelheim Predicting a Biological Response competition.\u00a0 Team 'Winter is Coming' ( Jeremy Achin and Tom DeGodoy, props for the name) joined forces with Sergey Yurgenson, exchanging 349 emails over 45 days, to build their winning bioresponse model.\n\nWhat was your background prior to entering this challenge?\nTom and I met while we were both studying Math and Physics at the University of Massachusetts at Lowell and have been friends and colleagues ever since. We both have 7+ years experience doing applied predictive analytics. Most recently we were both Directors of Research and Development at Travelers Insurance (Tom on the Business Insurance side and me on the Personal Insurance side). A couple months ago, we decided to quit our jobs to start our own Data Science company. We plan to use winnings from private Kaggle competitions as our initial source of seed funding.\nWe met Sergey while competing against him in Kaggle\u2019s first ever invite-only private competition. He has a background in Physics and currently works for Harvard Medical School. The Boehringer Ingelheim contest was his 8th Kaggle competition.\nI think it's safe to say that all 3 of us are obsessed with Data Science to an extent that I\u2019m not sure is healthy.\nWhat made you decide to enter?\nTom and I were looking for our next competition and heard that Sergey was looking for teammates for this problem. We thought this contest had the potential to be big in terms of the quantity and quality of competitors (which definitely turned out to be the case). Sergey\u2019s reason for entering the competition is analogous to the reason fish \u201cdecide\u201d to swim--it\u2019s required for survival.   In Sergey\u2019s words, he \u201cjust cannot stop competing.\u201d\nWhat preprocessing and supervised learning methods did you use?\nWe used Random Forests for feature ranking & selection. Methodologies explored for various roles included Random Forests, GBM, KNN, Neural Nets, Elastic Net, GLM, GAM, and SVM. Simple transformations & splines were used for some models.\nWhat was your most important insight into the data?\nI would say that the most important insight into the data was obtaining an accurate ranking of the relative importance of the variables. Eliminating variables reduced model training time (allowing us to try more things) and improved performance considerably.\nAnother important insight was recognizing the danger of overfitting inherent in this problem. This led us to design a testing framework that helped to ensure we didn\u2019t overfit the public leaderboard.\nWere you surprised by any of your insights?\nBy plotting the data points using the 2nd and 3rd principal components, you can see 4 very separated clusters of points. We thought this was going to be a very important finding, but it didn\u2019t turn out to help us.\nWhich tools did you use?\nR & Matlab. Rstudio Server is awesome if you are using multiple computers--I love having a browser open with many tabs each interfacing with a different RStudio Server.\nWhat have you taken away from this competition?\nFirst and foremost, having a team with diversity and relentless tenacity is extremely important. I\u2019ll quote Shea Parkes because I couldn\u2019t possibly phrase it any better: \u201cIt\u2019s quite obvious how much an ensemble of viewpoints can contribute above and beyond an ensemble of algorithms.\u201d\nThe ability to collaborate effectively is critical. We exchanged 349 emails over the 45 days that we worked on the competition, and we were in sync enough to be able to share, test, and improve on each other\u2019s work. At one point, Sergey took over 17,000 files we generated using R and combined them with his own results in Matlab allowing us to reliably test out many different blends.\nAlso, if you are going to run long jobs (20+ hours), make sure to put a sign up in your house to remind people not to plug an iron into the same circuit your computers are on. I went into shock when all of a sudden the monitors went black and the computers went silent. It was like someone yanked the Matrix data probe out of the back of my head.\n\n"}, {"url": "http://blog.kaggle.com/2012/07/03/bond-fire-of-the-data-scientists-interview-with-benchmark-challenge-3rd-place-finishers/", "link_text": "Bond-fire of the Data Scientists - Interview with Benchmark Challenge 3rd place Finishers", "id": 34, "title": "Permalink to Bond-fire of the Data Scientists - Interview with Benchmark Challenge 3rd place Finishers", "date": "2012-07-03", "text": "\nBefore we dive in to the slew of interviews with the winners of the many recently finished contests, we take a sec to catch up with Vik P. and Sergey\u00a0 E., the 3rd place team from the Benchmark Bond Trade Price Challenge back in May.\u00a0 What do you get when you combine an American diplomat with a Russian physicist?\u00a0 Read more to find out.\nWhat was your background prior to entering this challenge?\nVik: I have a bit of a strange background for a Kaggle participant. I was actually a member of the U.S. Foreign Service 8 months ago, and was serving as a diplomat in South America. I have also worked in operations management, and have a degree in American History.\nSergey: I graduated from Moscow Institute of Physics and Technology in 2005 with MS degree in applied physics and mathematics. Next years I\u2019ve been working as a senior software engineer in several different companies in Russia. I\u2019ve always been interested in AI and it\u2019s application for different type of real life problems.\n\nWhat made you decide to enter?\nVik: I had previously entered the algorithmic trading challenge on Kaggle, and was eager to apply some of what I had learned to a problem with some clear similarities. After I entered, the fact that the data had a lot of angles and a lot of potential for score improvement kept me motivated.\nSergey: I\u2019ve been studying machine learning for a while and wanted to test my skills. This is my first kaggle contest and I really enjoyed it.\nWhat preprocessing and supervised learning methods did you use?\nVik: I actually did not do any real preprocessing. The missing values for some of the previous trades could have been dealt with using preprocessing, but I dealt with them in feature extraction by using average values over the whole 10 trade range and the most recent 5 trade range.\nAs far as the learning algorithms, I used 2 slightly different random forest models, which I blended using stacked generalization. Due to time constraints, which prevented me from fully exploring parameter tuning, I was unable to get good results when I blended in GBM and linear models.\nSergey: I\u2019ve constructed a lot of features manually. The most important idea was data normalization. I tried to get rid of absolute values such as dollar prices. Then I simply used random forest implementation in R to generate predictions.\nWhat was your most important insight into the data?\nVik: My most important insight was probably how irregular the distribution of the target variable is. Various normalization methods really helped in terms of producing a stronger model. The maximum size of the terminal nodes for the random forest was also an important parameter. As a lot of the rows were related due to how the time series data was constructed, setting it too low meant that extremely similar trades were split into different nodes, which actually seemed to reduce accuracy.\nSergey: It was all about how good you construct your features. I spend too much time re-implementingrandom forest in Java specifically for this problem but it didn\u2019t give me any advantage. While some simple features build in a couple of minutes lead to significant improvement.\nWere you surprised by any of your insights?\nVik: I was very surprised by how different the prices were when dealers dealt with each other versus when dealers dealt with customers. The type of trade (dealer to dealer versus customer buy versus customer sell) was actually the single most important variable to the random forest. Although it certainly is to be expected, seeing it illustrated so starkly was interesting.\nSergey: Then I split data by trade_type and is_callable I got unexpectedly big gain in prediction performance.\nWhich tools did you use?\nVik: I solely used R.\nSergey: R for prediction and Java for feature extraction.\nWhat have you taken away from this competition?\nVik: The key takeaway that I took from this competition was to not get bogged down with any one idea for too long. These competitions take a lot of creativity, and with that creativity can come fixation on one concept, and the need to get it to work. In the algorithmic trading competition that I participated in previously, I got fixated on the idea of using a linear model, and never really wavered from that fixation. Here, I iterated through a lot of possibilities, used what worked, and didn\u2019t get stuck in the minutiae.\nSergey: Being in team is of crucial importance. Wish we joined our efforts a bit earlier.\nphoto by kate hiscock\n"}, {"url": "http://blog.kaggle.com/2012/07/02/up-and-running-with-python-my-first-kaggle-entry/", "link_text": "Up And Running With Python - My First Kaggle Entry", "id": 35, "title": "Permalink to Up And Running With Python - My First Kaggle Entry", "date": "2012-07-02", "text": "\nAbout two months ago I joined Kaggle as product manager, and was immediately given a hard time by just about everyone because I hadn't ever made a real submission to a Kaggle competition. I had submitted benchmarks, sure, but I hadn't really competed. Suddenly, I had the chance to not only geek out on cool data science stuff, but to do it alongside the awesome machine learning and data experts in our company and community. But where to start? I didn't have much of a stats background beyond a few undergraduate courses, and I had never done any machine learning. Once a week our data science team gets together and discusses the latest academic machine learning paper, but that certainly wasn't the straightest path to getting me up and running in a Kaggle competition.\n\nAs I was reading about random forests on Wikipedia (which I knew to be a favorite tool of our users), I started thinking about how I thought I might be able to do this - I could implement a simple tree structure, code up the Gini coefficient, aggregate the results, etc, etc. Heady stuff. And, it turns out, wholly unnecessary stuff.\nFor anyone else who, like me, might have some programming ability but doesn't have a clue about machine learning, I have good news: Someone has already done most of the hard work. With some of the machine learning libraries out there it is really very easy to get started (no Gini coefficient-building required). I'm hoping to make it even a bit easier with this post, by pulling together all the steps to get you up and running with Python, and competing in Kaggle competitions (although I feel obligated to note that while I did create a few plausible entries to a competition, I was not exactly vying for first place).\nThis tutorial assumes some knowledge of Python and programming, but no knowledge whatsoever of data science, machine learning, or predictive modeling (or, heck, even statistics). To the extent there is a target audience, it's probably hacker types who learn best by doing.\nAll the code from this tutorial is available on\u00a0github.\nYou might encounter terms you're not familiar with, but that shouldn't stop you from completing the tutorial. By the end, you won't know a heck of a lot more about data science\u00a0per se, but you'll have a nice environment set up where you can easily play with lots of different data science tools and even make credible entries to Kaggle competitions. Most importantly you'll be in a great position to experiment and learn more data science.\nHere's what you'll learn:\n\nHow to install popular scientific and statistical computing libraries for Python\nUse those libraries to create a benchmark predictive model and submit it to a competition.\nWrite your own evaluation function, and learn how to use cross-validation to test out ideas on your own.\n\nExcited? I thought so! So let's get going.\nEnvironment Setup\nFirst thing, we'll need a Python environment suitable for scientific and statistical computing. Assuming you already have Python installed (no? Well then\u00a0get it! Python 2.7 is recommended <insert snarky Python 3 remark>), we'll need three packages. You should install each in the order they appear here:\n1.\u00a0numpy\u00a0- (pronounced\u00a0num-pie) Powerful numerical arrays. A foundational package for the two packages below.\n2.\u00a0scipy\u00a0- (sigh-pie) Scientific, mathematical, and engineering package\n3.\u00a0scikit-learn\u00a0- Easy to use machine learning library\nNote: 64 bit versions of these libraries can be found\u00a0here.\nClick through the links above for the home pages of each project and get the installation for your operating system or, if you're running Linux, you can install from a package manager (pip). If you're on a Windows machine, it's easiest to install using the setup executables for scipy and scikit-learn rather than installing from a package manager.\nI'd also highly recommend to setting up a decent Python development environment. You can certainly execute Python scripts from the command line, but it's a heck of a lot easier to use a proper environment with debugging support. I use\u00a0PyDev, but even something like IPython is better than nothing.\nNow you're ready for machine learning greatness!\nYour First Submission\nThe\u00a0Biological Response\u00a0competition provides a great data set to get started with because the value to be predicted is a simple binary classifier (0 or 1) and the data is just a bunch of numbers, so feature\u00a0extraction\u00a0and\u00a0selection\u00a0aren't as important as in some other Kaggle competitions.\u00a0Download\u00a0the training and test data sets now. Even though this competition is over, you can still make submissions and see how you compare to the world's best data scientists.\nIn the code below, we'll use an ensemble classifier called a\u00a0random forest\u00a0that often performs very well as-is, without much babysitting and parameter-tweaking. Although a random forest is actually a pretty sophisticated classifier, it's a piece of cake to get up and running with one thanks to sklearn.\nRemember: You don't have to understand all of the underlying mathematics to use these techniques. Experimentation is a great way to start getting a feel for how this stuff works. Understanding the models is important, but it's not necessary to get started, have fun, and compete.\nHere's the code:\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom numpy import genfromtxt, savetxt\r\n\r\ndef main():\r\n    #create the training & test sets, skipping the header row with [1:]\r\n    dataset = genfromtxt(open('Data/train.csv','r'), delimiter=',', dtype='f8')[1:]\r\n    target = [x[0] for x in dataset]\r\n    train = [x[1:] for x in dataset]\r\n    test = genfromtxt(open('Data/test.csv','r'), delimiter=',', dtype='f8')[1:]\r\n\r\n    #create and train the random forest\r\n    #multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)\r\n    rf = RandomForestClassifier(n_estimators=100)\r\n    rf.fit(train, target)\r\n    predicted_probs = [x[1] for x in rf.predict_proba(test)]\r\n\r\n    savetxt('Data/submission.csv', predicted_probs, delimiter=',', fmt='%f')\r\n\r\nif __name__==\"__main__\":\r\n    main()\nAt this point you should go ahead and\u00a0actually get this running\u00a0by plopping the into a new Python script and saving it as makeSubmission.py. You should now have directories and files on your computer like this:\n \"My Kaggle Folder\"\r\n |\r\n |---\"Data\"\r\n |    |\r\n |    |---train.csv\r\n |    |\r\n |    |---test.csv\r\n |\r\n |---makeSubmission.py\nOnce you've run makeSubmission, you'll also have my_first_submission.csv in your data folder. Now it is time to do something very important:\nSubmit this file to the bio-response competition\nDid you do it? You did it! Great! You could stop here - you know how to make a successful Kaggle entry - but if you're the curious type, I'm sure you'll want to play around with other types of models, different parameters, and other shiny things. Keep reading to learn an important technique that will let you test models locally, without burning through your daily Kaggle submission limit.\nEvaluation and Cross-Validation\nLet's say we wanted to try out sklearn's\u00a0gradient boosting machine\u00a0instead of a random forest. Or maybe some simple\u00a0linear models. It's easy enough to important these things from sklearn and generate submission files, but it's not so easy to compare their performance. It's not practical to upload a new submission every time we make a change to the model - we'll need a way to test things out locally, and we'll need two things in order to do that:\n1. An evaluation function\n2. Cross validation\nYou'll always need some kind of evaluation function to determine how your models are performing. Ideally, these would be identical to the evaluation metric that Kaggle is using to score your entry. Competition participants often post evaluation code in the forums, and Kaggle has detailed descriptions of the metrics available on the\u00a0wiki. In the case of the bio-response competition, the evaluation metric is\u00a0log-loss\u00a0and user Grunthus has\u00a0posted\u00a0a Python version of it. We won't spend too much time on this (read the forum post for more information), but go ahead and save the following into your working directory as logloss.py.\nimport scipy as sp\r\ndef llfun(act, pred):\r\n    epsilon = 1e-15\r\n    pred = sp.maximum(epsilon, pred)\r\n    pred = sp.minimum(1-epsilon, pred)\r\n    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\r\n    ll = ll * -1.0/len(act)\r\n    return ll\nFinally, we'll need data to test our models against. When you submitted your first Kaggle competition entry earlier in this tutorial, Kaggle compared (using log-loss) your answers to the actual real world results (the \"ground truth\") associated with the test data set. Without access to those answers, how can we actually test our models locally? Cross-validation to the rescue! Cross-validation is a simple technique that basically grabs a chunk of the training data and holds it in reserve while the model is trained on the remainder of the data set. In case you haven't realized it yet, sklearn it totally awesome and is here to help. It has built in tools to generate cross validation sets. The sklearn\u00a0documentation\u00a0has a lot of great information on cross-validation. The code below creates 10 cross-validation sets (called\u00a0folds), each with 10% of the training data set held in reserve, and tests our random forest model against that withheld data.\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn import cross_validation\r\nimport logloss\r\nimport numpy as np\r\n\r\ndef main():\r\n    #read in data, parse into training and target sets\r\n    dataset = np.genfromtxt(open('Data/train.csv','r'), delimiter=',', dtype='f8')[1:]\r\n    target = np.array([x[0] for x in dataset])\r\n    train = np.array([x[1:] for x in dataset])\r\n\r\n    #In this case we'll use a random forest, but this could be any classifier\r\n    cfr = RandomForestClassifier(n_estimators=100)\r\n\r\n    #Simple K-Fold cross validation. 5 folds.\r\n    cv = cross_validation.KFold(len(train), k=5, indices=False)\r\n\r\n    #iterate through the training and test cross validation segments and\r\n    #run the classifier on each one, aggregating the results into a list\r\n    results = []\r\n    for traincv, testcv in cv:\r\n    probas = cfr.fit(train[traincv], target[traincv]).predict_proba(train[testcv])\r\n    results.append( logloss.llfun(target[testcv], [x[1] for x in probas]) )\r\n\r\n    #print out the mean of the cross-validated results\r\n    print \"Results: \" + str( np.array(results).mean() )\r\n\r\nif __name__==\"__main__\":\r\n    main()\nNote that your cross-validated results might not exactly match the score Kaggle gives you on the model. This could be for a variety of (legitimate) reasons: random forests have a random component and won't yield identical results every time; the actual test set might deviate from the training set (especially when the sample size is fairly low, like in the bio-response competition); the evaluation implementation might differ slightly. Even if you are getting slightly different results, you can compare model performance locally and you should know when you have made an improvement.\nThat's a wrap!\nThat's it! We're done! You now have great tools at your disposal, and I expect to see you at the top of the leaderboard in no time!\nphoto by Roberto Verso\n"}, {"url": "http://blog.kaggle.com/2012/05/13/asap-interview-with-martin-oleary/", "link_text": "ASAP interview with Martin O'Leary", "id": 36, "title": "Permalink to ASAP interview with Martin O'Leary", "date": "2012-05-13", "text": "\nFor the first of our interviews with top finishers in the Hewlett Automated Essay Scoring Challenge, we catch up with 6th place finisher and polymath Martin O'Leary (@mewo2).\u00a0 You can also check out his blog at\u00a0 http://mewo2.github.com/\nWhat was your background prior to entering this challenge?\nI'm a mathematician turned glaciologist, working as a research fellow at the University of Michigan. I've been involved with Kaggle for about a year now, and have had a few good finishes. I have a habit of doing well in the early part of competitions, which has got me some publicity, but doesn't translate well into final results.\nI've always had an interest in linguistics (at one point I considered it as a career), but this was the most serious text mining I've ever done.\n\nWhat made you decide to enter?\nMomchil Georgiev. He approached me early on about possibly collaborating, and we decided to produce individual entries first. Somehow we never got around to teaming up, and by the end he'd assembled a big enough team that I decided I'd rather try for a solo run than try to merge. I feel a little bit like Pete Best, who left the Beatles before they became famous.\nMore seriously, I liked the problem because it's an interesting dataset, and a problem which comes down to a lot more than just number-crunching.\nWhat preprocessing and supervised learning methods did you use?\nA lot of the difficulty in this problem was in finding meaningful features in the essays. I spent a lot of time on topic modelling, and looking at distributions of syntactic features. For the final prediction, I used a fairly large ensemble of different methods. Some of the essay sets worked better with boosted approaches, while others were more susceptible to neural nets.\nWhat was your most important insight into the data?\nThe choice of error metric is really important! Most algorithms are tuned to a particular notion of error, and it helps a lot to tweak things so that you're actually optimising for your target metric. In this case that meant some customisation, as the quadratic kappa used is a little unusual.\nWere you surprised by any of your insights?\nI was quite surprised how little measures of spelling and grammar \"correctness\" mattered. Except in one case where the grading rubric explicitly mentioned it, they didn't seem to matter much at all. It warms my descriptivist heart to see that teachers are grading on more than just who can use a spellchecker and a semicolon.\nWhich tools did you use?\nI started out using just R, but introduced Python fairly quickly because of its stronger NLP libraries. There's a good reason that NLTK is popular. I recycled a lot of old R code for various tasks, and used a mixture of custom and pre-packaged models.\nWhat have you taken away from this competition?\nThe benefits of multiple approaches. I think the winning teams did so well because they were able to combine several independently created models. Also, you can't take a month off from a competition and expect to still be winning when you get back.\n"}, {"url": "http://blog.kaggle.com/2012/05/09/speaking-in-hands-winner-of-round-1-of-the-chalearn-kinect-gesture-challenge/", "link_text": "Speaking in Hands:  Winner of Round 1 of the CHALEARN Kinect Gesture Challenge", "id": 37, "title": "Permalink to Speaking in Hands:  Winner of Round 1 of the CHALEARN Kinect Gesture Challenge", "date": "2012-05-09", "text": "\nWe catch up with Alfonso Nieto-Castanon, the winner of Round 1 of the CHALEARN Gesture Challenge.\u00a0 This fascinating series of 4 competitions revolves around gesture and sign language recognition using a Microsoft Kinect camera.\u00a0\u00a0 A must-read for anyone planning to throw their hat in the ring for CHALEARN Round 2.\nWhat was your background prior to entering this challenge?\nMy background is on computational neuroscience (Ph.D. Cognitive and Neural Systems, Boston University) and engineering (B.S./M.S. Telecommunication Engineering, Universidad de Valladolid). I work freelance as a research consultant and my latest projects range from development of functional connectivity MRI software and analysis methods, to brain computer interfaces for speech restoration in subjects with locked-in syndrome.\nWhat made you decide to enter?\nThe Chalearn dataset and goals were too interesting to pass up. I just had to give it a try.\n\nWhat preprocessing and supervised learning methods did you use?\nI did not implement any learning strategy but used instead a combination of ad hoc features from the depth videos (somewhat inspired by neural processes in the visual system) with a Bayesian network model for recognition.\nWhat was your most important insight into the data?\nThinking of gestures as a form of communication, and realizing that the subjects in those videos were already doing what they thought would work best in order for us to interpret and recognize those gestures correctly. I imagined that a system that would mimic the specificities of the human visual system would be most likely to pick up those helpful cues from the video sequences correctly.\nWhich tools did you use?\nI used Matlab (just the matlab base set, no specific toolboxes other than the nice set of functions provided by the contest organizers to browse the data and create a sample submission)\nWhat have you taken away from this competition?\nI enjoy developing problem-specific algorithms rather than using a combination of off-the-shelf procedures. This contest gave me the chance to do just that while working in one of those (few) areas where humans still outperform machines (and\u00a0I am curious to see if we can further bridge that gap!)\n"}, {"url": "http://blog.kaggle.com/2012/05/04/petterson-takes-home-the-emc-data-science-global-hackathon-prize/", "link_text": "Petterson takes home the EMC Data Science Global Hackathon Prize", "id": 38, "title": "Permalink to Petterson takes home the EMC Data Science Global Hackathon Prize", "date": "2012-05-04", "text": "\nThe EMC Data Science Global Hackathon prize was awarded to James Petterson.\u00a0 Check out his webpage for a more detailed description and the source code: http://users.cecs.anu.edu.au/~jpetterson/\nWhat was your background prior to entering this challenge?\nI am currently finishing my PhD in machine learning at ANU. Before that I worked as a software engineer for the telecom industry for many years.\nWhat made you decide to enter?\nThe challenge of kaggle competitions always attracted me - I took part in two other ones in the past (What Do You Know and Heritage Health Prize). I was abstaining from entering new ones as I know how time consuming this can be, but when I heard about this 24h one I couldn't resist.\nWhat preprocessing and supervised learning methods did you use?\nI computed a set of training instances based on:\n\n- mean of all variables for each prediction time\n- mean of all variables for each prediction time and chunkID\n- most recent value of all variables for each chunkID\n\nI did some bootstrapping to increase the size and variety of the training data, using a 24-hour moving window. I then trained 390 Generalised Boosted Regression models, one for each combination of target variable and prediction time.\nWhat was your most important insight into the data?\nI didn't spent much time looking at the data, so I can't think of any particular insight.\nWere you surprised by any of your insights?\nI was surprised that I had a good result without spending much time trying to understand the data. I suspect that wouldn't be the case in a longer competition, though.\nWhich tools did you use?\nOnly R.\nWhat have you taken away from this competition?\nI saw once again how powerful boosting methods are. Even though this was essentially a time series problem, a standard boosting regression method performed quite well.\nWhat did you think of the 24-hour hackathon format?\nNormally competitions take 3 months or more, which tends to favour those that can spend more time on them. The 24-hour format was great in the sense that it gave a chance to those that are more time constrained. And, of course, it was a lot of fun!\nI hope we will have more of these in the future.\nPhoto by\u00a0ninahale\n"}, {"url": "http://blog.kaggle.com/2012/05/03/1st-place-in-arabic-writer-indentification-challenge/", "link_text": "1st place interview for Arabic Writer Identification Challenge", "id": 39, "title": "Permalink to 1st place interview for Arabic Writer Identification Challenge", "date": "2012-05-03", "text": "\nWayne Zhang, the winner of the ICFHR 2012 - Arabic Writer Identification Competition shares his thoughts on pushing for the frontiers in hand-writing recognition.\nWhat was your background prior to entering this challenge?\nI'm pursuing my PhD in pattern recognition and machine learning. I have interests in many problems of this field, such as classification, clustering, semi-supervised learning and generative models.\nWhat made you decide to enter?\nTo test my knowledge on real-world problems, to compete with smart people, and to contribute in real-life prediction tasks.\nWhat preprocessing and supervised learning methods did you use?\nI used the provided features. The writer identification problem is a multi-class classification problem, and linear discriminant analysis is suitable for this task.\nWhat was your most important insight into the data?\nBoth the training and test set are of a small size, I had to be careful about the generalization ability of the model.\nWhich tools did you use?\nI used LDA, which was popular and successful in face recognition ten years ago. It appeared to have surprisingly good results on writer identification, possibly because the two tasks are similar. I implemented my code in Matlab, because of its superior matrix computation support.\nWhat have you taken away from this competition?\nTo work on real-world problems, you had to be careful about the overfitting problem. It is different from academic research. In real problems we need to consider many details to make a perfect system. One challenge of Kaggle competitions is that the discrepancy between the public and private scores. It makes me consider more about what the situation will be like in real world. You always have limited training data and validation data, but the test data usually are unbounded.\u00a0 How to generalize your model to the unbounded data could be a problem.\n"}, {"url": "http://blog.kaggle.com/2012/05/01/chucking-everything-into-a-random-forest-ben-hamner-on-winning-the-air-quality-prediction-hackathon/", "link_text": "Chucking everything into a Random Forest: Ben Hamner on Winning The Air Quality Prediction Hackathon", "id": 40, "title": "Permalink to Chucking everything into a Random Forest: Ben Hamner on Winning The Air Quality Prediction Hackathon", "date": "2012-05-01", "text": "\nWe catch up with Ben Hamner, a data scientist at Kaggle, after he won Kaggle's Air Quality Prediction Hackathon. As a Kaggle employee, he is ineligible for prizes.\nWhat was your background prior to entering this challenge?\nI graduated from Duke University in 2010 with a bachelors in biomedical engineering, electrical and computer engineering, and mathematics. For the next year, I applied machine learning to improve non-invasive brain-computer interfaces as a Whitaker Fellow at EPFL. On the side, I participated in or won a number of machine learning competitions. Since November 2011, I have designed and structured a variety of competitions as a Kaggle data scientist.\nWhat made you decide to enter?\nI was hanging out at Splunk (one of the SF venues hosting the hackathon). Anthony asked me some questions about extracting features from the data, which prompted me to open it up and look at it in the afternoon.\nWhat preprocessing and supervised learning methods did you use?\nI took the lagging N components from the full time series (N=8 for the winning submission, which was selected arbitrarily) as features, then each of the 10 prediction times and 39 pollutant measures as targets. I then trained 390 Random Forests over the entire training data, one for each predicted offset time-pollutant combination. The Random Forest parameters were selected so that the models would be quick to train. The code for creating the winning model is available here.\nSome straightforward approaches to improving this model include\n\nOptimizing the parameters for model performance as opposed to training time.\nDirectly optimizing for the error metric (mean absolute error) instead of RMSE.\nUsing a data-driven approach to select the number of previous time points to include.\n\nWhat was your most important insight into the data?\nI don\u2019t believe I had any specific insights on the data - I barely looked at it before training the model.\nWere you surprised by any of your insights?\nI was surprised that domain insight wasn\u2019t necessary to win the hackathon. Key insights have been crucial in many of our longer-running competitions.\nWhich tools did you use?\nOnce I decided to fiddle with the data, I asked David (a fellow Kaggle data scientist) to pick a random number between one and three. He picked two, and I used MATLAB. (If he said one I would have used R, and three would have been Python).\nWhat have you taken away from this competition?\nTaking all the features and chucking them into a Random Forest works surprisingly well on a variety of real-world problems. This is demonstrated more empirically in this paper. I'm very interested in domains such as CV and NLP where this doesn't hold true, or where the problem can't be simply formulated in the standard supervised machine learning framework.\nWhat did you think of the 24 hour hackathon format?\nIt was a lot of fun! I especially enjoyed seeing Kagglers in venues all over the world collaborating and competing on this problem. I'm curious to see how much better the results would be if we ran this as a standard competition over a couple months, and whether the work in the first day would comprise the majority of the improvement over the benchmark.\n"}, {"url": "http://blog.kaggle.com/2012/04/29/on-diffusion-kernels-histograms-and-arabic-writer-identification/", "link_text": "On Diffusion Kernels, Histograms, and Arabic Writer Identification", "id": 41, "title": "Permalink to On Diffusion Kernels, Histograms, and Arabic Writer Identification", "date": "2012-04-29", "text": "\nWe catch up with Yanir Seroussi, a graduate student in Computer Science, on how he took third place in the ICFHR 2012 - Arabic Writer Identification Competition.\u00a0 After signing up for a Kaggle account over a year ago, he finally decided to give one of the competitions 'just a quick try'.\u00a0 Famous last words...\nWhat was your background prior to entering this challenge?\nI'm currently in the final phases of my PhD, which is in the areas of natural language processing and user modelling. Even though I address some predictive modelling problems in my thesis, I've never done any image processing work, though it did help to have some background knowledge in machine learning and statistics.\nWhat made you decide to enter?\nI signed up to Kaggle over a year ago but never used my account. Recently, I started thinking about what I want to do once I graduate, and somehow bumped into Phil Brierley's blog. This inspired me to give one of the smaller competitions \"just a quick try\", which ended up consuming a lot of my free time...\nWhat preprocessing and supervised learning methods did you use?\nMy\u00a0most successful submission was based on SVMs. I joined the competition quite late and didn't have time to play with blending techniques, which seem to be a key component of many winners' approaches.\nAs to preprocessing, I don't have any prior knowledge about image processing, so I only briefly experimented with one idea that didn't require much knowledge: converting all the images to texts with freely-available OCR software, which is based on the idea that the same OCR errors would appear for the same writers. While using this as a standalone feature yielded some interesting results, it didn't improve accuracy when used in conjunction with the provided features.\nWhat was your most important insight into the data?\nAt first I played around with SVMs and the commonly-used kernels (linear, polynomial, RBF and sigmoid). Then I remembered a recent paper that was presented at the ACL conference, about using character histograms for authorship attribution (http://aclweb.org/anthology-new/P/P11/P11-1030.pdf). Since the provided features were given in the form of histograms, I figured that the same techniques would be applicable here. And indeed, using SVMs with a diffusion kernel proved to yield the most significant performance boost.\nWere you surprised by any of your insights?\nI wouldn't call it \"surprised\", but I was a bit frustrated by the apparent lack of correlation between cross-validation results on the training data and the accuracy on the validation set. This is probably because each of the 204 writers had only two training paragraphs (all containing the same text), while the test instances were a third paragraph with different content. So any form of cross validation yielded a training subset that was very different from the full training set, and a test subset that obviously couldn't contain the \"third paragraphs\".\nWhich tools did you use?\nMostly LibSVM and SVMLight (for some brief experiments with transductive SVMs that didn't go well). I used Python to parse the feature files, run the libraries, and produce the final results.\nWhat have you taken away from this competition?\nA better understanding of SVMs and the conclusion that I still have a lot to learn  \n"}, {"url": "http://blog.kaggle.com/2012/04/20/viva-libfm-steffen-rendle-on-how-he-won-the-grockit-challenge/", "link_text": "Viva libFM - Steffen Rendle on how he won the Grockit Challenge", "id": 42, "title": "Permalink to Viva libFM - Steffen Rendle on how he won the Grockit Challenge", "date": "2012-04-20", "text": "\nGrockit competition winner, Steffen Rendle, shares his Factorization Machine technique.\u00a0 In his own words, \"The combination of FMs and Bayesian learning was very handy as I didn't had to search for any regularization hyperparameters.\"\nWhat was your background prior to entering this challenge?\nI am an assistant professor in computer science at the University of Konstanz.\nWhat made you decide to enter?\nI wanted to study factorization machines on a competitive setting and get some empirical evidence that they work well. The Grockit challenge raised my interest because the dataset is of reasonable size (not too small) and has interesting variables.\nWhat preprocessing and supervised learning methods did you use?\nI used factorization machines (FM) which is a model class that I have developed over the last years. The model allows one to use feature engineering the same way as most standard machine learning tools (e.g. linear regression or SVMs) but it uses factorized variable interactions. For learning, I extended the Bayesian approach that was developed by Christoph Freudenthaler and me with classification capabilities.\nI used a simple preprocessing that removes cases (student-question-combinations) that appear very often.\nWhat was your most important insight into the data?\nThe most important predictors were the student, question and the time taken for answering a question. I also derived a variable that states how often a student has tried to answer a question before. Besides this I only used the subtrack name and the game type. All other variables didn't help -- at least not in my experiments.\nWere you surprised by any of your insights?\nEnsembles didn't work at all. I couldn't find any improvement by ensembling several models -- neither on my validation set, the public or private leaderboard. I didn't expect large improvements as I tried to have strong single models, however I was surprised to find no improvement at all.\nWhich tools did you use?\nI used libFM which is an implementation of factorization machines. The software is written in C++ and can be obtained from my website http://www.libfm.org/\nFor preprocessing, I used Perl.\nWhat have you taken away from this competition?\nI think the format of machine learning challenges gives very valuable feedback to researchers. When conducting experiments for a research paper, one cannot compare to so many different approaches as you have in a Kaggle competition.\n"}, {"url": "http://blog.kaggle.com/2012/04/12/grockit-2nd-place-interview-with-alexander-dyakonov/", "link_text": "Grockit 2nd place interview with Alexander D'yakonov", "id": 43, "title": "Permalink to Grockit 2nd place interview with Alexander D'yakonov", "date": "2012-04-12", "text": "\nWe caught up with all time top-ranked Kaggle competitor, Alexander D'yakonov, on his experience with the Grockit \"What Do You Know\" Competition.\nWhat was your background prior to entering this challenge?\nI\u2019m an Associate Professor at Moscow State University. Participating in Kaggle challenges is giving me a lot of valuable experience. I write popular scientific lectures about data mining.\u00a0 In the lectures I tell about my experiences. For example,\u00a0 Introduction to Data Mining\u00a0 and Tricks in Data Mining (both in Russian).\nWhat made you decide to enter?\nIn the last three competitions, I took the first, third and fourth places.\u00a0 Therefore I looked for a competition to take the second place.   And I found it!\nWhat preprocessing and supervised learning methods did you use?\nMy approach was to reduce this problem to a standard classification problem. I generated feature description of every pair \u201cstudent \u2013 question\u201d. I used pairs from valid_test.csv for tuning the algorithms. Here are some examples of features: an average student score, an average student score today, his time of the answering, the weighed average score (with different weighted schemes), the question difficulty, the question difficulty today, etc. There were also some features from SVD. I also added some linear combinations of the features (which increased performance). I blended GBMs (from R), GLM (from MATLAB) and neural nets (from CLOP library in MATLAB).\nWhat was your most important insight into the data?\nNothing, I solved it as a standard classification problem and did not look at the data.\nWere you surprised by any of your insights?\nI was surprised that Random Forests were essentially worse than GBMs and didn't increase performance in blending.\nWhich tools did you use?\nR and MATLAB (with CLOP library)\nWhat have you taken away from this competition?\nI really liked the winner\u2019s method. And I should admit that the method is more effective than my method. But when I solved the problem, I checked a hypothesis that it could be solved as a usual classification problem. I think that my hypothesis has proved to be true.\n"}, {"url": "http://blog.kaggle.com/2012/04/10/grocking-out-3rd-place-interview-with-pankaj-mishra/", "link_text": "Grocking Out - 3rd place interview with Pankaj Mishra", "id": 44, "title": "Permalink to Grocking Out - 3rd place interview with Pankaj Mishra", "date": "2012-04-10", "text": "\nThis week we catch up with the winners of the Grockit 'What Do You Know?' Competition, which ended on Feb 29th. \u00a0The challenge was to predict which review questions a student would answer correctly when studying for the GMAT, SAT or ACT. Pankaj Mishra placed 3rd, in his first ever Kaggle competition, and offers some great tips for how to get started.\nWhat was your background prior to entering this challenge?\nI am a Software Developer with an undergraduate degree in Aeronautics. I learned machine learning from the free Stanford Machine Learning class at ml-class.org and the AI class at ai-class.com. Big thanks to Andrew Ng, Sebastian Thrun, and Peter Norvig for teaching those classes so well!\nWhat made you decide to enter?\nI participated to gain experience in machine learning. I was excited to get access to high quality real-world data from Grockit and use it to solve a concrete problem.\nWhat preprocessing and supervised learning methods did you use?\nPreprocessing:\nI used Java to create a training file with one row (training example) for each user who answered five or more questions. Each row corresponds to the last question for a user. I added many columns to capture fraction-of-correct-answers-by-user, question-difficulty, etc.\nSupervised learning methods:\nMy solution is a mixture of a Neural Network ensemble and a Gradient Boosting Machine (GBM) ensemble. Both of them were trained on the same training data. The training data included columns that themselves were predicted values from other models such as various Collaborative Filtering models, IRT Model, Rasch Model, and the LMER benchmark. This approach to blending is inspired by the paper \"Collaborative Filtering Applied to Educational Data Mining\"\u00a0 [see Section(3): Blending].\u00a0 None of my individual models scored very well on the leader board, but they did much better when combined together.\nWhat was your most important insight into the data?\nThere were some columns in the training data (e.g. number of players for a question) that I did not use in my earlier models because I had thought they would have no effect on whether a user got the question right. I was surprised to discover later that adding them to the model did improve prediction accuracy. So I think the lesson is to not listen to your intuition; let data speak for itself. In practice, though, there is not enough time to try every possible feature, so we do have to go by intuition to a degree.\nWere you surprised by any of your insights?\nI was surprised by the low correlation between (1) an individual model\u2019s performance by itself and (2) the amount of performance improvement of an ensemble when the individual model is added to it.\u00a0 As an example, I had some very good individual models that when added to an ensemble barely improved the performance of the ensemble. By contrast, I also had some nearly hopeless models that when added to an ensemble significantly improved the performance of the ensemble.\nTherefore, we need lots of diverse models in the ensemble for good performance, not necessarily the best performing models. I think that is well known, but I was still surprised to observe it first hand.\nWhich tools did you use?\nJava and R.\nI used Java for pre-processing and building various Collaborative Filtering models and IRT models.\nI used R's nnet and gbm packages for Neural Networks and Gradient Boosting Machine respectively.\nWhat have you taken away from this competition?\n\nThe most fun way to get better at machine learning is to work really, really hard to win a machine learning competition. When I started in December, the leaderboard had many players with much better score than mine. However, for me, the knowledge that a much better model existed was a strong motivator for finding it, and I spent virtually all my free time researching and looking for better models and techniques.\nKaggle's forums for various competitions have top quality user generated content containing practical machine learning techniques that one does not find in text-books. I learned at least 2-3 techniques that helped me improve my score.\nOne can find readily applicable techniques and models in the papers from winners of other competitions. I read almost all the papers from the Netflix challenge, Heritage Health Prize, and KDD Cup 2010. The papers from the winners of \"KDD Cup 2010 Educational Data Mining Challenge\" contain a wealth of information relevant to the Grockit competition.\n\n"}, {"url": "http://blog.kaggle.com/2012/03/20/could-world-chess-ratings-be-decided-by-the-stephenson-system/", "link_text": "Could World Chess Ratings be decided by the 'Stephenson System'?", "id": 45, "title": "Permalink to Could World Chess Ratings be decided by the 'Stephenson System'?", "date": "2012-03-20", "text": "\nCongratulations to Alec Stephenson, who was recently announced as winner of the FIDE Prize in the Deloitte/FIDE Chess Rating Challenge! This prize was awarded to the submission which was the most promising practical chess rating system (the criteria can be found here). The World Chess Federation (FIDE) has administered the world championship for over 60 years and manages the world chess rating system.\nHere at Kaggle we're very excited about Alec's achievement. This is a major breakthrough in an area which has been extensively studied by some of the world's best minds. Alec wins a trip to the FIDE meeting to be held in Warsaw this April, where he will present his winning method. The next world chess rating system could be based on his model!\nWorld chess ratings have always used the Elo system, but in the last few years there has been a movement to make the rating system more dynamic. One approach is to modify the Elo system by adjusting the so-called 'K-factors', which determine how quickly individual match results change the overall rankings. Professor Mark Glickman, chairman of the United States Chess Federation ranking committee, has proposed the Glicko system, which was a key inspiration behind Microsoft's TrueSkill algorithm. Jeff Sonas, with the backing of FIDE, initiated this Kaggle contest to bring in fresh ideas. He says \"of all the things learned during the contest, the one that I am most excited about is the degree to which Alec was able to improve the accuracy of the well-established Glicko model without significantly increasing its complexity.\"\nWe interviewed Alec after his big win.\nWhat made you decide to enter?\nI make a couple of submissions in most competitions and then decide from that point whether my interest is sufficient to spend the time competing seriously. What I liked about the chess competition was that, unlike more traditional data mining competitions, the data was extremely simple, containing just player identifiers and results. This meant that the competition was more theoretical than is usually the case, which benefited me as a mathematician.\nWhat was your background prior to entering this challenge?\nMy background is in mathematics and statistics. I am currently an academic, teaching courses in R, SAS and SPSS, and have worked in a number places including The National University of Singapore and Swinburne University in Australia. I will soon be taking a position at CSIRO, Australia's national science agency.\nWhat preprocessing and supervised learning methods did you use?\nBecause of the simplicity of the data I took the view that the best approach would be to build upon methods that already exist in the literature. I took the Glicko sytem of Mark Glickman, added a couple of ideas from Yannis Sismanis and then used a data driven approach to inform further modifications. The Glicko system is based on a Bayesian statistical model; I took this and then let predictive performance, rather than statistical theory, determine my final scheme. I suspect my approach is less useful for other types of two-player games as it was essentially optimized for the chess data.\nWhat was your most important insight?\nThe most important and suprising thing was how competitive an iteratively updated ratings scheme could be in terms of predictive performance. It got in the top 20 overall, which was a great surprise to me, particularly given that the unrestricted schemes obtained an additional advantage from using future information that would not be applicable in practice.\nDo you have any advice for other Kaggle competitors?\nMy three tips are (1) Have a go! Start with some random numbers and progress from there. (2) Concentrate on learning new skills rather than the leaderboard. (3) Beware of anything that takes more than 10 minutes to run.\nWhich tools did you use?\nMy usual tool set is R, C, Perl and SQL, but for this competition I just used R with compiled C code incorporated via the .C interface. I'm currently working on an R package allowing users to examine different iteratively updated rating schemes for themselves. Hopefully it will also allow me to make my method a bit simpler without losing predictive performance, which may make it more palatable to the FIDE.\nWhat have you taken away from this competition?\nAn interest in methods for modelling two-player games, and a motivation to learn how to play chess! It's my second win in public Kaggle competitions, which is a nice personal achievement.\n"}, {"url": "http://blog.kaggle.com/2012/02/09/claiming-the-gold-matthew-carle-on-winning-the-claim-prediction-challenge/", "link_text": "Claiming the Gold: Matthew Carle on winning the Claim Prediction Challenge", "id": 46, "title": "Permalink to Claiming the Gold: Matthew Carle on winning the Claim Prediction Challenge", "date": "2012-02-09", "text": "\nAt long last, we catch up with Matthew Carle, the solo winner of the Claim Prediction Challenge.\nWhy did you decide to participate in the Claim Prediction Challenge?\nAs an actuary, I have worked on claims models in the past, and the Claim Prediction Challenge allowed me to see how my modelling skills compare with those of other modelling experts. It also provided a way to improve modelling skills and try new techniques.\nApart from monetary incentives, did anything else motivate you to participate in the competition?\nNormally when building predictive models it is not easy to understand how much better your model could be. The ability to rank your models against those of your competitors provides additional insight into how good your modelling skills actually are.\nHow many entries did you submit? \u00a0What drove you to continue submitting new entries?\nI made a total of 35 entries. The improvements made by other participants provided the incentive to continue making new submissions, particularly when in competition for the top places.\nWhat did you enjoy most about the competition?\nThe ability to compare model performance with those of other modelling experts, and improve modelling skills through testing of different techniques.\nWhat got you interested in actuarial science?\nI have always had an interest in mathematics and statistics, and in particular their application to real-world problems. The business context of actuarial science was also appealing.\n"}, {"url": "http://blog.kaggle.com/2012/02/06/kicking-goals-an-interview-with-marcin-pionnier-of-the-winning-team-in-dont-get-kicked/", "link_text": "Kicking Goals: An Interview with Marcin Pionnier of the winning team in Don't Get Kicked", "id": 47, "title": "Permalink to Kicking Goals: An Interview with Marcin Pionnier of the winning team in Don't Get Kicked", "date": "2012-02-06", "text": "\nMarcin Pionnier, member of the Don't Get Kicked winning team Sollers & Gxav, along with Xavier Conort, gives us some tips on how they blended different models to produce the best results.\nWhat made you decide to enter?\nThis particular competition was classical - it was easy to start and produce first submissions. Also the domain is something that I can understand - common sense is enough to interprete variable values and to create new ones.\nWhat was your background prior to entering this challenge?\nI graduated from Warsaw University of Technology (Poland), my master thesis was related to text/web mining. Currently I work as software architect/programmer for Sollers Consulting (we operate mainly in Europe) \u00a0in the insurance area, so preparation data sets from transactional systems for risk estimation is one of my typical everyday tasks. Also I think that participating in Kaggle challenges is giving me a lot of very valuable experience. \nHave you ever bought a used car?\nYes, but from authorized car dealer to mitigate the risk. It was \"Good Buy\" transaction - not to be confused with \"good bye\"  \nWhat preprocessing and supervised learning methods did you use?\nI did not use any external data sources. However, I added some variables to the initial dataset. The most important among them were:\n\nVarious differences between the prices given\nResults of regression models \u00a0built on training set: prediction of OdoVeh (it was then subtracted with just OdoVeh value), WarrantyPrice, Engine (in some cases it was possible to extract engine from text fields, for the rest it was made with this regression)\n\nMy part of our joint solution (simply blended with Xavier Conort's result) is average of seven internal models:\n\nSix variants of LogitBoost algorithm that internally use Dagging Algorithm - training set is divided into stratified folds and the internal models are averaged. As weak learners for Dagging method simple DecisionStump (one node decision tree) and some up to 3-level decision tree based on DecisionStump were used.\nSeventh model was Alternating Decision Tree which is also similar to boosting.\n\nWhat was your most important insight into the data?\nTo be honest there was no data transformation/additional data generated that gave me important progress. Various data enrichment approaches were improving my score only a little. I think that in this competition the most important task was to choose algorithms that could aggregate many weak predictors in an efficient way.\nWere you surprised by any of your insights?\nI am surprised with the power of boosting weak learners - their good performance is well-known fact, however I was using this technique for the first time.\nWhich tools did you use?\nI used Weka as library of algorithms linked with my own Java code for data pre-procesing and learning algorithms configuration. I think that such an approach gives the possibility of quick prototyping for initial solutions, and also it is possible to modify existing algorithms by copying the sources and introduce changes when needed.\nDo you have any advice for other Kaggle competitors?\nFusion of the R model developed by Xavier with my modeling ideas prepared on Weka-based tools gave us a big improvement on the leaderboard, so using different Machine Learning packages together with R (which seems to be most popular tool amongst Kagglers) might be good strategy.\n"}, {"url": "http://blog.kaggle.com/2012/02/03/vladimir-nikulin-on-taking-2nd-prize-in-dont-get-kicked/", "link_text": "Vladimir Nikulin on taking 2nd prize in Don't Get Kicked", "id": 48, "title": "Permalink to Vladimir Nikulin on taking 2nd prize in Don't Get Kicked", "date": "2012-02-03", "text": "\nVladimir Nikulin, winner of 2nd prize in the Don't Get Kicked competition, shares some of his insights and tells us why Poland is the place-to-be for machine learning.\nWhat made you decide to enter?\nBoth Challenges (Give Me Some Credit and Don't Get Kicked) could be regarded as classic and are very similar. That's why, I think, they were extremely popular. I have proper experience, and participated in the relevant Contests (see, for example, PAKDD07 and PAKDD10) in the past. In addition, the financial applications are directly relevant to the interests of my Department of Mathematical Methods in Economy at the Vyatka State University, Kirov, Russia.\nWhat was your background prior to entering this challenge?\nI have a PhD in mathematical statistics from the Moscow State University. By the way, I shall be visiting MSU in the middle of this February. Since 2005, I participated in many DM Challenges. In particular, some readers might be interested to consider text of my interview in Warsaw, Poland:\nhttp://blog.tunedit.org/2010/07/20/no-alternatives-to-data-mining/\nThis interview was given in June 2010. At that time, Kaggle was at the most early stages of development. \u00a0Also, I would like to use this opportunity to express my very high impression about support and recognition of the area of data mining in Poland.\nHave you ever bought a used car?\nYes, I bought three used cars while in Australia:\n\nToyota-Corona: {1978/1993/1995}\nToyota-Camry: {1992/1996/2000}\nToyota-Camry: {1999/2000/2011}\n\nwhere the meaning of the years is {made/bought/sold}.\nWhat preprocessing and supervised learning methods did you use?\nOn the pre-processing: it was necessary to transfer textual values to the numerical format. I used Perl to do that task. Also, I created secondary synthetic variables by comparing different Prices/Costs. On the supervised learning methods: Neural Nets (CLOP, Matlab) and GBM in R. No other classifiers were user in order to produce my best result.\nNote that the NNs were used only for the calculation of the weighting coefficient in the blending model. Blending itself was conducted not around the different classifiers, but around the different training datasets with the same classifier. I derived this idea during last few days of the Contest, and it produced very good improvement (in both public and private).\nWhat was your most important insight into the data?\nRelations between the prices are much more informative compared to the prices themselves.\u00a0 The next step was to rank and treat the relations in accordance to their importance.\nWere you surprised by any of your insights?\nYes, there was a huge jump from 0.26023 to 0.26608 in public, when I included in the model all the differences between Costs/Prices. I expected a jump, but not so big. On another occasion, I created two promising new variables, and thought it will produce some modest improvement at least. Instead, I observed deterioration.\nWhich tools did you use?\nPerl, Matlab, NNs in CLOP and GBM in R.\nDo you have any advice for other Kaggle competitors?\nBe flexible and patient. Do not worry too much about the LeaderBoard. Try to concentrate on the science and fundamentals, but not on how to win.\nAnything else that you would like to tell us about the competition?\nCurrently, I am working on the detailed description of my method, and would like to share an excerpt from the Introduction:\nSelection bias or overfitting represents a very important and challenging problem. As it was noticed in [1], if the improvement of a quantitativecriterion such as the error rate is the main contribution of a paper, the superiority of a new algorithms should always be demonstrated on independent validation data. In this sense, the importance of the data mining contests is unquestionable. The rapid popularity growth of the data mining challenges demonstrates with confidence that it is the best known way to evaluate different models and systems. Based on our own experience, cross-validation (CV) maybe easily overfit as a consequence of the intensive experiments. Further developments such as nested CV maybe overfitted as well. Besides, they are computationally too expensive [1], and should not be used until it is absolutely necessary, because nested CV may generate secondary serious problems as a result of 1) the dealing with an intense computations, and 2) very complex software (and, consequently, high level of probability to make some mistakes) used for the implementation of the nested CV. Moreover, we do believe that in most of the cases scientific results produced with the nested CV are not reproducible (in the sense of an absolutely fresh data, which were not used prior).\n[1] Jelizarow, M., Guillemot, V., Tenenhaus, A., Strimmer, K. and Boulesteix, A.-L. (2010) Over-optimism in bioinformatics: an illustration, Bioinformatics, Vol.26, No.16, pp.1990-1998.\n"}, {"url": "http://blog.kaggle.com/2012/02/02/momchil-georgiev-shares-his-chromatic-insight-from-dont-get-kicked/", "link_text": "Momchil Georgiev Shares His Chromatic Insight from Don't Get Kicked", "id": 49, "title": "Permalink to Momchil Georgiev Shares His Chromatic Insight from Don't Get Kicked", "date": "2012-02-02", "text": "\nMomchil Georgiev and Jason Tigg took home 3rd prize in Don't Get Kicked.\u00a0 SirGuessalot explains why our next used car should be orange, but that we should resist the urge to read too much into it.\nYour team uncovered\u00a0that\u00a0in order to avoid a \u201clemon\u201d, buyers might wish to try an orange \u2013 that is, an orange-colored car. Would you agree that the intuition behind this is that only a genuine enthusiast would own a car with such a wacky color, and would therefore be the kind of owner who would look after their vehicle?\nMomchil: \u00a0It sounds like a perfectly reasonable argument and would make a fantastic blurb, but let's take a deeper look into what's happening.\nHere's a quick breakdown of the cars in our training set by color and the respective percentage of lemons:\n\nWe can see that \"ORANGE\" is indeed the color with the lowest percentage of lemons. However, \"PURPLE\", an equally rare and odd color has the highest percentage of lemons and is 2 times more likely to be a lemon than an orange car. \u00a0So our argument about people with strange car colors taking better car of their cars is not supported by our data. At least, not until we look at the rest of the data fields in relation to Color.\nOrange may have been a unique color offered only by a car-maker with excellent maintenance record. \u00a0Or it may be that orange cars are so highly visible that they get in accidents less often. \u00a0While the former is very likely, the latter may not be because of the presence of \"GOLD\" and \"YELLOW\" at the bottom of our list.\nIt could be that most orange cars were purchased by the same couple of buyers whose favorite color was orange. \u00a0There is high variance in buyer skill when it comes to avoiding \"lemon\" buys.\u00a0 \nIn any case, speculation about the data is only useful inasmuch as it helps to generate ideas and jumpstart the analysis process. \u00a0This is an excellent illustration of how we need to be careful about making any assumptions about relationships in data. \u00a0\"Effect\" does not necessarily imply \"causation\".\n"}, {"url": "http://blog.kaggle.com/2012/02/01/of-caffeine-and-cross-validation-tim-veitch-on-dont-get-kicked/", "link_text": "Of Caffeine and Cross Validation: Tim Veitch on Don't Get Kicked!", "id": 50, "title": "Permalink to Of Caffeine and Cross Validation: Tim Veitch on Don't Get Kicked!", "date": "2012-02-01", "text": "\nTim Veitch, the 4th prize winner of used car prediction challenge Don't Get Kicked!, catches up with us about finishing in the money on his second Kaggle outing.\n\nWhat made you decide to enter?\nCuriosity, really! \u00a0Kaggle combines two of my favourite things: solving difficult problems and competition.\u00a0 I had a bit of spare time over Christmas, so I thought I'd give it a go.\u00a0 I'm also hoping to meet some interesting people from the Kaggle community - so feel free to get in touch!\nWhat was your background prior to entering this challenge?\nI work in my family's travel-modelling consultancy (Veitch Lister Consulting).\u00a0 My work involves trying to predict the daily travel made by the millions of people living in Australia's urban areas. \u00a0This has exposed me to fairly advanced choice modelling techniques (among them logistic regression), which has proved useful on Kaggle.\nHave you ever bought a used car?\nI drive a used car...but I can't say that I bought it. \u00a0It was a 'hand me down' from my Mum...Love You Mum! \u00a0I do, however, feel well qualified to buy a used car thanks to this competition!\nWhat preprocessing and supervised learning methods did you use?\nI used logistic regression to begin with. \u00a0This meant constructing ordinal variables from each of the numeric variables (e.g. the odometer), and adding some interesting variable interactions, particularly involving the MMR variables. \u00a0I also found some interesting temporal effects, and included a dummy variable for each month in the dataset. \u00a0I then extended my simple logit model by building \"logit trees\" - ie. binary splits (to a level of 1 or 2), with a logistic regression on each leaf. Late in the process I added two data driven approaches - random forests and GBMs, which used standard packages in R. \u00a0The GBM turned out to be my highest scoring individual model, with the logit forest second.\nWhat was your most important insight into the data?\nProbably the temporal effects. \u00a0My basic logit model suggested that the eight months from January to August 2009 were the eight months with lowest 'kick likelihood', all other things being equal. \u00a0I don't yet know the cause, but I think it would be very interesting to investigate why that period was such a good period for buying used cars. \u00a0If I'd gotten to the bottom of it, I'm sure it would have improved my model, as the effect probably varies spatially. \u00a0And it would certainly help with real life prediction.\nWere you surprised by any of your insights?\nI was continually surprised by the variables which proved important: wheel type, the month, or a lack of change in the MMR price (current - acquired). \u00a0It was surprising how relatively unimportant the make, model and vehicle type were.\nWhich tools did you use?\nI used my own C++ library for logistic regression, and the standard Random Forest and GBM packages in R (though I did try to implement my own GBM implementation on the last night, which didn't quite work as well as the R version). \u00a0I used the Ruby scripting language to tie it all together, and Excel pivot tables / charts to analyse the data.\nDo you have any advice for other Kaggle competitors?\nKaggle has really reinforced to me the importance of cross validation. \u00a0I've also found getting to know the inner workings of each algorithm very rewarding - it's interesting, and it helps. \u00a0I was surprised by how well GBMs worked...that's a key learning for me. \u00a0And drink lots of coffee...but not too much!\n\u00a0\n"}, {"url": "http://blog.kaggle.com/2012/01/30/owen-zhang-on-placing-2nd-in-the-claim-prediction-challenge/", "link_text": "Owen Zhang on Placing 2nd in the Claim Prediction Challenge", "id": 51, "title": "Permalink to Owen Zhang on Placing 2nd in the Claim Prediction Challenge", "date": "2012-01-30", "text": "\nOwen Zhang, who passed the 6 CAS exams \"just for fun\", discusses placing 2nd in the Claim Prediction Challenge\nWhy did you decide to participate in the Claim Prediction Challenge?\nTo continue improving and evaluating my predicative modeling knowledge and skills.\nApart from monetary incentives, did anything else motivate you to participate in the competition?\nTo master cutting-edge analytical methodology in the context of a real world business problem, and to see where I stand in insurance modeling.\nHow many entries did you submit?\nWhat drove you to continue submitting new entries?\u00a0\u00a0 I submitted 20 entries. The reason to keep submitting is to find out if the tricks that appeared to have worked on my own validation data would work on the 4th year data as well. Another purpose is, obviously, to \"catch\" those who were in front of me. In retrospect, my 3rd serious submission would have got the same 2nd place, but I didn't know then.\nHow would you characterize your competitors in this contest?\nI kind of \"know\" some of them (such as \"old dogs with new tricks\") through other modeling/data mining competitions. I feel this is a very diverse group of modelers. Some are obviously seasoned professionals and some have apparently just started learning. I also have the impression that many competitors are not from P&C insurance background.\u00a0 I guess we have more machine learners here than statisticians.\nWhat did you enjoy most about the competition?\nTrying to come up business stories behind partially anonymized data.\nWhat got you interested in actuarial science?\nI see myself as more a predictive modeler/data miner than an actuary (although I did pass 6 CAS exams just for fun), so this question doesn't really apply to me. I am interested in predictive modeling primarily because I find it is extremely intellectually stimulating AND I appear to be reasonably good at it.\n"}, {"url": "http://blog.kaggle.com/2012/01/26/meet-the-winner-of-the-algo-trading-challenge-an-interview-with-ildefons-magrans/", "link_text": "Meet the Winner of the Algo Trading Challenge: An Interview with Ildefons Magrans", "id": 52, "title": "Permalink to Meet the Winner of the Algo Trading Challenge: An Interview with Ildefons Magrans", "date": "2012-01-26", "text": "\nIldefons Magrans is the winner of the Algorithmic Trading Challenge.\u00a0 He explains why he chose to measure himself against the market.\nWhat was your background prior to entering this challenge?\nI hold a Masters in Computer Science, a Masters in Electrical Engineering and a PhD in Electrical Engineering.\u00a0 My first machine learning experience was with fuzzy logic clustering algorithms during the final project of MsC in CS.\u00a0 Recently, I have been working on two applied research projects: developing of a human-like dialog turn-taking model with a continuous-time Hidden Markov Model, and developing a classification system for a prosthetic ankle to infer the presence of stairs.\nWhat made you decide to enter?\nI have been interested in algorithmic trading since I finished my PhD 3 years ago. I have been studying market micro-structure, arbitrage opportunities at different frequencies, contributing to open-source algo trading infrastructure and so on. But I never dared to use real money.\u00a0 I was not sure about my skills compared to other people working in the field. This challenge was a wonderful opportunity to test myself.\nWhat preprocessing and supervised learning methods did you use?\nI tried many techniques: (SVM, LR, GBM, RF). Finally, I chose to use a random forest.\nWhat was your most important insight into the data?\nThe training set was a nice example of how stock market conditions are extremely volatile.\u00a0 Different samples of the training set could fit very different models. Lots of fun!\nWere you surprised by any of your insights?\nI was not surprised by the difficulty level. High frequency trading is a very competitive field full of smart people trying to fish small inefficiencies.\nWhich tools did you use?\nI did everything with R, without a database, on an i7 laptop with 16 Gbytes of RAM.\nWhat have you taken away from this competition?\nI have had to improve my parallel programming skills in R.\n"}, {"url": "http://blog.kaggle.com/2012/01/26/mind-over-market-the-algo-trading-challenge-4th-place-finishers/", "link_text": "Mind Over Market: The Algo Trading Challenge 4th Place Finishers", "id": 53, "title": "Permalink to Mind Over Market: The Algo Trading Challenge 4th Place Finishers", "date": "2012-01-26", "text": "\nAnil Thomas, Chris \"Swedish Chef\" Hefele and Will Cukierski came 4th in the Algorithmic Trading Challenge.\u00a0 We caught up with them afterwards.\nWhat was your background prior to entering this challenge?\nAnil: I am a Technical Leader at Cisco Systems, where I work on building multimedia server software. I was introduced to machine learning when I participated in the Netflix Prize competition. Other than Netflix Prize where I was able to eke out an improvement of 7% in recommendation accuracy, I have no significant data mining experience to speak of.\nChris: \u00a0I have a MS in electrical engineering, but I have no formal background in machine learning. \u00a0My first data-mining contest was the Netflix Prize, and I learned a tremendous amount by being part of the team that came in 2nd place. \u00a0Since then, I\u2019ve been hooked by these competitions, and have entered several Kaggle contests in my spare time. \u00a0During the day, though, I work on Voice-over-IP projects at AT&T Labs, where I\u2019m a systems engineer.\nWill: I studied physics at Cornell and am in the final stages of a PhD in biomedical engineering at Rutgers. \u00a0Like Chris and Anil, my first data mining contest was the Netflix prize, where I placed somewhere around 30,000th (the leaderboard doesn\u2019t go past 1000, but what\u2019s a few thousand places among friends). \u00a0Several years and many competitions later, I am lucky to rub elbows with the clever minds and talented folks on Kaggle.\nWill and Chris formed a team from the start, while Anil climbed the leaderboard separately. \u00a0In the closing days of the competition, the two teams agreed to merge in order to better their respective chances at the top and only prize. \u00a0Following a long weekend of furious model blending, they ended up in 4th. All three participants wish to thank Capital Markets Cooperative Research Centre and Kaggle.com for hosting this competition.\nWhat made you decide to enter?\nAnil: I had just completed the excellent online course on machine learning taught by Prof. Andrew Ng of Stanford and was looking for a challenge that goes beyond routine homework problems. This competition was a perfect fit. I have always found stock market data to be intriguing and this looked like a good opportunity to try my hand at analytics.\nWill: This was my first contest with financial data and a nice opportunity to peek into the world of short-term market dynamics, spreads, order books, etc.\nChris: \u00a0I have always been interested in \u201cquant\u201d finance topics. Also, I had some success in the INFORMS 2010 contest on Kaggle, which involved predicting short-term price movements of securities. \u00a0I thought some of the lessons I learned in that contest might be helpful in this one.\nWhat preprocessing and supervised learning methods did you use?\nWill: I had initial success with a kNN model and spent the majority of the competition convinced I could improve this model. \u00a0My initial feature set was picked by hand using feedback from the probe set (the last 50k trades of the train set). \u00a0Most of the features were basic transformations of the spread just before the liquidity shock. \u00a0Querying for neighbors within each security generally outperformed querying across all securities, but we did find that the combination of the two worked best. \u00a0I spent many weeks attempting to implement a more rigorous way to pick the feature space. \u00a0There are many published methods on how to learn a custom Mahalanobis distance metric using supervised labels (that is, to find S in the equation below such that trades with similar reactions would have similar distances in feature space).\n D_M(x) = \\sqrt{ (x - \\mu)^T S^{-1} (x - \\mu)}\n(Note that a diagonal S is the same thing as weighting each feature separately in Euclidean space)\nHowever, this contest was not a traditional supervised classification problem in the sense that we had a measure of dissimilarity (the RMSE between the bid/ask responses), as opposed to neat-and-tidy class labels. \u00a0Despite numerous promising modifications and a last minute multidimensional scaling idea, I ran out of time to find a suitable Mahalanobis matrix that beat the RMSE of the initial hand-picked feature set.\nChris: \u00a0I tried to keep it simple, and stuck with creating multiple variations on basic linear regression models. \u00a0I created more than 30 features derived from the original data (consisting mostly of the min/max/median/std.deviations of prices & spreads & the trade/quote arrival rate). \u00a0I fed those features plus the original data to a LASSO regression, which selected 18 variables. \u00a0Separate regressions were used for bids vs asks, and buys vs sells. \u00a0I also had models that predicted prices for each time period individually, as well as other models that predicted time-invariant, average prices. \u00a0Furthermore, the characteristics of the the testing & training sets differed, so I tried a variety of ways to weight each row of data to correct for those differences. In the end, I just weighted each row by the ratio of how often each security appeared in the testing vs training sets.\nAnil: The model that worked best for me was linear regression on various indirect predictors derived from the training data. I also tried Random Forest, k-NN, k-means and SVM regression techniques. As for preprocessing, I found it advantageous to set the base predictions to match the general trajectory of the prices and then model the residuals.\nWhat was your most important insight into the data?\nWill: This data was very fussy, and the use of un-normalized RMSE to score the competition made for a very skewed error distribution. Chris and Anil did some great due diligence into the quirks of this dataset, so I defer to them for details.\nChris: \u00a0The market open (at 8AM) was very extremely unpredictable, and contributed a disproportionally large amount of error. \u00a0For one model, I found 12% of squared error for the entire trading day occurred in the first minute of trading. To combat this, I trained some separate models for the market open, since it seemed so different (the naive benchmark model worked better than my regressions at the market open, for example).\nAdditionally, the farther away you got from the \u201cliquidity shock\u201d trade, the more unpredictable the prices were. \u00a0Looking backward in time from that \u201cliquidity shock\u201d trade, \u00a0my variable-selection algorithms dropped all historical bid/ask prices except those immediately before the trade, since those prices did not provide enough predictive value. As you moved forward in time from that trade, bid/ask prices got progressively harder to predict. \u00a0Using time-averages & PCAs, though, you could see two common patterns in the noise: \u00a0for buys, bid/ask prices jumped up sharply \u00a0& then rose slowly; for sells, bid/ask prices jumped down sharply, and then fell slowly. \u00a0Thus the \u201cliquidity shock\u201d trades seemed to have a permanent impact on prices, \u00a0\u00a0rather than a temporary, mean-reverting one.\nAnil: Categorizing and plotting the data clearly showed that the bid and ask prices followed separate paths and their trajectories differed depending on who initiated the trade - buyer or seller. Performing regression separately for each category led to dramatic improvement in prediction accuracy.\nWere you surprised by any of your insights?\nWill: I\u2019m unconvinced that I had noteworthy insights outside of the usual techniques to gain ground in a data mining competition. \u00a0RMSE falls by 3 methods: creating many models and blending them, better data/features, or better methods. \u00a0Chris and Anil each brought a nice blender and several models to the table. \u00a0I did what I could to make a better kNN method, but perhaps my time would have been better spent coming up with features or looking for outliers.\nAnil: The conventional wisdom seems to suggest that an ensemble of reasonably good models perform better than a finely tuned individual model. As a team, we had a great variety of models, but looking back, I think we would have fared better if we spent more efforts to tune the individual models. The models that used SVM and k-means with mediocre prediction accuracy ended up contributing almost nothing to the final blended result.\nChris: \u00a0I knew the market open & outliers would be important, but I was really surprised by how much of an impact they had on one\u2019s RMSE. \u00a0I was also surprised to find no mean-reversion in the bid/ask prices, since that differed from the examples that the contest organizers gave.\nWhich tools did you use?\nAnil: I am a minimalist and typically use little more than gVim, gcc and gnuplot. For this competition, I picked up some R and was impressed by its capabilities. One could just grab an off-the-shelf package, let it loose on the data and end up with decent results. I still think lower level languages have a place in data analytics because of the flexibility that they offer. Knowing what happens under the hood can give you an edge. Sometimes small tweaks to the underlying mechanism can give you a big boost when you desperately need it. My best model was written entirely in C++ without using any 3rd party libraries. This made it possible to mold the model well enough to fit the quirks within the data.\nChris: \u00a0I used R, mostly working with the glmnet package. \u00a0I also used Python (with \u00a0the numpy package) for blending prediction sets together.\nWill: \u00a0Matlab\nWhat have you taken away from this competition?\nWill: \u00a0Collaborating with new teammates was a nice experience. \u00a0Teammates bring different backgrounds, fresh ideas, and code in different ways. \u00a0Working alone it is easy to get stuck trying the same hammer on every nail, even if that nail happens to be a screw. That\u2019s when a teammate can step in and tell you to stop smashing with the hammer and try a screwdriver. \u00a0Witnessing another person dissect the same data problem is a great way to pick up new tools and skills.\nChris: \u00a0One lesson I learned from this competition is that one should always identify outliers as specifically as possible & decide how to best deal with them. \u00a0Also, it\u2019s really helpful to have teammates to bounce ideas off of, especially when you\u2019re stuck or losing motivation.\nAnil: The discussions on the forum, especially post-contest have been illuminating. I don't think any contestant individually had quite a handle on the data. The details that contestants have shared about their findings and methods have shed light on various aspects of the data. One can actually see the pieces coming together in the giant jigsaw puzzle. I am also thankful that I got to collaborate with Chris and Will, who are first-rate data scientists and fantastic people to work with.\n"}, {"url": "http://blog.kaggle.com/2012/01/05/jason-tigg-on-coming-third-on-the-planet-in-the-claim-prediction-challenge/", "link_text": "Jason Tigg on Coming Third on the Planet in the Claim Prediction Challenge", "id": 54, "title": "Permalink to Jason Tigg on Coming Third on the Planet in the Claim Prediction Challenge", "date": "2012-01-05", "text": "\nJason Tigg came third in the Claim Prediction Challenge and caught up with us afterwards.\nWhat was your background prior to entering the Prediction Claim challenge?\nAs I child I was interested in machine intelligence and when I was 14 I wrote my first \"intelligent\" program in assembler on my Dragon 32 computer to play Othello, inspired by a wonderful book \"Computer Gamemanship\" by David Levy. Through Kaggle I have made contact with David Slate of the team of \"Old Dogs with New Tricks\" who I have discovered was instrumental in pioneering the field of computer chess back in the 1970s. I studied at Oxford University where I obtained a doctorate in Elementary Particle Physics which made extensive use of an early version of Mathematica to solve\u00a0some fairly\u00a0complicated integral equations. Since then I have been working writing financial software for both trading and risk management. I previously entered a fascinating chess challenge on Kaggle, so this was my second competition.\nWhat made you decide to enter?\nI entered the competition mostly for the fun of the challenge. The leaderboard on Kaggle is addictive and gives a real sense of competition as well as giving you a sense of how well you are understanding the algorithms and the data.\nWhat was your most important insight into the dataset?\nI would say the most important insight was technical not algorithmic. The dataset was so large it required some compression to hold in RAM and some interesting iterator code to walk through one household at a time. Examining data clustered by household turned out to be particularly important.\nWhy the name Planet Thanet?\nI was born and grew up in the beautiful seaside town of Ramsgate in the Isle of Thanet in South East England.\nWhich tools did you use?\nNot many to be honest. All the code was written in Java and no third party libraries were used.\nWhat have you taken away from the competition?\nThe top two teams were some distance above me so clearly I must have missed something. Hopefully I will discover what that is!\n"}, {"url": "http://blog.kaggle.com/2012/01/03/the-perfect-storm-meet-the-winners-of-give-me-some-credit/", "link_text": "The Perfect Storm: Meet the Winners of 'Give Me Some Credit'", "id": 55, "title": "Permalink to The Perfect Storm: Meet the Winners of 'Give Me Some Credit'", "date": "2012-01-03", "text": "\nThe Perfect Storm, comprising Alec Stephenson, Eu Jin Lok and Nathaniel Ramm, brought home first prize in Give Me Some Credit. We caught up with Alec and Eu Jin.\nHow does it feel to have done so well in a contest with almost 1000 teams?\nEJ: Pretty amazing, especially when it was such an intense competition with so many good competitors. Personally, I felt a strong sense of achievement together as a team.\nAS: It feels great, particularly because we won by such a well-defined margin. The gap between first and second place was the largest gap in the top 500 placings.\nWhat were your backgrounds prior to entering this challenge?\nEJ: My background is in statistics and econometric modelling. More recently I've worked in data mining and machine learning for Deloitte Analytics Australia, where I am a Senior Analyst.\nAS: My formal background is in mathematics and statistics. I am a largely self-taught programmer, and have written a number of R packages. I do not work in data mining, but have picked up an interest in it over the last year or so, mainly due to Kaggle! I am an academic, originally from London, and have studied or worked at universities in England, Singapore, China and Australia.\nWhat preprocessing and supervised learning methods did you use?\nAS: We tried many different supervised learning methods, but we decided to keep our ensemble to only those things that we knew would improve our score through cross-validation evaluations. In the end we only used five supervised learning methods: a random forest of classification trees, a random forest of regression trees, a classification tree boosting algorithm, a regression tree boosting algorithm, and a neural network.\nThis competition had a fairly simple data set and relatively few features \u2013 did that affect how you went about things?\nEJ: It meant that the barrier to entry was low, competition would be very intense and everyone would eventually arrive at similar results and methods. Before we formed a team, I knew that I would have to work extra hard and be really innovative in my approach to solving this problem. Collaboration was the last ace and as the competition started to hit the ceiling, I decided to play that card.\nWhat was your most important insight into the data?\nEJ: I discovered 2 key features, the first being the total number of late days, and second the difference between income and expense. They turned out to be very predictive!\nWere you surprised by any of your insights?\nAS: I was surprised at how well neural networks performed. They certainly gave a good improvement over and above more modern approaches based on bagging and boosting. I have tried neural networks in other competitions where they did not perform as well.\nHow did working in a team help you?\nTOGETHER: As individuals, we were unlikely to win. But with Nathaniel's expertise in credit scoring, Alec's expertise in algorithms and Eu Jin's knowledge in data mining, we had something completely different to offer that was really powerful. In a literal sense, we stormed our way up to the top.\nWhich tools did you use?\nTOGETHER: SQL, SAS, R, Viscovery and even Excel!\nWhat have you taken away from this competition?\nAS: That data mining is fun when you are in a team, and also how effective a team can be if the skills of its members complement each other. You can learn a lot from the people that you work with.\n"}, {"url": "http://blog.kaggle.com/2011/12/21/score-xavier-conort-on-coming-second-in-give-me-some-credit/", "link_text": "Score! Xavier Conort on coming second in 'Give Me Some Credit'", "id": 56, "title": "Permalink to Score! Xavier Conort on coming second in 'Give Me Some Credit'", "date": "2011-12-21", "text": "\nXavier Conort came second in Give Me Some Credit and let us in on some of the tricks of the trade.\nHow does it feel to have done so well in a contest with almost 1000 teams?\nI feel great because Machine Learning is not part of my natural toolkit. I now look forward to exploiting this insight in my professional life and exploring new ideas and techniques in other competitions.\nWhat was your background prior to entering this challenge?\nI am an actuary and set up a consultancy called Gear Analytics a few months ago. It\u2019s based in Singapore, and helps companies to build internal capabilities in predictive modeling and risk management using R. Previously, I worked in France, Brazil, China and Singapore holding different roles (actuary, CFO, risk manager) in the Life and Non-Life Insurance industry.\nWhat preprocessing and supervised learning methods did you use?\nI didn't spend much time on preprocessing. My most important input was to create a variable which estimates the likelihood of being late by more than 90 days.\nI used a mix of 15 models including Gradient Boosting Machine (GBM), weighted GBMs, Random Forest, balanced Random Forest, GAM, weighted GAM, Support Vector Machine (SVM) and bagged ensemble of SVMs. My best score, however, was an ensemble without SVMs.\nThis competition had a fairly simple data set and relatively few features \u2013 did that affect how you went about things?\nThe data was simple yet messy. I found off-the-shelf techniques such as GBM could handle it. The relative simplicity of the data allowed me to allocate more time to trying different models and ensembling my individual models.\nWhat was your most important insight into the data?\nThe likelihood of being late was by far the most important predictor in my GBM and its inclusion as a predictor improved my individual fits accuracy.\nWere you surprised by any of your insights?\nI've always believed that people can benefit from diversity, but I was surprised to see how much data science can also benefit from it (through ensembling techniques). The strong performance achieved by\u00a0\u00a0Alec, Eu Jin and Nathaniel (Perfect Storm) also shows that teamwork matters.\nMy best individual fit was a weighted GBM which scored 0.86877 in the private set. Without ensembling weaker models, my rank would have been 25.\nWhich tools did you use?\nR, the excellent book The Elements of Statistical Learning and the great online Stanford course by Andrew Ng. All are free.\nWhat have you taken away from this competition?\nWhen I entered the competition, I was still unfamiliar with Machine Learning techniques as they are rarely used in the insurance industry. I was amazed by the capacity of Gradient Boosting Machine (also called Boosted Regression Trees) to learn non-linear functions (including interactions) and accommodate missing values and outliers. It is definitely something that I will include in my toolbox in the future.\n"}, {"url": "http://blog.kaggle.com/2011/12/20/credit-where-credits-due-joe-malicki-on-placing-third-in-give-me-some-credit-2/", "link_text": "Credit where credit's due: Joe Malicki on placing third in 'Give Me Some Credit'", "id": 57, "title": "Permalink to Credit where credit's due: Joe Malicki on placing third in 'Give Me Some Credit'", "date": "2011-12-20", "text": "\nJoe Malicki placed third in Give Me Some Credit despite health problems preventing him from submitting for the majority of the competition.\nHow does it feel to have done so well in a competition with almost 1000 teams?\nGreat! This was my first serious attempt at Kaggle - I've been doing data modeling for a while, and wanted to try cutting my teeth at a real competition for the first time.\nWhat was your background prior to entering this challenge?\nI have a computer science (CS) degree and have done some graduate work in CS, statistics, and applied math. I currently work for Nokia doing machine learning for local search ranking.\nWhat preprocessing and supervised learning methods did you use?\nI tried quite a bit of preprocessing, mentioned below, and in the code I posted to the forums.\nI used random forests, gradient boosted decision trees, logistic regression, and SVMs, with various subsampling of the data for various positive/negative feature balancing. In the end, using 50/50 balanced boosting, and 10/90 balance random forests, and averaging them, won.\nThis competition had a fairly simple data set and relatively few features \u2013 did that affect how you went about things?\nAbsolutely! One of the big problems in this competition was a large and imbalanced dataset - defaults are rare. I used stratified sampling for classes to produce my training set.\nTransformations of the data were key. To expand the number of features, I tried to use my prior knowledge of credit scoring and personal finance to expand the feature set.\nFor instance, knowing that people above age 60 are likely to qualify for social security, which makes their income more stable, and that 33% (for the mortgage) and 43% (for the mortgage plus other debt) are often magic debt to income (DTI) numbers was very useful. I feel strongly that knowing what the data elements actually represent in the real world, rather than numbers, is huge for modelling.\nRandom forests are a great learning algorithm, but they deal poorly where transforms of features are very important. So I identified some things that looked important - combining income and DTI to estimate debt and combining number of dependents and income as a proxy for disposable income/constraints. The latter is important since struggling to barely support dependents makes default more likely.\nAlso, trying to notice \"interesting\" incomes divisible by 1000 was useful - I was guessing that fraud might be more likely for these, and/or they may signal new jobs where a person hasn't had percent raises. I was going to try a Benford's law-inspired income feature, to help detect fraud, but had to leave the competition before I got a chance.\nWhat was your most important insight into the data?\nForum discussion pointed out that \u2018number of times past due\u2019 in the 90's seemed like a special code, and should be treated differently. Also, noticing that DTI for those with missing income data seemed weird was critical.\nWere you surprised by any of your insights?\nI had tried keeping a small hold-out data set, and then combining all of the models I trained via a logistic regression on that data set, using the model probabilities as features, but found that only taking the best models of various classes worked better.\nWhich tools did you use?\nR. It's a memory hog, but it has first-class algorithm implementations.\nWhat have you taken away from this competition?\nThe results I saw on the public test set differed dramatically from any results I could get on any held-out portion of the training set. In the end, the results on the private testing data set for all of the models I submitted were extremely close to my private evaluations, far closer than to the performance on the public set. This highlighted to me the importance of relying on cross-validation on large samples.\n"}, {"url": "http://blog.kaggle.com/2011/11/27/smile-alexander-dyakonov-on-placing-third-in-the-photo-quality-prediction-competition/", "link_text": "Smile!  Alexander D'yakonov on placing third in the Photo Quality Prediction competition", "id": 58, "title": "Permalink to Smile!  Alexander D'yakonov on placing third in the Photo Quality Prediction competition", "date": "2011-11-27", "text": "\nAlexander D'yakonov placed third in the Photo Quality Prediction competition and agreed to give us a peek into his process.\nWhat was your background prior to entering this challenge?\nI\u2019m an Associate Professor at Moscow State University. \u00a0I like different data mining problems and have participated in many Kaggle contests.\nWhat made you decide to enter?\nThe problem has few features, so it did not look very complicated, and it had interesting lists of words. There were many participants, so a good result would be more challenging.\nWhat preprocessing and supervised learning methods did you use?\nI combined random forests with a weighted k-NN. The combination used a weighted square root of the sum of squares of the predictions, with coefficients tuned by gradient descent. I did not use any external information.\nWhat was your most important insight into the data?\nNothing! I used Random Forests with some simple additional features. \u00a0For example, these included the \u201cratio\u201d (the width of the image divided by the height of the image) and \u201carea\u201d (the number of pixels in the photo). \u00a0I also used a merged word list (words from album name, album description, photo caption).\nWere you surprised by any of your insights?\nI was surprised that I couldn\u2019t build good features from the word lists. All my engineered features were worse than I expected.\nWhich tools did you use?\nMATLAB and R.\nWhat have you taken away from this competition?\nIt is necessary to be careful when you build the final decision. I made one mistake: instead of averaging six algorithms, I used only three algorithms and had worse performance.\n"}, {"url": "http://blog.kaggle.com/2011/11/23/picture-perfect-bo-yang-on-winning-the-photo-quality-prediction-competition/", "link_text": "Picture Perfect: Bo Yang on winning the Photo Quality Prediction competition", "id": 59, "title": "Permalink to Picture Perfect: Bo Yang on winning the Photo Quality Prediction competition", "date": "2011-11-23", "text": "\nWhat was your background prior to entering this challenge?\nI'm a software developer and got started in machine learning in the Netflix prize.\nWhat made you decide to enter?\nThe locations and lists of words seem to offer many possibilities. The data is clean with no missing values. And this was a short contest so I couldn't spend too much time on it.\nWhat preprocessing and supervised learning methods did you use?\nMy best result was a mix of random forest, GBM, and two forms of logistic regression. I put the raw data into a database and built many derived variables. I also used many raw & derived variables from external, location-based data. I wrote some Javascript code to call Google Maps API and retrieved:\n\nElevation at each latitude-longitude coordinate.\nCountry and administrative area. This could not be determined at some locations, they were all in the middle of the ocean, on small islands or boats I guess.\nNumber of \"places\" within 50 KM and 10 KM radius of each location, and users' ratings of these places.\n\nI downloaded a bunch of World Development Indicators data from\u00a0worldbank.org. Buried among these were 10 country-tourism data which I injected into my database.\nI downloaded population density data from\u00a0http://sedac.ciesin.columbia.edu/gpw/\u00a0and made a rough album per capita index: albumCount/populationDensity. I figured locations that score high on this index are remote, scenic places, and those that scored low are boring urban areas.\nAll these external data helped, but only a little. The raw and derived variables were fed to algorithms in different combinations.\nWhat was your most important insight into the data?\nI don't think I have any, and I'm actually very curious about Jason Tigg's insight. One day Jason suddenly gained a huge lead over everyone else, and I was convinced he found a great insight and/or external data. For the remainder of the contest, I was obsessed and went on a wild-goose chase after this insight, this \"one ring to rule them all (imagine Gollum hissing in the cave)\".\nMy most useful variables were simple and well known: average numbers of 'good' albums for each word, weighted with global average based on how many albums the word appear in. This was done separately for album name, album description, photo caption, and one merged word list. Then for each album and location, the average of word averages were calculated.\nWere you surprised by any of your insights?\nWell,\u00a0I was surprised I couldn't get\u00a0any signal\u00a0out of word pairs.\nWhich tools did you use?\nSQL, R, C++, C#, Javascript, Google web services, and Excel.\nWhat have you taken away from this competition?\nIt's probably not worth it to spend too much time on external data, as chances are any especially useful data are already included. Time can be better spent on algorithms and included variables. For example I didn't even try to use the number of times a word appeared in each album.\n"}, {"url": "http://blog.kaggle.com/2011/11/17/words-of-wisdom-from-ben-hamner-our-newest-recruit/", "link_text": "Words of Wisdom From Ben Hamner, Our Newest Recruit", "id": 60, "title": "Permalink to Words of Wisdom From Ben Hamner, Our Newest Recruit", "date": "2011-11-17", "text": "\nThis week, we were thrilled to welcome to the Kaggle team Ben Hamner, winner of the Semi-Supervised Learning Competition and one of our most successful competitors to date. Ben recently placed third in dunnhumby's Shopper Challenge, and had the following to say about the experience.\nWhat was your background prior to entering this challenge?\nI graduated from Duke University in 2010 with a bachelors in biomedical engineering, electrical and computer engineering, and mathematics. For the past year, I applied machine learning to improve non-invasive brain-computer interfaces as a Whitaker Fellow at EPFL. On the side, I\u2019ve participated in various predictive analytics competitions.\nWhat made you decide to enter?\nThe dataset was deceptively simple. It simply consisted of a list of customers\u2019 visits and spend amounts for the past year, whereas many datasets have a much higher dimensionality and require substantially more preprocessing. This simplicity provided a good competitive testbed for more statistically oriented methodologies.\nWhat was your most important insight into the data?\nTwo patterns in the data were key: periodic weekly behavior dominated when a customer visited a store (as opposed to the time since his last visit), and a customer\u2019s recent shopping behavior was more predictive than his past behavior. Simple models using these patterns performed very well. My best \u201csimple\u201d date predictor took the most commonly visited day of the week, weighted by how recent the visits were. My best \u201csimple\u201d spend predictor took the mode of a weighted kernel density estimate of the customer\u2019s previous spend behavior. As with most machine learning competitions, more complex methods that incorporated these simple models were necessary for bleeding edge performance.\nWere you surprised by any of your insights?\nStandard algorithms performed poorly on this task due to the atypical cost function. Many supervised regression methods optimize the mean squared error, but the spend prediction was evaluated with a binary metric: whether the predicted spend was with $10 of the actual spend. This meant that our task was to predict the most likely customer behavior, as opposed to the average customer behavior. All gradient based approaches I applied to this task performed relatively poorly, even when the cost function was modified to be a differentiable approximation of the binary evaluation metric. On the other hand, successful approaches to this competition performed poorly on the mean squared error metric.\nWhich tools did you use?\nI used Matlab and Python.\nWhat have you taken away from this competition?\nCarefully analyzing and optimizing according to the evaluation metric is crucial for competitors. The metric can dramatically affect which models perform well and which perform poorly, especially at the frontier of what is possible given the data. In my case, I optimized the model based on date prediction, and then on spend prediction given the predicted date. I optimized each of these over the individual evaluation metrics. I ran out of time to jointly optimize the date and spend prediction according to the final evaluation metric, and ultimately got beat by a model that did.\nFrom a competition host\u2019s perspective, the evaluation metric may not be as crucial. If the goal is to get the best models possible on a very well defined problem, then choosing the appropriate metric is absolutely vital. If the goal is to discover what is possible given an underlying set of data, or what useful patterns are hidden in the data, then the precise metric may not be as important as other considerations.\n"}, {"url": "http://blog.kaggle.com/2011/11/01/small-steps-gareth-campbell/", "link_text": "Small steps: Gareth Campbell on placing fourth in Wikipedia's Participation Challenge", "id": 61, "title": "Permalink to Small steps: Gareth Campbell on placing fourth in Wikipedia's Participation Challenge", "date": "2011-11-01", "text": "\nA quick interview today with Dr Gareth Campbell, who placed fourth in Wikipedia's Participation Challege.\nWhat was your background prior to entering the Wikipedia Challenge?\nI have a PhD in Finance.\nWhat made you decide to enter?\nI wanted to test how good my econometric skills were.\nWhat was your most important insight into the dataset?\nThe number of edits by Wikipedia users was almost entirely dependent on the number of edits made by them in the past few months. The number of reversions, the type of article edited, comments etc. had little predictive power.\nWere you surprised by any of your insights?\nA little surprised that the number of reversions had such little impact.\nWhich tools did you use?\nStata.\nWhat have you taken away from the competition?\nTesting lots of different small steps can lead to substantial improvements in predictive power.\nThanks Gareth!\n"}, {"url": "http://blog.kaggle.com/2011/10/26/long-live-wikipedia-dell-zhang/", "link_text": "Long live Wikipedia: Dell Zhang on placing third in Wikipedia's Participation Challenge", "id": 62, "title": "Permalink to Long live Wikipedia: Dell Zhang on placing third in Wikipedia's Participation Challenge", "date": "2011-10-26", "text": "\nDell Zhang placed third in Wikipedia's Participation Challenge and agreed to give us a peek into his process.\nWhat was your background prior to entering the Wikipedia Participation challenge?\nI got my PhD in Computer Science from the Southeast University in China 9 years ago, and then moved to Singapore to work as a postdoc research fellow under the supervision of Prof Wee Sun Lee. It was very kind of him to send me to the first-ever Machine Learning Summer School in 2002 - that's when I discovered the fascinating world of machine learning. Since then my research has been centred around using statistical machine learning techniques to improve information retrieval and organisation. I am currently a Senior Lecturer in Computer Science at Birkbeck, University of London. I have also joined the Royal Statistical Society and learned loads of interesting stuff done by statisticians.\n\nWhat made you decide to enter?\nBeing a busy academic I can no longer spend much time on coding as I used to, but I still enjoy getting my hands dirty and playing with data now and again. So when I got some time this summer, I decided to take part in a Kaggle competition to brush up on my rusty coding skills. The Wikipedia Participation Challenge attracted me most because as a heavy user of Wikipedia I felt obliged to do something helpful for the community.\nWhat was your most important insight into the dataset?\nThe most important insight was that most user's future behaviour can be largely determined by his or her recent behavioural dynamics - how the number of edits and the number of edited articles change in the last period of time.\nWere you surprised by any of your insights?\nI am a bit surprised that dynamics features alone can go such a long way when we choose proper temporal scales and employ a powerful machine learning method.\nWhich tools did you use?\nI used Python to write small programs for analysing data and making predictions. I like the simplicity of Python - simple is beautiful. The machine learning methods that I have tried all come from two open-source Python modules: one is scikit-learn, and the other is OpenCV. Finally, Gradient Boosted Trees (implemented in OpenCV) outperformed the other methods. I must mention that the parameter tuning was carried out on a big validation dataset shared to all participants by Twan van Laarhoven. \"Such a nice guy\", indeed!\nWhat have you taken away from the competition?\nA lot of fun, and a few lessons   I cannot wait to see others' secret weapons for tackling this problem. Long live Wikipedia!\nThanks very much Dell and congratulations on a great performance!\nUpdate\nDell has posted a detailed write-up at:\nhttp://arxiv.org/abs/1110.5051\nAnd \u00a0source code at:\nhttp://www.dcs.bbk.ac.uk/~dell/publications/wikichallenge_zeditor.zip\n"}, {"url": "http://blog.kaggle.com/2011/10/20/creatures-of-habit-neil-schneider/", "link_text": "Creatures of Habit:  Neil Schneider on placing second in the dunnhumby Shopper Challenge", "id": 63, "title": "Permalink to Creatures of Habit:  Neil Schneider on placing second in the dunnhumby Shopper Challenge", "date": "2011-10-20", "text": "\nNeil Schneider placed second in the dunnhumby Shopper Challenge with a breakout performance. Read on for some insights into his methodology, and visit the links at the bottom to view his code.\nWhat was your background prior to entering the dunnhumby Shopper Challenge?\nI am an Associate Actuary with Milliman's Indianapolis office. I have two degrees from Purdue University in Actuarial Science and Statistics. I graduated in December of 2007 and joined Milliman. Most of my experience is in creating actuarial models for pricing and reserving. During my time with Milliman, I have become proficient in SAS and JMP. Lately, I have completed some data analyses using R. I understand enough R to run various functions, but still rely on SAS for all my data manipulations. Our office has recently completed some work on a new method for reserve projections, based on published robust time series statistics. A lot of the research was applied from the inventory control field.\n\nWhat made you decide to enter?\nWe found Kaggle during our reserve methodology research, because of the Heritage Health Prize. I have competed in the \"Don't Overfit!\" and \"Mapping Dark Matter\" competitions prior to dunnhumby's challenge. While most of the competitions sound intriguing, I only have so much free time to devote to a competition. I thought this was an excellent example of sparse time series data and was hoping to leverage knowledge from our own model to predict the outcomes. As it turned out, this was not the case.\nWhat was your most important insight into the dataset?\nOne insight was that shoppers are habitual. They will tend to have their day of week to do their shopping. A customer may visit the store on different days of the week, but you can see that for different weekdays a customer will spend different amounts. Example: A customer typically spends $100-$150 on Saturday, but will visit the store just as often on Monday, but only spends $40-$60. Developing separate projections for the spend amount by weekday was important to correctly predicting the next spend.\nWere you surprised by any of your findings?\nI was surprised that the time series models for inventory controls performed so poorly on these projections. I was also surprised at how poorly regression models fit the spend amounts. This is probably due to the test statistic for the evaluation. Out of the box regressions will optimize predictions for the mean or quartiles of the dependant variable. This competition needed an optimization of the highest density area.\nWhich tools did you use?\nI used SAS for most of the heavy data work. This included proc sql statements to develop the maximum density visit_spend ranges.\nJMP was invaluable for visualizing the data and optimizing choices. I mainly used histograms, X vs Y plots and partition models.\nFinally, I used R to run more advanced statistical models (Generalized Boosting Regression Models - Package \"gbm\").\nWhat have you taken away from the competition?\nI learned that GBMs are indeed powerful for predictions, but the interpretation of coefficients for independent variables can be meaningless. This leads me to question what methods would be the most useful for modeling, but producing quality coefficient estimates. Maybe a future competition?\nCongratulations Neil on a fantastic performance! Neil has posted his methodology in more detail, along with his SAS and R code, on the Kaggle forum.\n"}, {"url": "http://blog.kaggle.com/2011/10/19/deceitful-beast-william-cukierski/", "link_text": "The Mode is a Deceitful Beast: William Cukierski on finishing fourth in dunnhumby's Shopper Challenge", "id": 64, "title": "Permalink to The Mode is a Deceitful Beast: William Cukierski on finishing fourth in dunnhumby's Shopper Challenge", "date": "2011-10-19", "text": "\nWilliam Cukierski finished fourth in the dunnhumby Shopper Challenge, backing up his previous second-place finish in the IJCNN Social Network Challenge (you can read his write-up of that challenge here). At the time of writing, Will has just WON/FINISHED SECOND in the Semi-Supervised Feature Learning.\nWhat was your background prior to entering the dunnhumby Shopper Challenge?\nI studied physics at Cornell and am now finishing a PhD in biomedical engineering at Rutgers. During my day job, I look at ways to apply machine learning and data mining to cancer diagnosis in pathology images. During my night job (once my fianc\u00e9e has gone to bed and the coast is clear) I fire up Matlab and trade hours of sleep for a chance at Kaggle glory.\n\nWhat made you decide to enter?\nSimple data sets always catch my eye. I like to be able to get right down to the analytics, without spending hours poring over data dictionaries and mucking with strings and categorical variables and all that business. I do enough of that in my day job!\nWhat was your most important insight into the dataset?\nLike forecasting weather, this was a very \u201clocal\u201d prediction contest. Most customers returned to the store quite soon, meaning that forecasting the date/spend too far out provided vastly diminishing returns. There was no Easter holiday to account for (which would have been a tricky detail had the competition been a year earlier). I polled friends for ideas on what might make people go shopping. Despite some good leads (one suggested that people might shop after getting a paycheck on the 1st or 15th of the month), I couldn't find any such macro trends that affected the short prediction time frame. It may seem obvious that an individual's decision to go shopping has very little to do with how much the store has taken in that day, or how many others have gone that day, but one can never ignore these possibilities in a data mining competition where fractions of a percent matter.\nDue to the nature of the scoring method (namely, that you had to get the date exactly right for the spend estimate to even matter), I focused almost entirely on predicting the date. I extracted features on the historical date information alone (ignoring how much was spent and ignoring the training data after April 1st, 2011). There were strong weekday patterns, which meant many of the features worked best when computed \u201cmodulo 7\u201d.\nThere are three generic classes of features one can consider in a problem like this:\n1. User features: prior number of visits, mode time since visit, PCA, SVDs, etc.\n2. User-Date features: prior probability of visiting on a given weekday, days since visiting, empirical probability distribution of having x days pass without a visit, etc.\n3. Date features: ignored... global information not very important!\nI performed logistic regression to obtain a probability of the customer visiting on each day for 2 weeks after the start of the prediction window. Each day has a different feature matrix due to the inclusion of the user-date features. The predicted date was then the one with the highest probability. For the spend, I used the \u201cmodal window\u201d concept, which I hope other contestants will describe in more detail.\nWere you surprised by any of your insights?\nI was most surprised by the seemingly endless list of things which didn't work on this data! In most data mining problems, if you have method A which does well and method B which does well, you can combine them and watch your score improve. This one was tough because if A says \u201cTuesday\u201d and B says \u201cThursday\u201d, you can't average them and say \u201cWednesday.\u201d This would have improved your score if something like RMSE was used, but it doesn't fly for the exact error metric. For all you know, that person has Yoga class and never goes shopping on Wednesday. Similarly, you can't toss the \u00a32 gum purchases in with the \u00a3200 weekly shops and guess the person will spend \u00a3100.\nTo alleviate this problem, I had to be careful about the types of features used for the regression. In metaphorical terms, I tried to take modes where I normally would have taken means. This required careful attention to statistical support. The mode is a deceitful beast in that the \u201cmost common\u201d pattern of past behavior can range from \u201cthis person comes in every 7 days without fail\u201d all the way to \u201cthis person comes in whenever they feel like it, and it just so happens that 7 was the number that won by chance.\u201d I experimented with a number of ways to use confidence estimates to weight, blend, or downplay features for customers who were strangers to the supermarket.\nThanks William - now we're looking forward to reading about your success in the Semi-Supervised Feature Learning competition!\n"}, {"url": "http://blog.kaggle.com/2011/10/16/kernel-density-at-the-checkout/", "link_text": "Kernel Density at the checkout: D'yakonov Alexander on winning the dunnhumby Shopper Challenge", "id": 65, "title": "Permalink to Kernel Density at the checkout: D'yakonov Alexander on winning the dunnhumby Shopper Challenge", "date": "2011-10-16", "text": "\nLong-time Kaggle competitor D'yakonov Alexander won the dunnhumby Shopper Challenge ahead of 537 other entrants who submitted a grand total of 2029 entries.  In addition to releasing his code and a description of his method, D'yakonov agreed to answer some background questions for us:\nWhat was your background prior to the dunnhumby Shopper Challenge?\nI received a PhD in Mathematics from Moscow State University, Russia (my supervisor was academician Yuri Ivanovich Zhuravlev), where I now work. I like different contests and this is my fifth Kaggle competition.\n\nWhat approaches did you try, and what worked best?\nAt first I tried to use simple heuristics to understand the 'logic of the problem'. My main idea was to split the problem into two parts: the date prediction and the dollar spend prediction. For the first task I initially thought to use probability theory. But I soon found out that it was useless to predict the date if we couldn't predict the spend. Therefore I calculated not only probabilities of visits, but also the stability of users\u2019 behavior (see Alexander's detailed description of his winning method). For that task, I used a kernel density (Parzen) estimator. But it was necessary to take account of the fact that 'fresh' data is more useful than 'old' data, so I used a weighted Parzen scheme to give greater weight to more recent data points. Then I hung parameters on my heuristics and performed my optimization.\nWhat tools did you use?\nI use MATLAB for all my Kaggle entries, just the basic M-language without any libraries.\nCongratulations D'yakonov Alexander on a fantastic result.  D'yakonov's winning method description and code are available to download from his website: \nClick here for his method description \nClick here for his winning code\n(startsolution2.m to run)\n"}, {"url": "http://blog.kaggle.com/2011/10/06/like-popping-bubble-wrap/", "link_text": "Like Popping Bubble-Wrap: Keith T. Herring on Placing Second in Wikipedia's Participation Challenge", "id": 66, "title": "Permalink to Like Popping Bubble-Wrap: Keith T. Herring on Placing Second in Wikipedia's Participation Challenge", "date": "2011-10-06", "text": "\nKeith T. Herring placed second in the Wikipedia Participation Challenge with just three entries on the board and agreed to talk to Kaggle about his process. Read on for the first in a great series of interviews with the top competitors from the Wikipedia challenge.\nWhat was your background prior to entering the Wikipedia Participation Challenge?\nI have a computer science degree from my home state, University of Illinois Urbana-Champaign (UIUC)... I then headed to Boston to get a Masters and Doctorate from MIT in Electrical Engineering and Computer Science. I now reside and work in the Queen City (official Seattle nickname from 1869 to 1982, ref: Wikipedia). I\u2019ve been fortunate to have been allowed to get my hands dirty in Robotics, Wireless Communications, Remote Sensing, Machine Learning, Network Security, Financial Markets, Casino Arbitrage, and the prediction of infinitesimally small subsets of the future universe, although I don\u2019t feel obligated to call myself a futurist or appear on the Discovery Channel as such.\n\nWhy did you decide to enter the Challenge?\nTwo Reasons.\nFirst, I have a lot of respect for what Wikipedia has done for the accessibility of information. Any small contribution I can make to that cause is in my opinion time well spent.\nSecond, a new data set is to me what a new sheet of bubble wrap is to Larry David. I can\u2019t wait to dive in and pop all the bubbles/bits of information I can find! So this is where I give props to Kaggle: they\u2019ve done a great job building on the success and excitement of the Netflix Prize. It's a win-win for data enthusiasts like myself and organizations like Wikipedia that have a lot of data and questions.\nWhich tools did you use?\nMy strategy was as follows:\n\nCompile a representative training set of Wikipedia Editing behavior. An interesting feature of this competition was that it involved a public data set. I wrote web scrapers to extract the editing history of approximately 1 million Wikipedia editors.\nTransform the raw edit data into a representative feature set for prediction of future editing volume. My final predictor operated on 206 features derived from editor attributes such as: edit timing, edit volume, name-space contributions, article concentration, article creation, edit automation, commenting behavior, etc.\nLearn a diverse set of future edit predictors. Each model/predictor I considered was a randomly constructed decision tree. I made use of an implementation (ref: Abhishek Jaiantilal and Andy Liaw) of Breiman and Cutler\u2019s Random Forest algorithm for constructing the individual random decision trees. Further diversity was achieved by randomizing the random forest parameters for each individual tree rather than using a single optimized set of parameter values for all trees. Randomized parameters included: feature set cardinality per decision node (weak vs strong learner), in-to-out-of-bag ratio, stop- ping conditions (under vs over fitting).\nMy final future edits predictor was formed as an optimized ensemble of the models in (3). I wrote an iterative quadratic optimizer for approximating the optimal model weighting using the out-of-bag samples, which varied across candidate models/trees. Out of the approximately 3000 models generated, 34 informative non-redundant models were retained in the final optimized ensemble.\n\nI used the following tools/setup to implement this strategy: Ubuntu Linux, Python, MySQL, C++, and Matlab.\nWhat was your most important insight into the dataset?\nA randomly selected Wikipedia editor that has been active in the past year has approximately an 85 percent probability of being inactive (no new edits) in the next 5 months. The most informative features (wrt the features I considered) captured both the edit timing and volume of an editor. More specifically the exponentially weighted edit volume of a user (edit weight decreases exponentially with increased time between the edit and the end of the observation period) with a half-life of 80 days provided the most predictive capability among the 206 features included in the model.\nOther attributes of the edit history, such as uniqueness of articles, article creation, comment behavior, etc. provided some additional useful information, although roughly an order of magnitude or less than the edit timing and volume when measured as global impact across the full non-conditioned editor universe.\nAn opportunity for future analysis would be to consider data relevant to any community political dynamics that may exist. Specifically edit reversion behavior and associated attributes.\nThanks Keith, and congratulations on a fantastic performance!\n"}, {"url": "http://blog.kaggle.com/2011/10/04/from-soundtracks-and-signatures/", "link_text": "From Soundtracks & Signatures to Stars & Galaxies: Ali Hassa\u00efne and Eu Jin Lok on finishing third on the Mapping Dark Matter Challenge", "id": 67, "title": "Permalink to From Soundtracks & Signatures to Stars & Galaxies: Ali Hassa\u00efne and Eu Jin Lok on finishing third on the Mapping Dark Matter Challenge", "date": "2011-10-04", "text": "\nFor the last of our series of interviews with the top Mapping Dark Matter competitors, Ali Hassa\u00efne and Eu Jin Lok pulled out the stops to give some generous insight into their techniques.\nWhat was your background prior to entering Mapping Dark Matter?\nAli: I graduated from Center of Mathematical morphology / Mines-ParisTech in 2009 with a PhD in morphological image processing. I worked within my thesis (under the supervision of Etienne Decenci\u00e8re and Bernard Besserer) on the restoration of optical soundtracks of old movies. I am currently a postdoctoral researcher at Qatar University working on writer identification and signature verification with Somaya Al-Ma'adeed. Earlier this year, I hosted the ICDAR2011 Arabic Writer Identification Contest on Kaggle.\nEu Jin: I have a masters degree in Econometrics and am currently an analyst at Deloitte Australia, working in data analytics. I've been with Kaggle for almost a year and have competed in many competitions, one of them being the writer identification contest hosted by Ali [Kaggle note - Eu Jin came in the top 5 in that contest as well!].\n\nHow did you come to form a team together?\nAli: I contacted Eu Jin because he showed very interesting ways of combining the features we provided in the writer identification contest. These same features have also been used in this contest along with many others.\nWhat was your most important insight into the dataset?\nAli: I combined several methods, some are inspired from my PhD thesis on soundtrack restoration. Other methods are inspired from my current research on writer identification and signature verification. I also tried some methods which were specifically developed for this problem. I will briefly describe here the methods I liked the most:\nMethods inspired from soundtrack restoration (the morphological approach)\nFor an introduction to mathematical morphology, Wikipedia's page is a good starting point. This instructional video about optical soundtracks might also be of interest.\nThe soundtrack (the part of the film stock which contains the audio information) is sometimes badly exposed due to light diffusion during the copying process.\n\n\nFigure 1 Location of soundtrack on film stock\n\nThe effect of bad exposure is clearly visible on the peaks and valleys of optical soundtracks.\n\n\nFigure 2 (a) overexposure\n\n\n\nFigure 2 (b) normal exposure\n\n\n\nFigure 2 (c) underexposure.\n\nThe effect of under-exposure is visually very similar to a morphological dilation with a certain structuring element. Given the physical process that causes under-exposure, it can be safely assumed that this structuring element is a disk. Therefore, the under-exposure might be restored by applying the dual morphological operator which is a morphological erosion with the same structuring element. Similarly, the over-exposure is very similar to a morphological erosion and might be corrected by applying a dilation [1] (references below).\nIn the same way, it can be assumed that the effect of lensing (or more generally, the application of a PSF) is very similar to a morphological operation with a certain structuring element. The structuring element is no longer a disk, however its shape can be computed from the provided star images as explained in following scheme.\nFigure 3 Computing basic morphological operations from galaxy and star images\nThe quadrupole moments are then computed from all the resulting images and used as predictors.\nMethods inspired from signature verification and writer identification\nSeveral geometrical features previously used in signature verification and writer identification happen also to be very powerful predictors for computing the ellipticity. These features are also computed on the thresholded galaxy and star images. The following figure illustrates some of them.\nFigure 4 Geometrical features\nAll these predictors, along with many others, have been combined using a linear fit. The strength of this method lies in the large number of diverse predictors it combines. For those of you who might be interested in trying other models, I\u2019ve put all the data together here.\nWhat tools did you use?\nThe code is written in C++ with a strong dependency on the excellent OpenCV library. I will hopefully make the code available soon.\nThanks and congratulations to Ali and Eu Jin!\nReferences:\n[1] J. Taquet, B. Besserer, A. Hassa\u00efne and E. Decenci\u00e8re, Detection and correction of under/overexposed optical soundtracks by coupling image and audio signal processing, EURASIP Journal on Advances in Signal Processing, Vol. 2008.\n[2] S. Al-Ma\u2019adeed, E. Mohammed and D. Al Kassis, Writer identi?cation using edge-based directional probability distribution features for Arabic words, International Conference on Computer Systems and Applications (AICCSA), pp. 582-590, 2008.\n[3] A. Hassa\u00efne, V. Eglin and S. Bres, Une m\u00e9thode de compression sans perte pour les images de documents bas\u00e9e sur la s\u00e9paration en couches\n"}, {"url": "http://blog.kaggle.com/2011/09/13/deepzot-on-dark-matter/", "link_text": "DeepZot on Dark Matter: How we won the Mapping Dark Matter challenge", "id": 68, "title": "Permalink to DeepZot on Dark Matter: How we won the Mapping Dark Matter challenge", "date": "2011-09-13", "text": "\nDaniel Margala and David Kirkby (as team DeepZot) placed first in the Mapping Dark Matter challenge. \u00a0Daniel agreed to answer a few questions for No Free Hunch as part of our series of posts on the best Mapping Dark Matter entries. \u00a0\nWhat was your background prior to entering Mapping Dark Matter?\nI graduated from the University of California, Los Angeles in 2009 with a B.S. in Physics. In the course of my studies at UCLA, I learned Linux system administration and various scripting languages managing a cluster of servers and data archive for an astro-particle research group. I became interested in numerical analysis while investigating the polarity and momentum of muons produced in the atmosphere by incident cosmic rays. Currently, I am a PhD student in the Physics and Astronomy Department at the University of California, Irvine. My advisor (and DeepZot team member), Prof. David Kirkby, and I are using the Baryon Oscillation Spectroscopic Survey (BOSS) to study the distribution of matter in our universe at the largest volumes. My work with BOSS has primarily focused on the operations software at the telescope, specifically, with the interfaces to the BOSS spectrograph, located at Apache Point Observatory in New Mexico.\nHow did you come to form a team together?\nI became interesting in working with David during a conversation that included an avid discussion of programming languages at a department event (where I was lured by the prospect of free food and drink, the perfect bait for graduate students). I began working on a variety of projects with David for about half a year, ranging from cosmology to electrical engineering, before we started working on the related GREAT10 challenge.\nWhat made you decide to enter?\nThe GREAT10 challenge was a perfect opportunity for me to bring my freshly developed proficiency in numerical analysis to bear. As a student looking to gain experience, this was also a chance to contribute to the forefront of analysis techniques employed by the weak lensing community. The comparatively compact size and similarity between data sets made participating in the MDM challenge very attractive. The ellipticity measurement (galaxy shape) in the MDM competition was a critical step in our GREAT10 analysis, where the goal is to disentangle the shear (due to dark matter) from the intrinsic galaxy shape.\nWhat was your most important insight into the dataset?\nThe most important insight was that the pixel-level residuals are a powerful tool for finding the best-fit models for galaxy and star images. We were able to assess the quality of various image models and parameters studying distributions of residuals across sets of images. This was essential to our method, which consisted of a pixel-level maximum-likelihood fit to each star and galaxy image.\nThe images below demonstrate our fit for a single galaxy image. From left to right, we see the original image (zoomed in on the center), our fitted model with the same resolution, and a higher resolution version of the fitted model:\n\n\n\n\n\n\n\n\n\n\n\n\nWere you surprised by any of your insights?\nWe used an artificial neural network to improve the ellipticity measurements from the fitting procedure. The neural network was trained to provide corrections using a non-obvious subset of the of the likelihood output. For example, feeding the centroid position for a galaxy image to the neural network worsened the predicted correction. Without the neural network, our best entry would have ranked 8th.\nWhich tools did you use?\nOur code consists of C++ libraries that we developed to perform the following tasks: generate images by convolving galaxy and point spread function models, access GREAT10 and MDM images, carry-out maximum-likelihood fit of images, do KSB measurement (moment inspired technique commonly used by astrophysicists) of ellipticity, and provide an ellipticity correction via machine learning algorithms.\nThe code utilizes a fit minimization engine (Minuit) and neural network engine (TMVA) that are both available as part of the open source (LGPL) ROOT data analysis framework (http://root.cern.ch), which is widely used by particle physicists.\nWhat have you taken away from the competition?\nI am intrigued by the tight cluster of scores near the top of the leaderboard despite the wide variety of methods applied. This suggests that those methods are making use of the maximum amount of information available in the images in the presence of pixelation and noise.\nWinning the MDM competition gave us confidence in our strategy for measuring the shapes of galaxies, which we have put to use in GREAT10.\nMost importantly, why 'DeepZot'?\nThe name DeepZot was formed by merging 'Deep Thought', the name of a fictional computer from Douglas Adams\u2019 Hitchhiker\u2019s Guide to the Galaxy, and 'Zot!', the battle cry of our school\u2019s mascot, Peter the Anteater.\nCongratulations to Daniel and David!\n"}, {"url": "http://blog.kaggle.com/2011/09/02/mapping-matter-with-matlab/", "link_text": "Mapping Matter with Matlab: Sergey Yurgenson on finishing second in Mapping Dark Matter", "id": 69, "title": "Permalink to Mapping Matter with Matlab: Sergey Yurgenson on finishing second in Mapping Dark Matter", "date": "2011-09-02", "text": "\nSergey Yurgenson finished second the Mapping Dark Matter challenge and agreed to answer a few 'How I Did It' questions for our first post in the Mapping Dark Matter series. \u00a0Over the next few weeks we will be posting regular interviews with more of the top competitors.\nWhat was your background prior to entering Mapping Dark Matter?\nI have a PhD in Physics from Leningrad (now St.Petersburg) State University, Russia. \u00a0I work at Harvard University developing software and hardware for neurobiology research and data analysis.\nMy first attempt at a data mining competition was the Netflix Prize. \u00a0I learned about it somewhere in the middle of the competition and spent several weeks building my model. \u00a0I managed to just barely beat the Netflix benchmark and realized that it required more time and hardware power than I was able to dedicate at the time.\nFortunately, many Kaggle competitions have a more manageable scope and can be done as a hobby rather than full time job.\u00a0 Mapping Dark Matter was my third Kaggle competition; before that I came second in the RTA competition and made one submission in the Chess Rating challenge.\nWhat was your most important insight into the dataset?\nInitially, I was trying to use a modified quadruple moments formula and fitting procedure. \u00a0However, I was not satisfied with the result.\u00a0 My mind was constantly coming back to neural networks. \u00a0The only question was \u2018what kind of parameters to use as inputs\u2019? \u00a0The number of those parameters should be reasonable, and they need to describe images well. \u00a0Thus, the images\u2019 principal components looked like the logical choice. \u00a0I calculated the positions of galaxies and stars and re-centered all the images, creating image stacks for galaxies and stars separately. \u00a0I then calculated the principle components for those stacks.\nTo my surprise, many of the principle components were easy to understand. \u00a0Here are components #2 and #3 and a scatter plot where x and y are amplitudes of components #2 and #3 and color corresponds to galaxy orientations:\n\n\n\n\n\n\n\n\n\n\n\n\nComponents #2 and #3 are quadruples with shifted phase and definitely reflect the orientation of elongated galaxies.\u00a0 Many other components were also easy to interpret. \u00a0I used some of them to improve the center of object calculation. \u00a0The amplitudes of principle components for galaxies and stars served as inputs to the neural network. \u00a0I played with the network parameters until I found a good combination of the number of parameters and the network configuration. \u00a0In the end, I combined the results of multiple networks to calculate my final submissions.\nWhich tools did you use?\nAll calculations were done using Matlab.\nThank you Sergey!\n"}, {"url": "http://blog.kaggle.com/2011/05/29/the-thrill-of-the-chase-tim-salisman-on-how-he-took-home-deloittefide-chess-comp/", "link_text": "The thrill of the chase: Tim Salimans on how he took home Deloitte/Fide chess comp", "id": 70, "title": "Permalink to The thrill of the chase: Tim Salimans on how he took home Deloitte/Fide chess comp", "date": "2011-05-29", "text": "\nMy name is Tim Salimans and I am a PhD candidate in Econometrics at Erasmus University Rotterdam. For my job I constantly work with data, models, and algorithms, and the Kaggle competitions seemed like a fun way of using these skills in a competitive and social environment. The Deloitte/FIDE Chess Rating Challenge was the first Kaggle contest I entered and I was very fortunate to end up taking first place. During the same period I also used Kaggle-in-class to host a prediction contest for an undergraduate course in Econometrics for which I was the teaching assistant. Both proved to be a lot of fun. This post is a nontechnical account of my experiences in the chess rating challenge. For more details, including my code, see my web page.\n\nChess rating systems\nThe first thing to do when tackling a new problem is to look up what other people have done before you. Since Kaggle had already organized an earlier chess rating competition, the blog posts of the winners were a logical place to start. After reading those posts and some of the academic literature, I found that chess rating systems usually work by assuming that each player\u2019s characteristics can be described by a single rating number. The predicted result for a match between two players is then taken to be some function of the difference between their ratings. Yannis Sismanis, the winner of the first competition, used a logistic curve for this purpose and estimated the rating numbers by minimizing a regularized version of the model fit. Jeremy Howard, the runner-up, instead used the TrueSkill model, which uses a Gaussian cumulative density function and estimates the ratings using approximate Bayesian inference.\nI decided to start out with the TrueSkill model and to extend it by shrinking each player\u2019s rating to that of their recent opponents, similar to what Yannis Sismanis had done in the first competition. In addition, I introduced weights into the algorithm which allowed me to put most emphasis on the matches that were played most recently. After some initial experimentation using the excellent Infer.NET package, I programmed everything in Matlab.\nUsing the match schedule\nThe predictions of my base model scored very well on the leaderboard of the competition, but they were not yet good enough to put me in first place. It was at this time that I realized that the match schedule itself contained useful information for predicting the results, something that had already been noticed by some of the other competitors. In chess, most tournaments are organized using to the Swiss system, in which in each round players are paired with other players that have achieved a comparable performance in earlier rounds. If in a Swiss system tournament player A has encountered better opponents than player B, this most likely means that player A has won a larger percentage of his/her matches in that tournament.\nIn order to incorporate the information present in the match schedule, I generated out-of-sample predictions for the last 1.5 years of the data using a rolling 3-month prediction window. I then performed two post-processing steps using these predictions and the realized match outcomes. The first step used standard logistic regression and the second step used a locally weighted variant of logistic regression. The most important variables used in the post-processing procedure were:\n\n- the predictions of the base model\n- the ratings of the players\n- the number of matches played by each player\n- the ratings of the opponents encountered by each player\n- the variation in the quality of the opponents encountered\n- the average predicted win percentage over all matches in the same month for each player\n- the predictions of a random forest using these variables\n\n\u00a0\nThis post-processing dramatically improved my score and put me well ahead of the competition for some time. Later, other competitors made similar improvements and the final weeks of the competition were very exciting. After a long weekend away towards the end of the competition I came back to find that I had been surpassed on the leaderboard by team PlanetThanet. By tweaking my approach I was able to crawl back up during the next few days, after which I had to leave for a conference in the USA. Upon arrival I learned that I was again surpassed, now by Shang Tsung. Only by making my last submissions from my hotel room in St. Louis was I finally able to secure first place.\nConclusions\nMuch of the contest came down to how to use the information in the match schedule. Although interesting in its own right, this was less than ideal for the original goal of finding a good rating system. To my relief, the follow-up data set that Jeff Sonas made available showed that my model also makes good predictions without using this information. Finally, I would like to thank both the organizers and the competitors for a great competition!\n"}, {"url": "http://blog.kaggle.com/2011/04/20/mick-wagner-on-finishing-second-in-the-ford-challenge/", "link_text": "Mick Wagner on finishing second in the Ford Challenge", "id": 71, "title": "Permalink to Mick Wagner on finishing second in the Ford Challenge", "date": "2011-04-20", "text": "\nBackground\nMy name is Mick Wagner and I worked by myself on this challenge in my free time.\u00a0 I am fairly new to data mining but have been working in Business Intelligence the last 5 years.\u00a0 I am a senior consultant in the Data Management and Business Intelligence practice at Logic20/20 in Seattle, WA.\u00a0 My undergrad degree is in Industrial Engineering with an emphasis on Operations Research and Management Science out of Montana State University.\u00a0 \u00a0This is my second Kaggle competition I have entered.\n\nThe Stay Alert challenge was sponsored by Ford to help prevent distracted drivers.\u00a0 The objective of this challenge is to design a detector/classifier that will detect whether the driver is alert or not alert, employing any combination of vehicular, environmental and driver physiological data that are acquired while driving.\nThe data for this challenge shows the results of a number of \"trials\", each one representing about 2 minutes of sequential data that are recorded every 100 ms during a driving session on the road or in a driving simulator.\u00a0 The trials are samples from some 100 drivers of both genders, and of different ages and ethnic backgrounds.\u00a0 The actual names and measurement units of the physiological, environmental and vehicular data were not disclosed in this challenge. Models which use fewer physiological variables are of particular interest; therefore competitors are encouraged to consider models which require fewer of these variables.\nTools Used\nMicrosoft Excel\nMicrosoft SQL Server Stack (SQL Server Engine, SQL Server Integration Services, SQL Server Analysis Services)\nData Analysis\nI spent the majority of my time analyzing the data.\u00a0 I inputted the data into Excel and started examining the data taking note of discrete and continuous values, category based parameters, and simple statistics (mean, median, variance, coefficient of variance).\u00a0 I also looked for extreme outliers.\nI read through the Kaggle discussion board to see if anything about the challenge had been changed or if Ford provided additional insight into the data.\nThe most important step I made was to take a step back and holistically examine the problem and the dataset.\u00a0 The data was composed of various trials with different human experimenters.\u00a0 My background in statistical quality control immediately told me that this would have a large impact on the statistical analysis and needed to be factored into the design of the system.\u00a0 To do this, I created my own sets of test and training data.\u00a0 Normally, Microsoft SQL Server randomly derives these sets based off of a HoldOutMaxPercent value that dictates the Test data size.\u00a0 I made the first 150 trials (~30%) be my test data and the remainder be my training dataset (~70%).\u00a0 This single factor had the largest impact on the accuracy of my final model.\nMy next breakthrough decision was limiting the copious amount of data that went into my algorithm.\u00a0 I was concerned that using the entire data set would create too much noise and lead to inaccuracies in the model.\u00a0 The final goal of the system is to detect the change in the driver from alert to not alert so that the car can self-correct or alert the driver.\u00a0 So I decided to just focus on the data at the initial moment when the driver lost alertness.\u00a0 This reduced my dataset significantly and I repeated my initial Excel analysis of the data.\u00a0 I highlighted which factors consistently had a relatively large delta between status changes.\nFailed Attempts\nMy first few attempts were running different types of models with the all the variables and Microsoft\u2019s recommended variables.\u00a0 I used a lift chart to compare their accuracy to each other.\u00a0 My goal was to narrow down the possible algorithms given by Microsoft SQL Server Analysis Services from seven down to 2.\u00a0 Several models (Association Rules, Linear Regression, and Logistic Regression) did not make sense to use because of the data types, structure of the data, and desired binary output.\u00a0 The Decision Tree and Neural Network scored the highest on my lift chart.\nWhat Ended Up Working\nAfter testing the Decision Tree and Neural Network algorithms against each other and submitting models to Kaggle, I found the Neural Network model to be more accurate.\nAfter initially trying the variables recommended by SSAS (SQL Server Analysis Services), I augmented the solution with the variables I found key in my initial analysis.\u00a0 These variables included: E4, E5, E6, E7, E8, E9, E10, P6, V4, V6, V10, and V11.\nAs recommended by Ford, I tried to avoid relying on physiological variables.\u00a0 I did find P6 to be helpful though.\nThe key strengths of my model are ease to build, ease to deploy and maintain in an industry setting with transactional data, and scalability within SQL Server.\u00a0 My model uses an out of the box algorithm that is well understood and respected.\u00a0 It also took me significantly less attempts (and time) to scale to the top of the standings:\n1st place: 24 entries\n2nd place (me): 8 entries\n3rd place: 39 entries\n4th place: 25 entries\n\u00a0\n"}, {"url": "http://blog.kaggle.com/2011/03/25/jose-p-gonzalez-brenes-and-matias-cortes-on-winning-the-rta-challenge/", "link_text": "Jos\u00e9 P. Gonz\u00e1lez-Brenes and Mat\u00edas Cort\u00e9s on winning the RTA challenge", "id": 72, "title": "Permalink to Jos\u00e9 P. Gonz\u00e1lez-Brenes and Mat\u00edas Cort\u00e9s on winning the RTA challenge", "date": "2011-03-25", "text": "\n\nWe are Team Iraz\u00fa, Jos\u00e9 P. Gonz\u00e1lez-Brenes and Mat\u00edas Cort\u00e9s. We finished 1st in the RTA  Challenge.\nWhat didn\u2019t  work\nWe started off exploring the data by calculating means for  different combinations of time, day of week and month. We plotted these means to  identify patterns in the data. We also explored forecasting based on linear regressions.\n\nWe realized that including historical data led to less  accurate predictions, so we attempted using weighted linear regressions where  proportionally higher weights were assigned to more recent datapoints.\nWe also tried forecasting through autoregressions, where the  preceding n observations (t-1, t-2, \u2026, t-n) are used to make a prediction.\nIn the end, we decided to work only with the more recent  dataset, and ignore the historical data provided by Kaggle.\nWhat ended up  working\nWe used a statistical technique called Ensemble of Decision  Trees (often called Random Forest\u2122).\u00a0 One  of the nice properties of this approach is that it doesn\u2019t assume a linear  relationship between explanatory variables and the predicted outcome.\nWe used the following explanatory variables: \n\nWhat time is it? (Hour + (Minute/60))\nWhat is the date? (Month + (Day/31))\nWhat day of the week is it? (Monday, Tuesday\u2026)\n\n\u201cAll roads are equal,  but some roads are more equal than others!\u201d \nOur final algorithm combines two methods. Both methods  include time, date, and day of week as explanatory variables.\n \nMethod A:\nAdditionally encodes most recent observation for 2 neighboring routes\nMethod B:\nAdditionally encodes most recent observation for 4 neighboring routes and recent trend in travel time\n\n\n\n\u00a0\n\n\nWe tried Method A and Method B, and discovered that for some  route segments, Method A performed better, while for others, Method B performed  better!\u00a0\n\u00a0\nOur final algorithm uses:\nMethod A for route segments 40105-41160\nMethod B for route segments 40010-40100 \n\n\n\n\u00a0\nWe prepared a PDF providing more details about Random Forests  and our solution. Click here to read  it!\n\n"}, {"url": "http://blog.kaggle.com/2011/03/25/inference-on-winning-the-ford-stay-alert-competition/", "link_text": "Inference on winning the Ford Stay Alert competition", "id": 73, "title": "Permalink to Inference on winning the Ford Stay Alert competition", "date": "2011-03-25", "text": "\nThe \u201cStay Alert!\u201d competition from Ford \u00a0challenged competitors to predict whether a car driver was not alert based on various measured features.\nThe \u00a0training \u00a0data \u00a0was \u00a0broken \u00a0into \u00a0500 \u00a0trials, \u00a0each \u00a0trial \u00a0consisted \u00a0of a \u00a0sequence \u00a0of \u00a0approximately \u00a01200 \u00a0measurements \u00a0spaced \u00a0by \u00a00.1 \u00a0seconds. Each measurement consisted of 30 features; \u00a0these features were presentedin three sets: \u00a0physiological (P1...P8), environmental (E1...E11) and vehic-ular (V1...V11). \u00a0 Each feature was presented as a real number. \u00a0 For each measurement we were also told whether the driver was alert or not at thattime (a boolean label called IsAlert). \u00a0No more information on the features was available.\n\nThe test data consisted of 100 similar trials but with the IsAlert label hidden. 30% of this set was used for the leaderboard during the competition and 70% was reserved for the \ufb01nal leaderboard. \u00a0Competitors were invited to submit a real number prediction for each hidden IsAlert label. \u00a0This realprediction should be convertible to a boolean decision by comparison with a threshold.\nThe accuracy assessment criteria used was \u201carea under the curve\u201d (AUC). \u00a0 The \u201ccurve\u201d is the receiver-operating characteristic (ROC) curve where \u00a0the \u00a0true-positive \u00a0rate \u00a0is \u00a0plotted \u00a0against \u00a0false-positive \u00a0rate \u00a0as \u00a0this threshold is varied. \u00a0An AUC value will typically vary between 0.5 (random guessing) and 1 (perfect prediction).\nSee the full explanation of Inference's method in the attached PDF.\n"}, {"url": "http://blog.kaggle.com/2011/03/23/getting-in-shape-for-the-sport-of-data-sciencetalk-by-jeremy-howard/", "link_text": "\u201cGetting In Shape For The Sport Of Data Science\u201d\u2013Talk by Jeremy Howard", "id": 74, "title": "Permalink to \u201cGetting In Shape For The Sport Of Data Science\u201d\u2013Talk by Jeremy Howard", "date": "2011-03-23", "text": "\n\nI recently gave a talk to the local R meetup group, in which I gave a brief overview of my \u201cdata scientist\u2019s toolbox\u201d (using a few Kaggle competitions as practical examples), and also provided an introduction to ensembles of decision trees (including the well-known Random Forest\u2122 algorithm).\n\u00a0\n"}, {"url": "http://blog.kaggle.com/2011/03/22/phil-brierley-on-winning-tourism-forecasting-part-two/", "link_text": "Phil Brierley on winning tourism forecasting part two", "id": 75, "title": "Permalink to Phil Brierley on winning tourism forecasting part two", "date": "2011-03-22", "text": "\nI was Team \u201cSali Mali\u201d which won the seasonal part of the online tourism forecasting competition. The aim was to produce the smallest MASE for the 427 quarterly time series and 366 monthly timeseries. In this article, I brie\ufb02y describe the methods used.\n\nBasic algorithms\nNo new time series forecasting algorithms were developed speci\ufb01cally for this contest. We basically took algorithms that already existed as \u2018building blocks\u2019 and combined their forecasts in speci\ufb01c ways. Based on Athanasopoulos et al. (2011), and using the R package \u2018forecast\u2019 (Hyndman, 2011), four base algorithms were used:\n1. \u00a0Seasonal Na\u00efve;\n2. Damped Holt-Winters\n3. ARIMA;\n4. ETS.The benchmark to beat was the ETS algorithm.\nThe benchmark forecasts were replicated and visually inspected and it became clear that the forecasts for some of the series (only one or two) were clearly \u2018unlikely\u2019, in the sense that they basically went o\ufb00 the scale. The other algorithm sseemed to give much more realistic forecasts for these particular series.\nThe approach then taken was to concentrate on protecting against these \u2018disastrous\u2019 forecasts. The mindset was not thinking how to improve the overall accuracy, but how to prevent the worst case events. One way of achieving this is by not putting all your eggs in one basket (i.e. only relying ona single algorithm). This technique is commonly known as \u2018ensembling\u2019\nAn ensemble approach\nTwo methods were used in the ensembling process:\n1. \u00a0The last 12 months of the training data was set aside as a holdout set, and the MASE acrossall the series was calculated for each algorithm on this set. Based on these MASE values, aweighting was assigned to each algorithm, with the total of the weightings summing to 1.\u00a0Thus four predictions were made for each series, with the \ufb01nal prediction being a weighted av-erage. The weights for each algorithm where consistent across each series within the monthlyand quarterly series types. This global weighted average method gave an improvement overthe baseline method.\n2. Forecasts for three algorithms (Damped, ARIMA, ETS) were generated using four di\ufb00erentsized training windows. The Seasonal Na\u00efve forecast was then added, to give 13 forecasts foreach point in each series. The \ufb01nal forecast for each point was then the median value of these 13 individual forecasts.\u00a0This local selection method also gave an improvement over the baseline method.\nThe \ufb01nal solution was then a weighted average of methods 1) & 2).\nThe cheat factor\nIf the \ufb01rst 12 months of the benchmark forecast is simply replicated for the second 12 months, rather than relying on the algorithms forecasts for months 13-24, then the overall benchmark accuracy was improved. This is in line with the organisers\u2019 \ufb01ndings that as the time horizon increases, the gain in accuracy of certain algorithms over the seasonal na\u00efve method was diminished. The leaderboard was used to determine the \u2018year two growth factor\u2019 that should be applied. It was found that just multiplying the \ufb01rst year\u2019s predictions by approximately 1.04 gave the best second year predictions, as determined by the leaderboard score. In other words, the forecasts for months 13\u201324 obtained from the algorithm were completely disregarded and replaced by a simple multiple of the \ufb01rst year\u2019s forecasts.\nComments on the competition\nThe prediction dates for the time series are more than likely to be the last two years available, meaning that the physical dates are the same for many series (this hypothesis is based on our \ufb01nding that a growth rate of 1.04 seemed to work across all series). Because tourism \ufb01gures are likely to be a\ufb00ected by global factors (GFC, exchange rates, etc., that are largely unpredictable) it is likely the values will all trend similarly. Thus the conclusion that one algorithm is better for tourism data than another algorithm must be treated with caution as it might just be the algorithm that fortunately got the trend correct for that particular moment in time. \u00a0It is suggested that more generalizable results may have been obtained if the series were deliberately staggered in time.\nIt seems common practice to report time series on a calendar month basis. \u00a0In reality people operate on a weekly cycle, not a monthly cycle. This causes problems in series that have signi\ufb01cant daily \ufb02uctuations \u2014 for example car hire volumes can be very di\ufb00erent during the week than the weekend. The implication for forecasting is that a big di\ufb00erence between one month and the next, or the same month in the previous year, can be due to the di\ufb00erence in having four weekends in a month and \ufb01ve weekends in a month. Reporting tourism \ufb01gures in a four weekly cycle rather than a monthly cycle would lead to improvements in forecast accuracy, and this may give better reward for e\ufb00ort than additional algorithm development.\n"}, {"url": "http://blog.kaggle.com/2011/03/16/junpei-komiyama-on-finishing-4th-in-the-ford-competition/", "link_text": "Junpei Komiyama on finishing 4th in the Ford competition", "id": 76, "title": "Permalink to Junpei Komiyama on finishing 4th in the Ford competition", "date": "2011-03-16", "text": "\nMy background\nMy name is Junpei Komiyama. I obtain a Master's degree in computational and statistical physics at The University of Tokyo, Japan. I have been working in a team developing a live-streaming website (http://live.nicovideo.jp) for two years, contributing mainly to designing and implementation of DB tables, cache structures, and front-end programs of the site.\n\nWhat I tried\nEach team received three sets of data: Training data, testing data and submission example. The training and the test data are the records of sequential observations of car drivers. The observation data consisted of eight physiological data (P1-P8), eleven environmental data (E1-E11) and eleven vehicular data (V1-V11), recorded in every 100ms. Information about meaning of each data was not given. The training data consisted of 500 car drivers' records of observation data associated with judgments as to whether each driver was alert or not. The test data set consisted of another 100 drivers' records of observation and had no overlap with the training data. We were requested to estimate whether each driver in the test data set was alert or not. Each team was evaluated by the value of\u201carea under the receiver-operating-characteristics curve (AUC)\". All data in the training set were used for machine training. To solve this problem, I constructed a Support Vector Machine (SVM), which is one of the best tools for classification and regression analysis, using the libSVM package.\nIn my first attempt, I simply put the training data into libSVM with default settings (C-SVC and RBF kernel) and evaluated the test data with the trained model. This approach took more than 3 hours to complete the training step and yielded a modest AUC score (0.745). Meanwhile, observations were visualized for each person using python/gnuplot. I found some data (P3-P6) were characterized by strong noise, fluctuating more than 50 percent for every 100ms. Also, many environmental and vehicular data showed discrete values continuously increased and decreased. These suggested the necessity of pre-processing the observation data before SVM analysis for better performance.\nTo improve the AUC score, I took the following approaches:\n1.To smoothen the observation data by removing extremely small temporal values. Sometimes observation values dropped to nearly zero and then immediately resumed to the former level, presumably due to failure of observation. Therefore, I attempted to remove such instantaneous near-zero values. However, this attempt rather caused slight reduction in the AUC score and so was abandoned.\n2.To integrate the present and old time points in the observation data. The observation records were taken at every 100 ms (milliseconds), which is fairly more frequent than many events of human activities. I assumed that changes in the alert state might have a certain length of delay after an observation was made. I tested this possibility by averaging each observation datum point with an old datum point collected 100-500 ms before. However, such an attempt of data integration did not improve the AUC score.\n3.To average several datum points. I found that this simple process not only improved the AUC score and also significantly reduced computing time. Therefore, this approach was taken for further optimization. Since there was the two-submissions-per-day limit for evaluation, it was necessary to examine changes in performance in a local setting through cross-validation of the SVM training data set.\nWhat ended up working\nPre-processing before SVM: I attempted to determine the optimal number of datum points to be averaged. Averaging 100 rows into a single row resulted in too coarse data whereas averaging only 3 rows had little effect. I empirically determined that averaging 7 consecutive datum points provided the optimal result, reducing the SVM training time by 86% and increasing the AUC by approximately 0.01.\nSVM: Among the available options offered by libSVM (i.e., C-SVC, nu-SVC, one-class SVM, epsilon-SVR and nu-SVR), the epsilon-SVR performed best for this problem, enhancing the AUC than C-SVC/option -b by about 0.05. Linear, polynomial, RBF and sigmoid kernels were then tested. I found that kernel choice had relatively minor effects although RBF kernel performed best.\nWith these optimized SVM type and kernel function, I optimized the SVM parameters namely, cost parameter c, kernel parameter g, and loss function parameter p.\nIn the final setting, epsilon-SVR, RBF kernel with optimized parameters (c=2, g=1/30 and p=0.1) yielded a greatly improved AUC (0.839).\nTools I used\nPython for processing the CSV data.\nGnuplot for visualizing the observation data.\nLibSVM for constructing a SVM model, including processes of training and prediction.\n"}, {"url": "http://blog.kaggle.com/2011/03/01/yuanchen-he-on-finishing-third-in-the-melbourne-university-competition/", "link_text": "Yuanchen He on finishing third in the Melbourne University competition", "id": 77, "title": "Permalink to Yuanchen He on finishing third in the Melbourne University competition", "date": "2011-03-01", "text": "\nBackground \nI am Yuanchen He, a senior engineer in McAfee lab. I have been working on large data analysis and classification modeling for network security problems.\nMethod\nMany thanks to Kaggle for setting up this competition. And congratulations to the winners! I enjoyed it and learned a lot from working on this challenging data and reading the winners' posts. \u00a0I am sorry I didn't find free time last week to write this report.\n\nThe data came with a lot of categorical features with a high number of values. At the very beginning, I removed useless features (by weka.filters.unsupervised.attribute.RemoveUseless -M 99.0) and removed the features with almost 100% missing values. After that, I tried to transform the categorical features into a group of binary features with each is a yes or no on a specific value. I also generated 4 quarter features and 12 month features from startdate and generated binary indicator features for missing values. The binary features, date-based features, indicator features, as well as other numerical features, after simply filling missing values with mean, were fed into R randomForest classifier for RFE. With that I got 94.9x on the leaderboard. I kept tuning along this way but the accuracy cannot be improved further. Then I started to suspect there were some information loss during the process of feature transformation and feature selection.\nSo I tried to build classifiers directly on the categorical features without transforming them into binary features. A simple frequency based pre-filtering was applied. For a raw categorical feature, all values presented less than 10 instances in the data were combined into a specific common value \"-1\". However, R randomForest cannot accept a categorical feature with more than 32 values. So I had to split each categorical feature again into \"sub features\", with each has no more than 32 values. The way I split the values into different sub features was sorting the values with information gain first, and then top 31 values were assigned into sub feature 1, the next 31 values were assigned into sub feature 2, and so on. With this feature transformation strategy I got 94.6x on the leaderboard.\nThe next one I tried was simply combining the top features from the above two methods. The randomForest classifiers on the combined feature sets can improve the leaderboard ROC to 95.1x-95.3x, depending on the instances used for training. The best classifiers were generated from training only on instances after 0606, only on instances after 0612, and only on instances after 0706. Finally, I observed the prediction results from these classifiers were different enough and hence it was worth to make a major voting from them, and I got my best leaderboard AUC 95.555, which generalized to the other 75% test instances with the final AUC 96.1051\n"}, {"url": "http://blog.kaggle.com/2011/02/21/quan-sun-on-finishing-in-second-place-in-predict-grant-applications/", "link_text": "Quan Sun on finishing in second place in Predict Grant Applications", "id": 78, "title": "Permalink to Quan Sun on finishing in second place in Predict Grant Applications", "date": "2011-02-21", "text": "\nMy background\nI'm a PhD student of the Machine Learning Group in the University of Waikato, Hamilton, New Zealand. I\u2019m also a part-time software developer for 11ants analytics. My PhD research focuses on meta-learning and the full model selection problem. In 2009 and 2010, I participated the UCSD/FICO data mining contests.\n\nWhat I tried and What ended up working\nI tried many different algorithms (mainly weka and matlab implementations) and feature sets in nearly 80 submissions. This report will briefly introduce two approaches that worked for this competition. Each of them will be discussed sequentially in the order of submissions.\nAfter the first 10 testing submissions, I realised that there was a concept drift happening between 2007 and 2008. The success rates decline gradually from 2007. Also, on the information page of the contest, it states that \u201cIn Australia, success rates have fallen to 20-25 per cent\u2026\u201d. To me, this probably means, the decision rules for grant applications were somehow changed during 2007 and 2008. Here are some consequences that I could think of, including but not limited to:\n\nThe overall success rates will continue to drop\nSuccessful applications in 2005/2006 would be declined in 2007/2008, so for 2009/2010\nSuccess patterns becoming to be \u201cmore\u201d random\nDecision rules for year 2009/2010 will be close to that for 2007/2008, compared with rules for year 2006 and prior.\n\nBased on the information and assumptions above, I decided to mainly use data points from 2007 and 2008 for training my classifiers, which turns out to be a reasonable choice.\nApproach A: Ensemble Selection with transformed feature set (used in the first 20 submissions)\nData engineering/transformation part\n\n\n\nOriginal attribute\nTransformation method\n\n\nStart.date\nto numeric, year, month,   day in numbers\n\n\nRFCD.Code.X (X=1 to 5)\nto nominal\n\n\nPerson.ID.X (X=1 to 15)\nto nominal\n\n\nNumber.of.Grant.X (X=1 to   15)\nTotal number of successful/unsuccessful   grants per application\n\n\nPublications AA, A, B, C\nTotal number of AA, A, B, C   publications per application\n\n\nRole.X\nTotal number\u00a0 of CHIEF_INVESTIGATORs,   PRINCIPAL_SUPERVISORs, DELEGATED_RESEARCHER, EXT_CHIEF_INVESTIGATORs per   application\n\n\nCountry.of.Birth.X\nTotal number of   Asia_Pacific born, Australia, Great_Britain, Western_Europe, Eastern_Europe,   North_America, New_Zealand, Middle_East_and_Africa per application\n\n\nWith.PHD\nTotal number of PhDs per application\n\n\nYears.IN.UNI\nTotal number of people who   has been in the University for more than 5 years\n\n\n\nAfter all those transformations are done, I also had a java program to transform all nominal attributes to its corresponding frequency. The frequency counting is based on all the available data points. So, the final feature set consists of the original features, transformed features and frequency.\nModeling part\nMy main method is called Ensemble Selection, originally proposed by Rich Caruana and co-authors of Cornell University (http://portal.acm.org/citation.cfm?id=1015432). The following pseudocode demonstrates the basic idea of Ensemble Selection:\n0. Split the data into two parts: The build set and the hillclimb set\n1. Start with the empty ensemble.\n2. Add to the ensemble the model (trained on \u201cbuild\u201d set) in the library that maximizes the ensemble\u2019s performance to the error metric (AUC for this contest) on a \u201chillclimb\u201d (validation) set.\n3. Repeat Step 2 for a \ufb01xed number of iterations or until all the models have been used.\n4. Return the ensemble from the nested set of ensembles that has maximum performance on the hillclimb (validation) set.\nModel library used for my Ensemble Selection system:\nAdaBoost, LogitBoost, RealAdaBoost, DecisionTable, RotationForest, BayesNet, NaiveBayes, 7 algorithms with different parameters, in total 28 base classifiers.\nBuilding set and hillclimb set for Ensemble Selection:\nData points from year 2007 are used as the \u201cbuild set\u201d\nData points from year 2008 are used as the \u201chillclimb set\u201d\nOr\nData points from year 2007/01/01 to 2008/04/30 are used as the \u201cbuild set\u201d\nData points after year 2008/04/30 are used as the \u201chillclimb set\u201d\nBoth setups worked well for the Ensemble Selection approach.\nIn summary, the final system for Approach A consists of three main components:\nData points from 2007 for training and 2008 for hillclimbing.\nEnsemble Selection, num of bags: 10, hillclimb iterations = size of the model library.\nIn total 352 features.\nLearderboard AUC: 0.956X, Best final test set AUC: 0.961X\nFrom submission 20 to the end of the competition, the following features are added to Approach A feature set:\nNumber of missing values\nNumber of non-missing values\nMissing value rate\nTransform \u201cContract.Value.Band\u201d to numeric values\nAverage contract value\nRFCD.CODE mean, sum, max, min, standard deviation per application based\nRFCD.PCT mean, sum, max, min, std per application based\nSEO.CODE mean, sum, max, min, std per application based\nSEO.PCT mean, sum, max, min, std per application based\nSuccessful.grant mean, sum, max, min, std per application based\nUnsuccessful.grant mean, sum, max, min, std per application based\nSuccessful.grant mean average per application based\nSuccessful.grant sum average per application based\nAll the above features for the first three applicants\nAll the above features for Unsuccessful.grant\nSuccess rate of applicant 1, applicant 2, and applicant 3 per application based\nSuccess rate of all applicants per application based\nMean, max, std success rates of all applicants per application based\nNumber of publications mean, sum, max, min, std per application based\nExcept the frequency counting described in Approach A, only \u201crow-based (per-application-based)\u201d statistical features were gradually introduced to my system during the competition, because I thought that, compared with \u201ctime based/column based features\u201d, \u201crow-based\u201d statistical features would reduce the chance of overfitting.\nAlso, the following algorithms (with different/diverse parameter settings) were gradually added to the model library while the competition:\nRandomForest\nRacedIncrementalLogitBoost\nBagging with trees\nADTree\nLinear Regression\nRandomCommittee with Random Trees\nDagging\nJ48\nApproach B: Rotation Forest with the feature set from Approach A\n \nI tried using only Rotation forest (http://www.computer.org/portal/web/csdl/doi/10.1109/TPAMI.2006.211) with the following setup:\nBase classifier: M5P model tree (weka default is J48)\nRotation method: Random Projection with Gaussian distribution (weka default is PCA)\nThe Rotation forest classifier was trained on data points from 2007 and 2008 with the feature set from Approach A. Here are the results:\nLeaderboard AUC: 0.947X, Final test set AUC: 0.962X\nAveraging the two approaches could improve the final test set AUC to 0.963X.\n \n \nWhat tools I used\n \nSoftware/Tools used for modelling and data analysis:\nWeka 3.7.1 is used for modelling (with my own improved version of the Ensemble Selection algorithm)\nMatlab and SAS are used for data visualization and statistical analysis\nJava is used as the main programming language for this project\nMost experiments were done on my home PC: AMD 6-core, 16G ram on Windows system.\n"}, {"url": "http://blog.kaggle.com/2011/02/21/max-lin-on-finishing-second-in-the-r-challenge/", "link_text": "Max Lin on finishing second in the R Challenge", "id": 79, "title": "Permalink to Max Lin on finishing second in the R Challenge", "date": "2011-02-21", "text": "\nI participated in the R package recommendation engine competition on Kaggle for two reasons.  First, I use R a lot.  I cannot learn statistics without R.  This competition is my chance to give back to the community a R package recommendation engine.  Second, during my day job as an engineer behind a machine learning service in the cloud, product recommendation is one of the most popular applications our early adopters want to use the web service for.  This competition is my opportunity to stand in users' shoes and identify their pain points associated with building a recommendation system.\n\nI treat R package recommendation as a binary classification task: a classifier $latex f(u, p)$ takes as input a user $latex u$ and a package $latex p$, and predicts whether the user will install the package or not.  My final submission (team name: Record Me Men) combines four classifiers, labeled as large dots in Figure 1. The four classifiers share the same form of objective function that minimizes loss $latex L$ plus regularizers $latex R$:\n$latex J(\\theta) = \\sum_i L(y_i, f(u_i, p_i; \\theta)) + \\lambda R(\\theta)$\n, where $latex f(u, v)$ is a classification model, $latex \\theta$ is the parameters of the classification model, $latex y_i$, $latex u_i$, $latex p_i$ are the class label (package installed or not), user and package of the i-th training example, respectively. $latex \\lambda$ controls the penalty from the regularizing function, and is chosen using cross validation.\nModel 1: Baseline.  Model 1 is example_model_2.R that the competition organizer provides as a baseline.  In this model, a user is encoded as dummy variables $latex \\mathbf{u}$, and a package is represented by seven features $latex \\mathbf{p} = \\{p_1, \\dots, p_7\\}$ (e.g, the logarithmic count of dependency.  See the R script for the complete list).  The classification model is a linear combination of user dummy variables and package features with weight parameters, $latex f(u, p) = \\mathbf{\\theta_u}^T \\mathbf{u} + \\mathbf{\\theta_p}^T \\mathbf{p}$.  The parameters are $latex \\theta_u$ and $latex \\theta_p$ (and intercept).  The loss function $latex L$ is negative logistic log likelihood.  There is no regularizer in the model.\nThe first model establishes a strong baseline, achieving AUC of ~0.94, labeled as M1b2 in Figure 1.  A variant of this model that omits the user feature (see example_model_1.R, also provided by the contest organizers) achieves noticeable lower AUC of 0.81 (not shown in Figure 1).  This suggests that some users are more likely to install R packages than others.  In the next model, I will explore a classification model that incorporates not only user variations but also package variations.\nModel 2: Latent factor Models. In contrast to Model 1 with features derived from metadata, I focus on the user-package rating matrix.  Model 2 consists of two components: baseline estimates and latent factors.  Baseline estimates are linear combinations of three parts: global (one parameter $latex \\mu$), user (one parameter per user $latex \\mu_u$), and package (one parameter per package $latex \\mu_p$).  For latent factors, I assumes that there are K latent factors for each user, $latex \\mathbf{\\beta}_u$ and each package $latex \\mathbf{\\beta}_p$, and the inner product of these two factors captures the interaction between a user and a package.  The classifier model in Model 2 is $latex f(u, p) = \\mu + \\mu_u + \\mu_p + \\mathbf{\\beta}_u^T \\mathbf{\\beta}_p$.  This kind of latent factor model, also known as \"singular value decomposition\",  has been reported with great success in previous collaborative filtering studies and at the Netlifx Prize.   I choose an exponential loss function for Model 2,\u00a0$latex L(y, f(u, p)) = \\exp(- y f(u, p))$,\nwhere $latex y_i \\in \\{1, -1\\}$.  I choose exponential loss over squared loss because 1) exponential loss matches 0-1 loss better than squared loss 2) exponential loss is differentiable.  I apply L2 regularizers, $latex R(\\cdot) = ||\\mu_u||^2 + ||\\mu_p||^2 + ||\\beta_u||^2 + ||\\beta_p||^2$.  I minimize the objective function using stochastic gradient descent.  The number of latent factors, K, is chosen by cross validation.\nThe latent factor model works very well on the R package recommendation data, achieving AUC of ~0.97 (See the M2 family in Figure 1).  I plot the performance of five latent factor models with different Ks, ranging from 10 to 50, labeled as M2k10, M2k20, ..., M2K50 in Figure 1.  As K increases, the latent factor model becomes more expressive and fit the data better, resulting in higher AUC.\nModel 3: Package LDA topic.  In Model 3,  I explore new features not used in Model 1 and 2.  The new feature is a package's topic based LDA (Latent Dirichlet Allocation).  The LDA topic of a package is inferred from the word counts of its man pages.  I use the topics.csv, kindly prepared by the contest organizers, to map a R package to one of the 25 LDA topics (See topic_models.R for details on running LDA, provided by the contest organizer).\nThe classification model of Model 3 is similar to Model 2.  Model 3 replaces user factors in Model 2 with T LDA factors weights $latex \\mathbf{t}_u$ (T=25 here), and replaces package factors in Model 3 with T dummy variables $latex \\mathbf{p}$.  The classification model is $latex f(u, v) = \\mu + \\mu_u + \\mu_p + \\mathbf{t}_u^T \\mathbf{p}$.  The loss function is the same as Model 2, and the regularizer is L2, $latex R(\\cdot) = ||\\mu_u||^2 + ||\\mu_p||^2 + ||\\mathbf{t}_u||^2 $.\nModel 3 achieves better AUC than Model 1, resulting in AUC of ~0.97 (labeled as M3u in Figure 1).  Prior to M3u, I explore a simpler model that users share the same weights $latex \\mathbf{t}$.  The parameter space becomes smaller, but Model 3 with shared parameters (labeled as M3b in Figure 1) performs slightly worse than Model 3 with user-specific parameters.  This makes sense because it is unlikely every R user shares the same interest in the same set of LDA topics.\nModel 4: Package task view.   R packages are organized into task views (e.g., high-performance computing,  survival analysis, and time series.  See the page for the complete list of views).  It is possible that a user interested in a particular task would install not just one but many, even all, packages in the same task view (e.g., using install.views() R function).  We could improve recommendation if we know how much a user is interested in a particular task view.  I use the views.csv, provided by the contest organizers, to map a package to its task view.\nThe classification model of Model 4 is similar to Model 3, except that LDA topics in Model 3  are replaced with task views (T=29, 28 task views plus 1 unknown view).  The performance of Model 4 is labeled as M4u in Figure 1.  Model 4 performs well and better than Model 1.  Similarly, I experiment with a variant of Model 4 that all users share the same task view parameters $latex \\mathbf{t}$, labeled as M4b i n Figure 1, and it performs worse than Model 4 with per-user parameters.  The finding is consistent with Model 3, and R users, at least on the R recommendation data set, seem to have different preferences in task views.\n\nFigure 1.  The performance of individual models on the training set.  The x-axis is selected ten models in four model families, and the y-axis is the pooled AUC over 10-fold cross validation on the training set (the higher, the better).  The larger dots are those chosen for ensemble learning.\nEnsemble learning.  I combine four classifiers using logistic regression.  To collect out-of-sample training data for the ensemble learner, I divide the training data into three sets: 80%, 10%, and 10%.  I train individual classifier on the 80%.  I then apply the trained classifiers on the 20%, and combine the scores from individual classifiers as training data for the ensemble learner. Both individual classifiers and the combiner are evaluated on the last 10% data set (as those shown in Figure 1 and Figure 2).  After evaluating one fold, I shift data and conduct another folds until all data are used, resulting in a total of 10 ensemble learners.  The output of these 10 ensemble learners are averaged to produce final predictions.\nThe ensemble learning works very well, as shown in Figure 2.  Combining M1 and M2 achieves AUC of 0.9721, which is higher than either M1 0.94 or M2 0.9702 alone.  When I combine more models and include M3 and M4, the performance gets even better.  The submission of combining four models achieves best performance on the training set among all models.  On the test set the final model achieves AUC of 0.983289 (after post-processing, see below), the highest among my submissions.  The success of ensemble training is possibly because the individual models are strong performers, and models are diverse with different classification models and features such that they complement each other.\n\n\n\nFigure 2. The performance of ensemble learning models. The x-axis is three ensembles, ranging from two to four individual models. The y-axis is the pooled AUC of 10-fold cross validation on the training set.  The dash lines are the performance of four individual models.\n\n\nPost-processing.  I apply two post-processing steps before submitting each entry.  First, the label and (user, package) association on the training set is memorized.  For any (user, package) pairs in the test set that are already seen in the training set,  their labels on the training set are recalled and used instead.  Second, I assume when a user install a package P, the user also installs the packages that P depends on.  I record all packages a user installs on the training set as well as their dependent packages.  Given a pair (U, P) on the test set, if the package P is found in the set of the packages on which the user U' installed packages depend, I ignore the prediction and output 1.0 instead.  Although the dependency assumption does not completely hold true (there are known contradictory examples on the training set),  the two filters combined increases absolute AUC ~0.004, which matters in a competition with razor-thin margins between leading submissions.\nI develop the classification training programs for Model 2, 3, and 4 in Python.  I use R to explore data, run logistic regression (glm() in the stats library), calculate AUC (performance() in the ROCR library), and plot results (ggplot() in the ggplot2 library).  All programs are developed and run on a commodity PC with dual-core 2GHz CPU and 4G RAM.  I make the source codes available at github.\nAlthough my submissions are based on only the data the contest organizers provide and simple linear models, by no means you should stop here.  There are many directions worth exploring, for examples, crawling CRAN to analyze the rich graphs of depends, imports, suggests, and enhances between R packages.   In an open-end contest like this, the sky is the limit.\nMax Lin is a software engineer with Google Research in New York City office, and the tech lead of the Google Prediction API.  Prior to Google, he published research work in video content analysis, sentiment analysis, machine learning, and cross-lingual information retrieval.  He has a PhD in Computer Science from Carnegie Mellon University.\nHe would like to thank the organizers of the R package recommendation engine competition for their hard work and efforts in putting this competition together.  He participates the competition as individuals, and writes all codes in his spare time.  Nothing expressed here may or should be attached to his employer.\n"}, {"url": "http://blog.kaggle.com/2011/02/21/jeremy-howard-on-winning-the-predict-grant-applications-competition/", "link_text": "Jeremy Howard on winning the Predict Grant Applications Competition", "id": 80, "title": "Permalink to Jeremy Howard on winning the Predict Grant Applications Competition", "date": "2011-02-21", "text": "\nBecause I have recently started employment with Kaggle, I am not eligible to win any prizes. Which means the prize-winner for this comp is Quan Sun (team 'student1')! Congratulations!\nMy approach to this competition was to first analyze the data in Excel pivottables. I looked for groups which had high or low application success rates. In this way, I found a large number of strong predictors - including by date (new years day is a strong predictor, as are applications processed on a Sunday), and for many fields a null value was highly predictive.\n\nI then used C# to normalize the data into Grants and Persons objects, and constructed a dataset for modeling including these features: CatCode, NumPerPerson, PersonId, NumOnDate, AnyHasPhd, Country, Dept, DayOfWeek, HasPhd, IsNY, Month, NoClass, NoSpons, RFCD, Role, SEO, Sponsor, ValueBand, HasID, AnyHasID, AnyHasSucc, HasSucc, People.Count, AStarPapers, APapers, BPapers, CPapers, Papers, MaxAStarPapers, MaxCPapers, MaxPapers, NumSucc, NumUnsucc, MinNumSucc, MinNumUnsucc, PctRFCD, PctSEO, MaxYearBirth, MinYearUni, YearBirth, YearUni .\nMost of these are fairly obvious as to what they mean. Field names starting with 'Any' are true if any person attached to the grant has that feature (e.g. 'AnyHasPhd'). For most fields I had one predictor that just looks at person 1 (e.g. 'APapers' is number of A papers from person 1), and one for the maximum of all people in the application (e.g. 'MaxAPapers').\nOnce I had created these features, I used a generalization of the random forest algorithm to build a model. I'll try to write some detail about how this algorithm works when I have more time, but really, the difference between it and a regular random forest is not that great.\nI pre-processed the data before running it through the model by grouping up small groups in categorical variables, and replacing continuous columns with null values with 2 columns (one containing a binary predictor that is true only where the continuous column is null, the other containing the original column, with nulls replaced by the median). Other than the Excel pivottables at the start, all the pre-processing and modelling was done in C#, using libraries I developed during this competition. I hope to document and release these libraries at some point - perhaps after tuning them in future comps.\n"}, {"url": "http://blog.kaggle.com/2011/02/17/marcin-pionnier-on-finishing-5th-in-the-rta-competition/", "link_text": "Marcin Pionnier on finishing 5th in the RTA competition", "id": 81, "title": "Permalink to Marcin Pionnier on finishing 5th in the RTA competition", "date": "2011-02-17", "text": "\nMy background\nI graduated on Warsaw University of Technology with master thesis about text mining topic (intelligent web crawling methods). I work for Polish IT consulting company (Sollers Consulting), where I develop and design various insurance industry related stuff, (one of them is insurance fraud detection platform). From time to time I try to compete in data mining contests (Netflix, competitions on Kaggle and tunedit.org) - from my perspective it is a very good way to get real data mining experience.\n\nWhat I tried\nAs far as I remember, the basis of the solution I defined at the very beginning: to create separate predictors for each individual loop and time interval. So my solution required me to build 61x10=610 regression models. I was playing with various regression algorithms, but quickly chose linear regression - because the results were good and the computation time was short. I think the key to get quite good result (especially on public RMSE   ) was the set of attributes used. I used the following attributes for the linear regression for each individual loop&time interval:\n- number of minutes from 0:00 hours up to current moment (\"now\")\n- average drive time for given loop&interval\n- loop times for current moment and some number of historical moments\nbefore (the number of time points and the loop varied between the\nmethods)\n- differences between \"neighboring\" time moments for the above data:\njust differences or differences transformed with logistic function\n(1/1+e^-difference). Use of logistic function  gave a jump from public\nRMSE at about 198 to 189. The idea to use of sigmoid function here was\njust my intuition inspired by differences distribution.\n- \"saturations\" for for each loop (except the 2 first loops at both\ndirections ).\nI introduced the simple (and very naive) model of traffic growth:\nIf the speed at given loop is up to 40 km/h - the saturation is 1;\nIf the difference between the previous loop and the given loop is more than 5 km/h: it is assumed that this road part is partially saturated: there is segment that is moving at 30 km/h and second segment with the same speed as in the loop that is before given loop. The saturation is derived as the proportion of first segment to the whole road part. Each loop detector has its minimal value in RTAData file - after the regression this minimal value was used if predicted value was less than minimum.\nI did not use historical data at all - I found them useless during the initial tests (maybe too hastily). The only source of data for learning and testing was RTAData and lengths files (also no weekends, holidays, weather conditions).\nWhat ended up working\nFor each of 610 regression models the following 3 models were competing. Models were being trained with all data availabe in RTAData\nfile:\nModel 1: For all (61) loops: current + 5 times moments before and 5 simple differences - 675 attributes,\nModel 2: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 simple differences, saturations (for current time moment only) - 204 to 404 atrributes,\nModel 3: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 sigmoided differences,\nsaturations (for current time moment only) 204 to 404 atrributes,\nModel with least RMSE computed on the train file was selected for particular loop.  It is not a very good strategy, however I thought\nthat generally linear regression was resistant to overfitting (it is not true - as the number of variable grows, the more variance can be explained - this is what I have learnt).\nThis strategy gave me public RMSE 189.3\nI added also 4th model, that I just used for 15, 30 minutes predictions arbitrarily:\nModel 4: For all (61) loops: current + 5 times moments before and 5 sigmoided differences, saturations (for current time moment only) - 614 attributes. This turn gave mi 188.6 public result.\nWhat is interesting, the best private solution (however not selected by me since I relied to much on public results) was 190.819 (public 197.979) , it was just the model 3 described above combined with model 5 (model 5 was used for 15,30,45,60,90 minutes predictions arbitrarily, rest model 3):\nModel 5: like model 3 but also loop times are \"sigmoided\" not only differences.\nWhat tools I used\nMy solution is written as Java application with Weka linked as library (as always when I try to compete in data mining contests).  Since linear regression requires to solve matrix equation (in this case quite huge), the memory allocated by the program was becoming more and more important issue (3,5GB for one thread) - at the of the competition i was using computer with 4 processors and 12 GB of RAM - with 3 separate threads building and testing the models. The whole computation for my last attempts took about 48 hours of computations.\n"}, {"url": "http://blog.kaggle.com/2011/02/15/dave-slate-on-winning-the-r-challenge/", "link_text": "Dave Slate on Winning the R Challenge", "id": 82, "title": "Permalink to Dave Slate on Winning the R Challenge", "date": "2011-02-15", "text": "\nI (David Slate) am a computer scientist with over 48 years of programming experience and more than 25 years doing machine learning and predictive analytics.  Now that I am retired from full-time employment, I have endeavored to keep my skills sharp by participating in machine learning and data mining contests, usually with Peter Frey as team \"Old Dogs With New Tricks\".  Peter decided to sit this one out, so I went into it alone as \"One Old Dog\".\n\nFor this contest I used essentially the same core forecasting technology that I've employed in other contests: a home-grown variant of \"Ensemble Recursive Binary Partitioning\".  This is a robust algorithm that can handle large numbers of records and large numbers\nof feature (predictor) variables.  Both outcome and feature variables can be boolean (2 classes), categoric (multiple classes), or numeric (real numbers plus a missing value).  For the R contest the outcome was boolean, and the features provided in the training set were a mix of all three variable types.\nTo help tune modeling parameters and select feature variables, I relied both on a cross-validation procedure and also on feedback from the leaderboard.  For most of the cross-validation runs I partitioned the training data into 5 subsets of roughly equal population, trained a model on the data in 4 of the 5 subsets, tested it on the 5th to produce an AUC score, and then repeated this process 4 more times, rotating the subsets so that each subset got to play the role of test set once.  I then repeated this 5-fold procedure one more time, after scrambling the data to ensure a different partitioning, so as to produce 10 AUC scores altogether.  These were averaged together into a\u00a0composite score for the run.  I also computed a standard deviation and standard error of the mean for the 10 scores to get some idea of their statistical variability.  In the course of the competition I performed a total of 628 of these cross-validation runs.  By the time of my first submission on Dec 11, I had already done 115 of them.\nAlthough my tests involved a large number of feature variable selections and parameter settings, testing was not systematic enough to conclude that the winning model was in any way optimal.  There were too many moving parts for that.\nTo produce my first submission I used only the feature variables provided in the training set, but I enhanced the results in two ways. One was to exploit the fact that some records occurred in both the training and test sets, so that their forecasts could simply be copied from the training labels.  The other was to use the package dependency information in the depends.csv file from the supplementary archive johnmyleswhite-r_recommendation_system-36f8569.tar.gz, which, as suggested on the contest \"Data\" page, I downloaded from http://github.com/johnmyleswhite/r_recommendation_system.  For each record whose Package depended on a Package known to be not Installed\u00a0by this User, I produced the forecast 0, and for each record whose Package was depended on by a Package known to be Installed by this User, I produced the forecast 1.\nAlthough this first submission received the lowest final score (0.983419) of all my 55 submissions, it turned out that unbeknownst to me this would have been just sufficient to win the contest.\nIn the course of the contest I produced and tested a variety of additional variables, many of them based on other files in the github archive, such as imports.csv, suggests.csv, and views.csv.  I also made use of the one-line package descriptions on the \"Available Packages\" list at cran.r-project.org.  Finally, I created variables from the text in the package index pages acquired by downloading all the pages http://cran.r project.org/web/packages/PKGNAME/index.html, where PKGNAME stands for each package name.\nI failed to include in my final 5 selections the submission that received the highest final score, 0.988189.  But I did include my 2nd best (0.988157), and I'll describe that submission in some detail. Note that both of these submissions were made the day before the contest ended.\nThe winning submission model utilized 43 features.  These included the 15 provided in the training file plus 28 synthesized feature\nvariables.  Although my model-building algorithm will naturally give greater weight to highly-predictive variables, it is also possible to assign an \"a priori\" weight to each variable, and I tried various values of these.  Here is a table of feature variable names, together with their types (B = boolean/binary, C = categoric/class, N = numeric), their assigned or default relative weights, and, in the case of each B or N variable, a crude indication of its utility in the form of its correlation coefficient with the outcome (Installed).  The\nfinal column contains a brief description of the variable.\nSeveral of the synthesized features involve some crude text analysis. In the description of those features, a \"word\" refers to a contiguous sequence of alphanumeric characters, and a \"name\" is an upper case letter followed by a contiguous sequence of alphanumeric characters.\n\n\nVariable name\u00a0\nType\u00a0\nWeight\u00a0\nCorr\u00a0\nDescription or source\u00a0\n\n\nPackage             \u00a0\nC\u00a0\n0.50\u00a0\n\u00a0\nTraining file\u00a0\n\n\nUser                \u00a0\nC\u00a0\n1.00\u00a0\n\u00a0\nTraining file\u00a0\n\n\nDependencyCount     \u00a0\nN\u00a0\n1.00\u00a0\n 0.0722\u00a0\nTraining file\u00a0\n\n\nSuggestionCount     \u00a0\nN\u00a0\n1.00\u00a0\n 0.3856\u00a0\nTraining file\u00a0\n\n\nImportCount         \u00a0\nN\u00a0\n1.00\u00a0\n 0.2849\u00a0\nTraining file\u00a0\n\n\nViewsIncluding      \u00a0\nN\u00a0\n1.00\u00a0\n 0.1603\u00a0\nTraining file\u00a0\n\n\nCorePackage         \u00a0\nB\u00a0\n1.00\u00a0\n 0.0538\u00a0\nTraining file\u00a0\n\n\nRecommendedPackage  \u00a0\nB\u00a0\n1.00\u00a0\n 0.2858\u00a0\nTraining file\u00a0\n\n\nMaintainer          \u00a0\nC\u00a0\n0.80\u00a0\n\u00a0\nTraining file, but mapped to lower case and with non-alphnumerics mapped to '_'\u00a0\n\n\nPackagesMaintaining \u00a0\nN\u00a0\n1.00\u00a0\n 0.1379\u00a0\nTraining file\u00a0\n\n\nLogDependencyCount  \u00a0\nN\u00a0\n1.00\u00a0\n 0.4112\u00a0\nTraining file\u00a0\n\n\nLogSuggestionCount  \u00a0\nN\u00a0\n1.00\u00a0\n 0.4526\u00a0\nTraining file\u00a0\n\n\nLogImportCount      \u00a0\nN\u00a0\n1.00\u00a0\n 0.3464\u00a0\nTraining file\u00a0\n\n\nLogViewsIncluding   \u00a0\nN\u00a0\n1.00\u00a0\n 0.1386\u00a0\nTraining file\u00a0\n\n\nLogPackagesMaintaining\u00a0\nN\u00a0\n1.00\u00a0\n 0.1333\u00a0\nTraining file\u00a0\n\n\nMaintainerName      \u00a0\nC\u00a0\n0.25\u00a0\n\u00a0\nName extracted from Maintainer field\u00a0\n\n\nMaintainerEmail     \u00a0\nC\u00a0\n0.25\u00a0\n\u00a0\nEmail address extracted from Maintainer field\u00a0\n\n\nCountSuggest2       \u00a0\nN\u00a0\n1.00\u00a0\n 0.4184\u00a0\nCount of packages installed by User that suggest Package\u00a0\n\n\nCountImport2        \u00a0\nN\u00a0\n1.00\u00a0\n 0.2291\u00a0\nCount of packages installed by User that import Package\u00a0\n\n\nComPkgDescWordCnt   \u00a0\nN\u00a0\n1.00\u00a0\n 0.3443\u00a0\nSum, over all distinct \"words\" of length >= 5 chars in 1-line description of Package, the count of packages installed by User whose 1-line descriptions also contain this \"word\"\u00a0\n\n\nComPkgDescWordCntRat3\u00a0\nN\u00a0\n1.00\u00a0\n 0.1779\u00a0\nRelated to ComPkgDescWordCnt, but takes the ratio of sum of counts per installed package to sum per not installed\u00a0\n\n\nComPkgNamSubMatCnt  \u00a0\nN\u00a0\n1.00\u00a0\n 0.1926\u00a0\nCount of packages installed by User whose names, mapped to lower case, have at least 1 substring of length >= 5 in common with Package name\u00a0\n\n\nComPkgNamSubMatFracRat2\u00a0\nN\u00a0\n1.00\u00a0\n 0.0369\u00a0\nRelated to ComPkgNamSubMatCnt, but takes the ratio of count of matching substrings per installed package to count per not installed\u00a0\n\n\nComViewsCnt         \u00a0\nN\u00a0\n1.00\u00a0\n 0.3384\u00a0\nCount of packages installed by User with a view in common with Package\u00a0\n\n\nComViewsFracRat     \u00a0\nN\u00a0\n1.00\u00a0\n 0.1647\u00a0\nRelated to ComViewsCnt, but takes the ratio of count of installed packages to count of not installed\u00a0\n\n\nComViewsCntAll      \u00a0\nN\u00a0\n1.00\u00a0\n 0.3384\u00a0\nSame as ComViewsCnt due to a bug, but was supposed to be somewhat different\u00a0\n\n\nComViewsFracRatAll  \u00a0\nN\u00a0\n1.00\u00a0\n 0.1647\u00a0\nSame as ComViewsFracRat due to a bug, but was supposed to be somewhat different\u00a0\n\n\nDependsOnCnt        \u00a0\nN\u00a0\n1.00\u00a0\n 0.1024\u00a0\nCount of packages installed by User that Package depends on\u00a0\n\n\nSuggestsOnCnt       \u00a0\nN\u00a0\n1.00\u00a0\n 0.2266\u00a0\nCount of packages installed by User that Package suggests\u00a0\n\n\nPubYear             \u00a0\nN\u00a0\n1.00\u00a0\n 0.0612\u00a0\nPublished year, including fraction, derived from \"Published:\" field in package index page\u00a0\n\n\nComMaintCnt         \u00a0\nN\u00a0\n1.00\u00a0\n 0.3406\u00a0\nCount of packages installed by User that have same MaintainerEmail address as Package\u00a0\n\n\nPkgIdxTxtLen        \u00a0\nN\u00a0\n0.60\u00a0\n 0.2338\u00a0\nLength in chars of package index page text\u00a0\n\n\nPkgIdxTxtWordCnt    \u00a0\nN\u00a0\n0.40\u00a0\n 0.2506\u00a0\nCount of distinct \"words\" of length >= 6 chars in Package index page text\u00a0\n\n\nComPkgTxtWordCnt    \u00a0\nN\u00a0\n0.40\u00a0\n 0.5188\u00a0\nSum, over all distinct \"words\" of length >= 6 chars in index page text of Package, the count of packages installed by User whose index page texts also contain this \"word\"\u00a0\n\n\nComPkgTxtWordCntRat \u00a0\nN\u00a0\n0.40\u00a0\n 0.0832\u00a0\nRelated to ComPkgTxtWordCnt, but takes the ratio of sum per installed packages to sum per not installed\u00a0\n\n\nComPkgTxtWordCntFrac\u00a0\nN\u00a0\n0.40\u00a0\n 0.4597\u00a0\nRatio of ComPkgTxtWordCnt to PkgIdxTxtWordCnt\u00a0\n\n\nPkgIdxTxtNameCnt    \u00a0\nN\u00a0\n0.40\u00a0\n 0.3219\u00a0\nCount of distinct \"names\" of length >= 5 chars in Package index page text\u00a0\n\n\nComPkgTxtNameCnt    \u00a0\nN\u00a0\n0.40\u00a0\n 0.5147\u00a0\nSum, over all distinct \"names\" of length >= 5 chars in index page text of Package, the count of packages installed by User whose index page texts also contain this \"name\"\u00a0\n\n\nComPkgTxtNameCntRat \u00a0\nN\u00a0\n0.40\u00a0\n 0.0691\u00a0\nRelated to ComPkgTxtNameCnt, but takes the ratio of sum per installed packages to sum per not installed\u00a0\n\n\nComPkgTxtNameCntFrac\u00a0\nN\u00a0\n0.40\u00a0\n 0.4571\u00a0\nRatio of ComPkgTxtNameCnt to PkgIdxTxtNameCnt\u00a0\n\n\nDependsOnRecMis     \u00a0\nN\u00a0\n1.00\u00a0\n-0.1335\u00a0\nCount of packages not installed by User that Package depends on\u00a0\n\n\nDependsByRecMis     \u00a0\nN\u00a0\n1.00\u00a0\n 0.1743\u00a0\nCount of packages installed by User that depend on Package\u00a0\n\n\nMaintainerNameOrEmail\u00a0\nC\u00a0\n0.25\u00a0\n\u00a0\nMaintainerName unless missing, in which case MaintainerEmail\u00a0\n\n\nVarious other feature variables were tried, but for whatever reasons did not make the final cut.\nMy computing platform consisted of two workstations powered by multi-core Intel Xeon processors and running the Linux OS.  The core forecasting engine was written in C, but was controlled by a front-end program written in the scripting language Lua using LuaJIT (just-in- time Lua compiler version 2 Beta 5) for efficiency.\n"}, {"url": "http://blog.kaggle.com/2011/02/11/how-i-did-it-ming-hen-tsai-on-finishing-third-in-the-r-competition/", "link_text": "How I did it: Ming-Hen Tsai on finishing third in the R competition", "id": 83, "title": "Permalink to How I did it: Ming-Hen Tsai on finishing third in the R competition", "date": "2011-02-11", "text": "\nBackground\nI recently got my Bachelor degree from National Taiwan University (NTU). In NTU, I worked with Prof. Chih-Jen Lin's on large-scale optimization and meta-learning algorithms. Due to my background, I believe that good optimization techniques to solve convex model fast is an important key to achieve high accuracy in many application because we can don't have to worry too much about the models' performance and focusing on data itself.\n\nNoticing that the machine learning society lacks software on various kinds of algorithms that may be beneficial to our daily life, I am now developing pieces of tools that compile various state-of-the-art algorithms performing well in many data or contests. One of these pieces is lib-GUNDAM.\nMethods I tried \nI entered the competition on Jan 30, 2011. That time, I had a submission just trying to make sure whether my lib-GUNDAM performs well on data sets other than my experimental sets. It ended up that I got 10th in my first submission. I only expanded categorical features,\ntrained it with Linear-SVM and submit its decision values on the data set.\nThen, I used lib-GUNDAM to do data scaling and perform parameter search on linear-SVM using the training data set. Shortly, I rose to about 7th place.\nI somehow got stuck here and I decided to investigate other classifiers. I tried the tree bagger in Matlab, but I found it ran too low on the data. I thought I did not have time to wait for it. Somehow, I noticed that L2 logistic regression(LR) performs better than L1-SVM in most case on the data set. Thus, I switched to using it afterwards. Fortunately, my friend Hsiang-Fu Yu made a study months ago about training L2 LR in short time and incorporated it into LIBLINEAR. By using the software, I conducted parameter search very fast. (Search for positive instance penalty weight and negative instance penalty weight.)\nAlso I found that explicit 2-degree polynomial expansion (bi-gram expansion), worked well on the data. I began to used it before training on all data after wards,\nI rose to 4th place by using LR on best cross-validation parameters and using 2-degree polynomial expansion.\nThen, I had about two days left. I thought I did not have time to train other classifiers. Two ideas came to my mind:\n1. to boost the L2 LR or\n2. try to add more features.\nThus, I began to try random subspace methods and adaboost to boost the performance of L2 LR. Random subspace methods usually needs diverse learners, but I did not have fast learners, so used only liblinear, which might not be diverse enough. In my expectation, it did not work.\nAdaboost boosted my learners very trivially.\nIn time, I began to visit the forums to check whatever had been discussed. I noticed two things :\n1. training and testing data have overlap instances and\n2. the contest holder seemed to suggest contestants to investigate user-oriented features.\nTherefore, I hard-coded labels to those testing instances which occurred in the training data. Also, I decided to add features to users. Since we have five features\n* DependencyCount\n* SuggestionCount\n* ImportCount\n* ViewsIncluding\n* PackagesMaintaining for packages\nI added for each user five feature by averaging the individual features his packages own. For example, for DependencyCount, for user i, we add a feature userDependencyCount to the user by \\sum_{[i installed j]==1} DependencyCount_j / \\sum_{[i installed j]==1}. This yielded very good performance and rose me to the third place.\n8 hours before the contest ended I came up with the idea that recommendation system problems could be viewed as link prediction problems. Thus, I tried to add some link prediction features. Due to time constraint, I added only preferential attachment. Although it worked better on my cross validation, it did not work better than the previous model. I thought, I might be utilizing too much graph information. (I want to prediction if there is a link between node a and b, but I used the information regrading links between a and b. I think is a kind of\noverfitting.)\nThen, my submission quota was full, I did not do further experiments although I was to boost L2 LR again. I ended up in the 3rd place.\nTools I used\n1. lib-GUNDAM (http://www.csie.ntu.edu.tw/~b95028/software/lib-gundam/index.php) for feature preprocessing\n2. LIBLINEAR (www.csie.ntu.edu.tw/~cjlin/liblinear/) for training fast L2 LR\n3. LIBLINEAR with instance weight (http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances)\n4. for training adaboost because adaboost needs base learners able to take instance weight into account when learning\n5. hi-ensemble (http://www.csie.ntu.edu.tw/~b95028/software/hi-ensemble/index.php) for random subspace method\n6. adaboost\nFor algorithm details and experimental statistics, please refer to\nhttp://www.csie.ntu.edu.tw/~b95028/papers/2011kaggleR.pdf \n"}, {"url": "http://blog.kaggle.com/2011/02/08/how-i-did-it-yannis-sismanis-on-winning-the-elo-chess-ratings-competition/", "link_text": "How I did it: Yannis Sismanis on Winning the first Elo Chess Ratings Competition", "id": 84, "title": "Permalink to How I did it: Yannis Sismanis on Winning the first Elo Chess Ratings Competition", "date": "2011-02-08", "text": "\nThe attached article discusses in detail the rating system that won the Kaggle competition \u201cChess Ratings: Elo\u00a0vs the rest of the world\u201d. The competition provided a historical dataset of outcomes for chess games, and\u00a0aimed to discover whether novel approaches can predict the outcomes of future games, more accurately\u00a0than the well-known Elo rating system.\u00a0The major\u00a0component of the winning system is a regularization technique that avoids overfitting.\nkaggle_win.pdf\n"}, {"url": "http://blog.kaggle.com/2011/01/19/how-i-did-it-finishing-second-from-bo-yangs-perspective/", "link_text": "How I did it: finishing second from Bo Yang's perspective", "id": 85, "title": "Permalink to How I did it: finishing second from Bo Yang's perspective", "date": "2011-01-19", "text": "\nI first saw kaggle.com in Nov 2010. I looked at the ongoing contests and found the IJCNN Social Network Challenge most interesting and decided to join, mostly because of its possible real-world application due to popularity of online social networks.\n\nI cut my teeth on collaborative filtering, prediction engine, etc on the Netflix Prize. At first I was motivated by the one million dollar prize (yay, lottery for geeks !), but found the whole technical aspect very interesting. In addition to the prize, the fact that it was a competition also served as a \"motivation engine\", of course this applies to kaggle.com too.\nAnyway, I tried some quick-and-dirty methods with various levels of success, and slowly got my score up to .830 or so. At this point I decided to create my own validation data sets. At first I just picked nodes at random and I got a validation set that was very easy to predict, with my validation scores having no bearing on my test scores.\nPreviously I have asked a question on the discussion forum about the data sets, and in the subsequent discussion, the contest organizer gave some info on the creation of the test data set. Based on that info, I wrote a second validation data set builder, and was able to build validation data sets whose scores were reasonably close to the test scores. So my question has rewarded me greatly, in a totally unexpected way, and I also learned everything I could about the test data set, or so I thought (more on this later).\nNext I tried some algorithms in earnest, adapting some of my Netflix prize code to this task. Due to limited amount of time available, I read a grand total of 3 research papers, and missed a number of well known algorithms in related fields, but as it turned out, it didn't matter.\nBy late 2010/early 2011, I got my score to about .930, and making progress was becoming difficult. So I contacted William Cukierski about collaboration and eventual teaming up. William agreed and once we started talking, I was astounded by the breadth of his arsenal. It looked like he had implemented almost all implementable algorithms, given his hardware constraints. In fact, the only thing holding him back was that he had inadvertently created a bad validation data set. Once that was fixed his score shot up to .950 and kept improving.\nWilliam broke the node similarity problem down into 3 sub-problems:\n(1) How similar is node A to B?\n(2) How similar are the outbound neighbors of A to B?\n(3) How similar are the inbound neighbors of B to A?\nI improved by score to about .935 based on his input. William also had a random forest overlord that was vastly superior to my own blender.\nIn the last week of the contest, we contacted Benjamin Hamner about teaming up and Ben agreed. Not surprisingly, Ben had a number of strong predictors, and he shared with us a startling insight about the test data set. It alone boosted my score from .935 to .952 ! But in testimony to the strength of William's random forest, in hardly made any difference there. In the end we fed almost everything into the forest and the final score was about .970.\nTechnical Details of My Methods\nUnsurprisingly, some of the methods I tried didn't work, they include a neural network, a SVD/clustering-inspired method, and some node distance-based methods. Their scores were as high as .860, and by \"didn't work\" I mean they didn't contribute meaningfully to my blends. And fortunately, most of the methods that didn't work run much slower than the ones that did.\nIn the end, I had 3 major parts in my \"solution\":\n1. A genetic algorithm blender. At first I used my linear regression blender from the Netflix prize. It was designed for RMSE scoring so of course it didn't work well for AUC scoring. I could easily out-blend it by picking the weights manually. Then I decided to try my Netflix GA blender, which was useless for that task, but it worked well enough here. I was still going to do a logistic blender, until I ran into William's random forest, and decided there was no point.\n2. KNN. There're two keys to this:\n2A. The graph is very sparse, so to predict link between nodes A and B, you should go beyond 1st-level neighbors and use 2nd-level neighbors. I tried using 3rd-level neighbors too, but at that distance the neighborhood was getting too big.\n2B. Links should weight less as the link count goes up. Intuitively, imagine one million people following a celebrity, chances are most of them never met each other or the celebrity; on the other hand, if you have 30 contacts in your social network, chances are they're your family and friends and many of them know each other.\nI tried a number of variations for KNN, for example following only inbound link and different link weighting functions. The best one scored 0.915 by itself, and was created by going to 2nd-level neighbors, but applying link weighting (1/sqrt(LinkCount+1)) only to 1st-level neighbors, and checking how outbound neighbors of A are similar to B (William's 2nd point above).\n3. Global Link Distance (my own term here). To predict link from A to B, calculate the link distances from A to all other nodes, and the link distances from B to all other nodes. And the prediction is simply:\n1/SumOverAllNodesN( 2^( LinkDistance(A,N)+LinkDistance(B,N) ) )\nBy itself, GLD's score was only about .850, but it blended well with KNN. I suspect it's because they complement each other: KNN works on the local neighborhood while GLD works on the entire network.\nFor my best submission, I blended 4 variations of KNN and 2 variations of GLD. These methods are relatively simple, fast, and effective. They all took about 1 minute to run (2.66GHz CPU, single-threaded), and the blender usually came up with a good set of weights in 15 seconds. All together they could generate a .935 test submission in about 7 minutes.\nWhich brings me to Ben's insight that boosted by score to .952: as an artifact of the test data set creation process, the link from A to B is almost guaranteed to be true if A has only 1 bound neighbor, or B has 0 or 1 inbound neighbor. Apparently a few other contestants on the leaderboard discovered it too. It's extremely simple, fast, and effective.\n"}, {"url": "http://blog.kaggle.com/2011/01/18/how-i-did-it-benjamin-hamners-take-on-finishing-second/", "link_text": "How I did it: Benjamin Hamner's take on finishing second", "id": 86, "title": "Permalink to How I did it: Benjamin Hamner's take on finishing second", "date": "2011-01-18", "text": "\nI chose to participate in this contest to learn something about graph theory, a field with a huge variety of high-impact applications that I'd not had the opportunity to work with before. \u00a0However, I was a late-comer to the competition, downloading the data and submitting my first result right before New Years. \u00a0From other's posts on this contest, it also seems like I'm one of the few who didn't read Kleinberg's link prediction paper during it.\n\nThere were a few reasons I did not do a literature review while participating in the contest. \u00a0One was time - I had a late start, and I knew I had little time to devote to the contest for its remainder. \u00a0Short on time, I preferred to dive right into the data and see what I could make of it, as opposed to doing an extensive review of the prior literature. \u00a0I did this at the risk of missing something obvious that would help the models or \"reinventing the wheel,\" but with the benefit of having a fresh start.\nTo begin, I wrote a quick script to generate my own validation sets from the data and a framework to extract features from the data. \u00a0Features I extracted fell into three categories: those pertaining only to the \"A\" node (the source of the potential edge), those pertaining only to the \"B\" node (the destination of the potential edge), and those pertaining to the relation between A and B.\nNext, I looked at the nodes known to follow B. \u00a0Let \u00a0 denote the set of nodes N follows, and  denote the set of nodes that follow N. \u00a0For each pair A and\u00a0B, I took the following sum as a feature:\n\nThis feature by itself worked decently well, and almost got me in the top 10. \u00a0I continued to add features that I thought would be relevant to the problem. \u00a0These included the following:\n- Mean of values in the above summand- Max of values in the above summand- In-degree of A- Out-degree of A- In-degree of B- Out-degree of B- SVD- BFS-based features on the subgraph limited to the 37,689 nodes with outward links.\nA Random Forest trained on this combination succeeded in getting a leaderboard AUC above 0.91.\nNext, I thought about how some edges in the graph may be more important than others. \u00a0If a node follows only a few other nodes, then it is more likely to pay attention to or follow the nodes that those nodes follow. \u00a0I constructed a feature to value each potential edge based on random walks on the graph, starting at the A node. \u00a0Allowing the random walks to traverse the edges in either direction substantially improved this feature, giving it an AUC above 0.90 alone.\nHowever, the random walk feature was computationally expensive to evaluate and optimize. \u00a0As a result, I used linear algebra to iteratively approximate the solution, and termed it edgeRank after I realized this was similar to Google's PageRank. \u00a0I optimized this feature independently over two parameters: the probability of restarting at the A node during each step in the random walk, and the weights outbound edges were assigned relative to inbound edges. \u00a0By itself, this feature achieved around 0.93 on the leaderboard. \u00a0(After reading Kleinberg's paper, a very similar feature is in there, termed a rooted PageRank).\nOne issue I ran into was that my predicted AUC scores from my private validation sets were much higher than my leaderboard validation scores. \u00a0As a result, I analyzed any differences between my validation sets and Dirk's test set. \u00a0The primary difference was as follows:\n#A nodes, fan-out=1, test: 96 \u00a0, valid: ~374#B nodes, \u00a0 fan-in=1, test: 399, valid: ~2569\nIt ends up Dirk was not considering A nodes with a fan-out of 1 and B nodes with a fan-in of 1 for false edges, substantially altering the test distribution. \u00a0When I corrected for this, the test dataset distribution fell well within the conference intervals predicted by my private validation datasets on all the features I looked at. \u00a0Also, this likely meant that all 493 unique edges in the test set fulfilling this criteria (fan-out A == 1 or fan-in B <= 1) were true edges. \u00a0This information plus the edgeRank feature alone was enough to put my leaderboard AUC above 0.95, and a Random Forest trained on all the features put me in 2nd at the time, behind IND CCA. \u00a0(When the answers were released, I saw that 488 out of 493 of those edges were true edges, so my guess was only 99% right).\nA couple days before the end of the contest, I received a message from Will (who had just passed me on the leaderboard), seeing if I wanted to join him and Bo on a team to try beating IND CCA. \u00a0Given the magnitude of IND CCA's lead, I decided to. \u00a0As Will has described, we pooled our features, bounced ideas off one another, shared validation sets, and then trained a number of Random Forests for our final submission.\nI'd also like to comment on the applicability of this contest to real-world problems. \u00a0In one sense, the manner the problem was presented made link-prediction almost trivially easy: it was a binary classification problem with balanced classes. \u00a0The true edges almost always belonged to nodes close together on the graph, while the false edges almost always belonged to nodes very far apart on the graph. \u00a0Thus, the AUC scores for this competition were very high. \u00a0In reality of course, there is a huge imbalance between the two classes.\nA different evaluation procedure could have been providing participants with a list of originating nodes from which one or more edges may have been removed, and then having participants submit a short, ordered list of potential destination nodes for each originating node. \u00a0Participants could be evaluated on the Mean Average Precision of their submitted lists, or a similar metric. This is more applicable to social networks recommending new friends, and it encourages methodologies that could analyze and rank large numbers of potential edges for each originating node in a computationally efficient manner (such as edgeRank).\nIn another sense, this link prediction problem was far more difficult than it would be from the standpoint of a social network. \u00a0Networks like Facebook have vast quantities of additional information on the types and timing of interactions between nodes (each of which forms an edge), in addition to the friendship graph itself. \u00a0It probably is easier to predict new edges based on features like \"just appeared in a picture together\" in combination with the feature types used in this contest.\nCongratulations to IND CCA, who won the contest with a very clever de-anonymization scheme coupled with a more standard machine-learning approach. \u00a0I considered trying to do something similar, but didn't have enough time left in the contest to dive into the Flickr API. \u00a0Additionally, I thought that de-anonymizing the data would be, in many ways, a more difficult challenge (as they said, their de-anonymization procedure was not sufficient to win).\nIn one regard, it's a relief to have been beaten by a team in this manner: they won by tackling a different problem, and my team still had the best machine learning methodology applied to the dataset in the competition (disregarding outside information).\nMany thanks to my teammates, Will and Bo - y'all were great to work with and we made a strong run at the end! \u00a0And many thanks to Anthony and Dirk for running a well-organized contest. \u00a0I know I learned a lot, and I believe many of my competitors did the same!\nImage credit: Dusk Eagle\n"}, {"url": "http://blog.kaggle.com/2011/01/15/how-we-did-it-the-winners-of-the-ijcnn-social-network-challenge/", "link_text": "How we did it: the winners of the IJCNN Social Network Challenge", "id": 87, "title": "Permalink to How we did it: the winners of the IJCNN Social Network Challenge", "date": "2011-01-15", "text": "\nFirst things first: in case anyone is wondering about our team name,  we are all computer scientists, and most of us work in cryptography or related  fields. IND CCA refers to a property of an encryption algorithm. Other than that, no  particular significance.\nI myself work in computer security and privacy, and my  specialty is de-anonymization. That explains why the other team members (Elaine Shi,  Ben Rubinstein, and Yong J Kil) invited me to join them with the goal of  de-anonymizing the contest graph and combining that with machine learning.\n\nTo clarify: our goal was to map the nodes in the training dataset to the real identities in the  social network that was used to create the data. That would allow us to simply look  up the pairs of nodes in the test set in the real graph to see whether or not the  edge exists. There would be a small error rate because some edges may have changed  after the Kaggle crawl was conducted, but we assumed this would be negligible.\nKnowing that the social network in question is Flickr, we crawled a few million users'  contacts from the site. The crawler was written in python, using the curl library,  and was run on a small cluster of 2-4 machines.\nWhile our crawl covered only a  fraction of Flickr users, it was biased towards high degree nodes (we explicitly  coded such a bias, but even a random walk is biased towards high degree nodes), so we were all set. By the time we crawled 1 million nodes we were hitting a 60-70%  coverage of the 38k nodes in the test set. But more on that later.\nOur basic approach  to deanonymization is described in my paper with Vitaly Shmatikov. Broadly, there are two steps: \u201cseed finding\u201d and \u201cpropagation.\u201d In the former step we somehow  deanonymize a small number of nodes; in the latter step we use these as \u201canchors\u201d to propagate the deanonymization to more and more nodes. In this step the algorithm  feeds on its own output.\nLet me first describe propagation because it is simpler. As  the algorithm progresses, it maintains a (partial) mapping between the nodes in the  true Flickr graph and the Kaggle graph. We iteratively try to extend the mapping as  follows: pick an arbitrary as-yet-unmapped node in the Kaggle graph, find the \u201cmost similar\u201d node in the Flickr graph, and if they are \u201csufficiently similar,\u201d they get mapped to each other.\nSimilarity between a Kaggle node and a Flickr node is  defined as cosine similarity between the already-mapped neighbors of the Kaggle node  and the already-mapped neighbors of the Flickr node (nodes mapped to each other are  treated as identical for the purpose of cosine comparison).\n\nIn the diagram, the blue  nodes have already been mapped. The similarity between A and B is 2 / (\u221a3\u00b7\u221a3) =  \u2154. Whether or not edges exist between A and A\u2019 or B and B\u2019 is irrelevant.\nThere \u00a0are many heuristics that go into the \u201csufficiently similar\u201d criterion, which will be described in our upcoming paper. There are two reasons why the similarity between \u00a0a node and its image may not be 100%: because the contest graph is slightly different from our newer crawled graph, and because the mapping itself might have \u00a0inaccuracies. The /latter is minimal, and in fact the algorithm occasionally revisits \u00a0already-mapped nodes to correct errors in the light of more data.\nI have elided over \u00a0many details \u2014 edge directionality makes the algorithm significantly more complex, \u00a0and there are some gotchas due to the fact that the Kaggle graph is only partially \u00a0available. Overall, however, this was a relatively straightforward adaptation of the \u00a0algorithm in the abovementioned paper with Shmatikov.\nFinding seeds was much harder. \u00a0Here the fact that the Kaggle graph is partial presented a serious roadblock, and \u00a0rules out the techniques we used in the paper. The idea of looking at the \u00a0highest-degree nodes was obvious enough, but the key observation was that if you look at the nodes by highest in-degree, then the top nodes in the two graphs roughly \u00a0correspond to each other (whereas ordered by out-degree, only about 1/10 of the \u00a0Flickr nodes are in the Kaggle dataset). This is because of the way the Kaggle graph \u00a0is constructed: all the contacts of each of the 38K nodes are reported, and so the \u00a0top in-degree nodes will show up in the dataset whether or not they\u2019re part of the \u00a038K.\nStill, it\u2019s far from a straightforward mapping if you look at the top 20 in the two graphs. During the contest I found the mapping by hand after staring at two \u00a0matrices of numbers for a couple of hours, but later I was able to automate it using \u00a0simulated annealing. We will describe this in detail in the paper. Once we get 10-20 \u00a0seeds, the propagation kicks off fairly easily.\nOn to the results. We were able to \u00a0deanonymize about 80% of the nodes, including the vast majority of the high-degree \u00a0nodes (both in- and out-degree.) We\u2019re not sure what the overall error rate is, but for the high-degree nodes it is essentially zero.\nUnfortunately this translated to \u00a0only about 60% of the edges. This is because the edges in the test set aren\u2019t \u00a0sampled uniformly from the training graph; it is biased towards low-degree nodes, and deanonymization succeeds less often on low-degree nodes.\nThus, deanonymization alone \u00a0would have been far from sufficient. Fortunately, Elaine, Ben and Yong did some \u00a0pretty cool machine learning, which, while it would not have won the contest on its \u00a0own, would have given the other machine-learning solutions a run for their money.\nElaine sends me the following description:\n\n \nOur ML algorithm is quite similar to \u00a0what vsh described earlier. However, we implemented fewer features, and spent less \u00a0time fine tuning parameters. That's why our ML performance is a bit lower, with an \u00a0*estimated* AUC of 93.5-94%.\n(Note that the AUC for ML is not corroborated with \u00a0Kaggle due to the submission quota, but rather, is computed over the ground truth \u00a0from the deanonymization. The estimate is biased, since the deanonymized subset is \u00a0not sampled randomly from the test set. [The 93.5-94% number is after applying \u00a0Elaine\u2019s debiasing heuristic. -- Arvind])\nWe also used Random Forest over features a bunch of features, including the following (with acknowledgements to the \"social \u00a0network link prediction\" paper):\n1) whether reverse edge is present\n2) Adamic\n3) \u00a0|intersection of neighbors| / |union of neighbors|\n4) Neighborhood clustering \u00a0coefficient\n5) Localized random walk 2 - 4 steps\n6) Degree of n1/n2.\n7) ... and a few \u00a0other features, most are 1-3 hop neighborhood characteristics.\nFor some of the \u00a0above-mentioned features, we computed values for the out-graph, in-graph, and the \u00a0bi-directional graph (i.e., union of in-graph and out-graph). We wanted to implement \u00a0more features, but did not due to lack of time\nThe best feature by itself is the \u00a0localized random walk 3-4 steps. This feature on its own has an *estimated* AUC of \u00a092%. The Random Forest implementation used the Python Milk library. \u00a0 http://pypi.python.org/pypi/milk/\nCombining DA and ML:\n1) Naive algorithm:\nFor \u00a0each test case:\u00a0output DA prediction if DA prediction exists, else ML \u00a0score\n2) Improved version:\nFor edge (n1, n2), if n1 and/or n2 has multiple DA \u00a0candidates, and all candidates unanimously vote yes or no, output the corresponding \u00a0prediction.\nFinally, like every one else, there were some trial-and-error tuning and \u00a0adjustments.\nOnce we hit No. 1, we emailed the organizers to ask if what we did \u00a0was OK. Fortunately, they said it was cool. Nevertheless, it is interesting to wonder how a future contest might prevent deanonymization-based solutions. There are two \u00a0basic approaches: construct the graph so that it is hard to deanonymize, or require \u00a0the winner to submit the code for human inspection. Neither is foolproof; I\u2019m going to do some thinking about this but I\u2019d like to hear other ideas.\nLast but not least, a big thanks to Kaggle and IJCNN!\n"}, {"url": "http://blog.kaggle.com/2011/01/14/how-i-did-it-will-cukierski-on-finishing-second-in-the-ijcnn-social-network-challenge/", "link_text": "How I did it: Will Cukierski on finishing second in the IJCNN Social Network Challenge", "id": 88, "title": "Permalink to How I did it: Will Cukierski on finishing second in the IJCNN Social Network Challenge", "date": "2011-01-14", "text": "\nGraph theory has always been an academic side interest of mine, so I was immediately interested when Kaggle posted the IJCNN social network challenge.\u00a0 Graph-theoretic problems are deceptively accessible and simple in presentation (what other dataset in a data-mining competition can be written as a two-column list?!), but often hide complex, latent relationships in the data.\n\nThere\u2019s an old adage among researchers, \u201ca week in the library saves a year in the lab.\u201d\u00a0 Hoping not to reinvent the wheel, I combed through about 20 papers on the link prediction problem, the most helpful & accessible of which was Kleinberg\u2019s \u201cThe link prediction problem for social networks.\u201d Unfortunately, at over 1 million x 1 million nodes, the graph size of the Flickr dataset precluded many of the published methods from running on a commodity desktop computer.\nI programmed about 20 features (Jaccard, Adar, Katz, various neighbor-related metrics, path lengths, unseen bigrams, etc.) along with a framework whereby I would extract local \u201cneighborhood graphs\u201d to make the O(N^2)\u2014or worse\u2014computations feasible.\u00a0 I had this running in parallel in Matlab on an 8-core machine with 10 gigs of RAM and an SSD drive, which really helps keep things moving when the RAM runs out and the pageouts start climbing.\u00a0 I used the Boost Graph Library to speed up graph calculations.\u00a0 \u00a0I spent a good deal of time experimenting with different ways to build the local graphs.\u00a0 Taking the first-level neighbors was possible, but adding the second-level neighbors was intractable.\u00a0 I also tried, among others, methods based on enumerating the simple paths between A and B, using the nodes these paths traverse most often.\u00a0 This proved to be too expensive, so I ended up using the first neighbors of A and B.\nI was the first to pass the 0.9 public AUC mark when I realized that all the standard similarity features in the literature could be applied to 3 distinct problems:\n(1) How similar is node A to B?\n(2) How similar are the outbound neighbors of A to B?\n(3) How similar are the inbound neighbors of B to A?\nThis was probably my most novel contribution to the contest and the idea that produced some of my best features.\u00a0 I also found a method from a querying algorithm called Bayesian Sets that worked very well. \u00a0\u00a0As far as I know, this was the first time it has been applied to link prediction.\nUsing the 60+ features I obtained from applying my features to problems (1), (2) and (3), I tested neural networks, random forests, boosting, and SVMs. I decided the forests were the way to go, since they yielded the best AUC and required no data preprocessing. It was important to do regression rather than strict classification.\u00a0\u00a0 ROC analysis cares only about the relative order of the points, so having \u201cdegeneracy\u201d in prediction values is detrimental.\u00a0 Neural networks were almost as good as forests, but their ROCs were just worse enough to make blending neural nets and forests worse than forests alone. With random forests, I was able to score 0.98 and above on my own validation sets, but my public score was paradoxically stuck at 0.92, no matter how many sophisticated features I added or how many training points I churned out.\u00a0 As the competition continued and other teams jumped ahead, I still couldn\u2019t pinpoint the disparity between my private and public scores.\nAbout the same time, Bo Yang (who was on the 2nd place team in the Netflix prize and had a similar score on the IJCNN leaderboard), approached me about teaming up.\u00a0 It was through my emails with Bo that I realized I had created my validation sets improperly. I was not limiting the outnodes to appear just once in the training data, which meant I was biasing the data towards outnodes with more edges.\u00a0 Once I corrected this, my private AUC dropped from 0.98 to around 0.95, while the public one increased to around the same. What a relief!\nThe last two weeks of the competition were a frenzy to calculate features, exchange ideas with Bo, and play catch up to IND CCA\u2019s lead.\u00a0 I had the computer running 100% (x 8), 24 hours a day at this point.\u00a0 When IND CCA kept improving, we decided to approach Benjamin Hamner (who had a score near 0.95 too) to make a last effort to merge and overtake.\u00a0\u00a0 Ben agreed to team up. We all exchanged a common set of validation files (which Ben created to precisely match the distribution of the real test set), extracted our own features, and then I combined them all by running several hundred random forests and blending the output.\u00a0 Running this many forests helped to order the points in \u201cno man\u2019s land\u201d around 0.5, which have the highest variance from trial to trial.\nBen had discovered a group of points he believed were true, based on a discrepancy in the way that Dirk sampled the graph (there should have been a larger number of false B edges with an in-degree of 1). \u00a0\u00a0I also found a set of suspected true edges based on a probabilistic argument.\u00a0\u00a0 If B->A exists and A is the type of user who forms reciprocal friendships, it is highly likely that A->B exists and was removed.\u00a0 Reciprocal edges were fairly rare in the data, and having a path length B->A of 1 was also rare.\u00a0 Thus, having both together in the test set had an extremely low probability of occurring at random.\nBen and Bo each contributed some very strong features to my own (I\u2019ll leave it to them to explain the details).\u00a0 Ben\u2019s Edgerank feature by itself would have finished in the top 5 of the competition!\u00a0 Bo developed and tuned some very accurate kNN models, several of which scored above 0.9 by themselves.\nOur best submission was the result of 94 features, some using the entire graph, some calculated on a graph consisting of local neighbors of each node. \u00a0\u00a0Below I attached a table of some of our features and the AUCs they give by themselves on our validation sets.\u00a0 We had 8 validation sets of 4480 points each, half fake and half real.\u00a0 Random forests combined our features to a private AUC of about 0.972, which resulted in a 0.969 public final score.\n\n\n\nAUC\nBrief Description\n\n\n0.936\nEdgerank\n\n\n0.931\nkNN3\n\n\n0.923\nkNN4\n\n\n0.907\nJaccard applied to (2)\n\n\n0.902\nkNN1\n\n\n0.893\nAdarB All\n\n\n0.892\nCommon Neighbors applied to (2)\n\n\n0.889\nBayesian sets\n\n\n0.889\nCosine applied to (2)\n\n\n0.888\nCosine applied to (1)\n\n\n0.88\nJaccard applied to (1)\n\n\n0.857\nAdar   applied to (2)\n\n\n0.854\nSimrank applied to (2)\n\n\n0.854\nPercent of non-existing edges < SVD(A,B)\n\n\n0.852\nCommute Time in the local graph\n\n\n0.852\nGlobal link distance 2\n\n\n0.846\nTime to 1st similar neighbor in   Bayesian sets\n\n\n0.843\nkNN2\n\n\n0.84\nSVD rank 80\n\n\n0.831\nCommon Neighbors applied to (1)\n\n\n0.831\nAdarA All\n\n\n0.826\nSimrank\n\n\n0.812\nDot product of columns in SVD rank 80 approx\n\n\n0.812\nSVD values of neighbors\n\n\n0.805\nSimrank applied to (1)\n\n\n0.794\nUnseen   bigrams on Simrank\n\n\n0.793\nKatz   similarity of A and B\n\n\n0.781\nKatz   applied to (1)\n\n\n0.776\nBounded   Walks in the local graph\n\n\n0.769\nMaximum   flow in the local graph\n\n\n0.764\nShortest   Paths Histogram applied to (1)\n\n\n0.739\nCommon   neighbors of in2\n\n\n0.73\nCommon   neighbors\n\n\n0.73\nCosine   similarity of A and B\n\n\n0.73\nJaccard   similarity of A and B\n\n\n0.73\nAdar   similarity of A and B\n\n\n0.727\nIn   degree of node B\n\n\n0.714\nBayesian   Sets applied to (1)\n\n\n0.706\nAdar   applied to (1)\n\n\n0.701\nPagerank   of node A\n\n\n0.689\nNumber   of paths between A and B of length 2\n\n\n0.686\nPower   law exponent of the local graph\n\n\n0.675\nClustering   Coeff. Node A\n\n\n0.663\nUnseen   bigrams on Katz\n\n\n0.661\nPreferential   Attachment\n\n\n0.657\nBetweenness   Centrality in the local graph\n\n\n0.654\nClustering   Coeff. Node B\n\n\n0.593\nCommon   neighbors of in1\n\n\n0.569\nCommon   neighbors of out1\n\n\n0.56\nPagerank   of node B\n\n\n0.541\nKatz   applied to (2)\n\n\n0.538\nB->A   exists and node A forms mutual edges?\n\n\n0.533\nCommon   neighbors of out2\n\n\n0.532\nOut   degree of node B\n\n\n0.521\nIn   degree of node A\n\n\n0.521\nOut   degree of node A\n\n\n\n \nSpecial thanks to Kaggle and the organizers for sponsoring this competition, as well as my teammates Bo Yang and Benjamin Hamner for their valuable contributions.\nWilliam Cukierski is a PhD candidate in the Biomedical Engineering Department at Rutgers University. He has a bachelor\u2019s degree in physics from Cornell University.\nhttp://pleiad.umdnj.edu/~will/\n"}, {"url": "http://blog.kaggle.com/2010/12/13/how-we-did-it-jie-and-neeral-on-winning-the-first-kaggle-in-class-competition-at-stanford/", "link_text": "How we did it: Jie and Neeral on winning the first Kaggle-in-Class competition at Stanford", "id": 89, "title": "Permalink to How we did it: Jie and Neeral on winning the first Kaggle-in-Class competition at Stanford", "date": "2010-12-13", "text": "\nNeeral (@beladia) and I (@jacksheep) are glad to have participated in the first Kaggle-in-Class competition for Stats-202 at Stanford and we have learnt a lot!\u00a0With one full month of hard work, excitement and learning coming to an end and coming out as the winning team, it certainly feels like icing on the cake. The fact that both of us were looking\u00a0for nothing else than winning the competition, contributed a lot to the motivation and zeal with which we kept going each and every day. Each of us may have spent about 100 hours on this competition, but it was totally worth it.\n\nEven though the professor and textbook already mentioned some points which ended up to be very useful, the experiences we gained from this practice were much more influential to us. We strongly recommend every data mining student to participate in such competitions, make your hands dirty, and eventually every minute you invest on it will reward back.\nANALYZE THE DATA FIRST \nThe most important lesson we learnt was: we should always analyze the data first.\nAs newbies in data mining, we had thought a cool model could give us the best result, so the main effort was to find such advanced models and keep tuning it. We used the features from the raw data, did some feature transformation to deal with missing values, and tried some supervised learning model, such as lm, randomForest and glmboost in R. It seemed working, because Random Forest could improve 18% comparing to linear regression.\nHowever, we found the top 3 teams were more than 30% better than our result. We felt our approach was not on the right track, because we didn\u2019t believe any model on the same features could achieve such big improvement. So we switched the focus to the data itself and began studying its characteristics. We hoped to find clues from the data: was there any trick we didn\u2019t figure out yet?\nSoon later, we found about 30% of test instances could exactly match instances in training set and got 0 error. Naturally, the next question was: for each of the rest 70% of the test instances, how to find the best training instance approximate the price for the test instance? Intuitively, a wine in the same location with most similar name and the closest year could be a good candidate. Following this idea, we thought k-NN may be a good model for this data.\nThe next things became relatively easy: how to define the distance between instances. We explored different distances and tried some optimizations, and finally combined the results from different k-NN modes to reduce the variance, which was inspired by the idea of ensemble methods.\nIn a word, we would not have thought of the k-NN model without carefully studying the data. If you only can remember one thing from this blog, we hope it is: analyze the data first.\nMODELING\nThe main model we used was k-NN (k-Nearest Neighbor, with k=1) using edit distance, tf-idf cosine similarity and lucene score on wine titles as the distance metrics. We found k-NN achieved much better performance (40+% improvement) than other supervised learning methods that we tried; and it works well even when there are many missing values. One challenge to use k-NN is that the computation cost is very high. We solved this problem by clustering the wines based on a fewer indexes, such as location (the combination of country, sub-region and appellation), cleaned name (after removing the descriptive information), short name (the first 2 words of length >=3 in cleaned name), brand name (the string in quotes); to ensure a cluster assignment for every test instance.\nWhile performing look-up of neighbors for each test instance, we only selected the training instances in the same cluster (with the same index). For brand name based k-NN, for each test instance, we find the corpus of wines from the training set with the exact brand name as the test instance and find the closest match in terms of clean name(edit distance), volume and price.\nFinally, we averaged all the predictions from the k-NN models. For test instances that had a missing prediction from a k-NN model, missing values were imputed as the average of non-missing predictions from other k-NN models for the test instance. We also used predictions from randomForest built on previous/elsewhere-year to replace the average of predictions from the k-NN models. If available, now price replaced the final prediction.\nWe found the prices of the same wines were proportional to its volume. Therefore, the price should be normalized by the volume. The prediction of price from k-NN for each test instance was then computed as the price of the closest training instance in the same cluster * (volume of test instance wine/volume of closest training instance wine).\nEach k-NN model (using different clustering or distance metrics) can generate a prediction. Finally we average all the predictions to make the final one. The missing values are imputed using the average prediction from other models with non-missing predictions for the test instance.\nSome improvements and tuning were made based on the ideas. For example, we applied query expansion method on the year and wine scores, we considered the difference of years and volumes when compute the distance so that the model will prefer the neighbor wines with closest year and volume, etc.\nFEATURES\nEven though about 80% of features in the data were continuous variables(score, alcohol content) and about 20% were nominal variables(location based, name), our final model, comprising of k-NN models utilized only the nominal variables as explanatory variables and a lot of credit goes to the fact that we were able to discover hidden useful information residing within the name/title of the wine. The following were the features derived from the name/title field:\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 volume\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 previous/elsewhere price\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 now price\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 pack (# of bottles)\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 brand name (string withing quotation marks)\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 clean name (name/title, after removing quoted text, year, punctuations, etc.)\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 short name (first 2 words (length>=3) of the clean name)\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 loc (contanenation of Country, SubRegion and Appellation delimited by \u2018.\u2019)\nA standard bottle of wine has a volume of 750mL and hence wines for which volume wasn\u2019t explicitly mentioned in the name/title field, volume was defaulted to 750mL. Also, for wines where #bottles or pack size was available, volume was adjusted as the volume specified(or default) times the pack size.\nHANDLING OUTLIERS\nAny previous price(prevprice) in training or test data or predicted price as mapped to minprice(minimum price from training data, if predicted/previous price < minprice) or maxprice(maximum price from training data, if predicted/previous price > maxprice)\nMODEL SELECTION\nWe selected final prediction based on the following order of model precedence\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 If the test instance has now price available from the title, use it as the final prediction (MAE was 0 in \u2153 cross-validation training data used as test set during learning phase)\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 If the test wine finds a neighbor in training set with edit distance <= 1, directly use the price of this neighbor (MAE < 0.1 in \u2153 CV training data)\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 If the test wine has previous or elsewhere price in the title, we used randomForest to predict the price using the previous/elsewhere price and year as features (MAE was about 9 for \u2153 CV training data).\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 For all the rest of the test instances, use k-NN models with k=1, then build a SVM model using the predictions from the 5 best k-NN models as the features, and finally use the average of the 5 k-NN predictions and SVM prediction.\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 Fix outliers, for e.g. if predicted price > 1000, set predicted price = 999.99, which is the maximum price in the training data(based on information provided that all wines with price > 1000 were removed from training and test data)\nR packages used\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 randomForest\u00a0 (for random forest)\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 e1071\u00a0 (for svm)\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 cba\u00a0 (for edit distance)\nJava package used\n\u25cf\u00a0\u00a0\u00a0\u00a0\u00a0 Apache Lucene\nFUTURE WORK\nIf time permitted, we would try our hands on sub-models based on locations, ratings, etc, and locally regression models based on the distance. We also thought about tuning this problem into a search problem, and using learning to rank models (using the price difference to rank items) to find the closest instance (answer) given a test instance (query).\nACKNOWLEDGEMENT\nWe thank Nelson Ray, the TA for Stats202(Data Mining and Analysis) class at Stanford University to organize the competition and provide great support throughout. We also thank Prof. Susan Holmes for imparting both theoretical and practical knowledge during the class that helped us exploit different methodologies and packages on the wine data.\nS202 Team Eureka\nJie Yang, Neeral Beladia\njieyang, beladia AT stanford * edu\n"}, {"url": "http://blog.kaggle.com/2010/11/30/how-i-did-it-diogo-ferreira-on-4th-place-in-elo-chess-ratings-competition/", "link_text": "How I did it: Diogo Ferreira on 4th place in Elo chess ratings competition", "id": 90, "title": "Permalink to How I did it: Diogo Ferreira on 4th place in Elo chess ratings competition", "date": "2010-11-30", "text": "\nMy first contact with the inner workings of the Elo rating system was in the mid-90s, when I came across an article in the Europe Echecs magazine. I remember thinking that the problem of ranking chess players was much different from chess itself, so I didn\u2019t pay much attention to it at the time.\n\nIn August 2010, when I was checking some news on slashdot.org, I was surprised to find a post on a chess ratings competition. I thought it would be interesting to give it a try, and perhaps I could spend two or three days of my holidays on it. I never thought that during the following 15 weeks(!) I would be so absorbed by the competition, with an average of one submission a day.\nThere were times when my laptop (a 2.4GHz dual core) would be running for days on end with both processors running two different parameter tunings. There was also a Saturday morning when I got up in the middle of the night to check how the tuning was going. At 5am I decided to do another submission; it was my 51st submission, and it was the one that made it to 4th place.\nThe thing that got me so enthusiastic about the competition was more than the challenge itself. It was the fact that dozens of other people were thinking about the same problem, and that every day there was some change in the leaderboard, either I was able to jump a few positions, or someone else just popped out with an amazing score! It was this continuous challenge that I found so thrilling.\nBack in August I started thinking about the problem with pencil and paper, while doing a few trial submissions. From the beginning I looked at ratings as an indication of the probability of one player beating another, and then devised a probabilistic approach where the rating of each player was something between 0 and 1. Later I found that this approach was very much related to the well-known Bradley-Terry model.\nThe details can be found in the link below, together with an implementation (source code) in Python 2.6:\nhttp://web.tagus.ist.utl.pt/~diogo.ferreira/chess/\nI\u2019m making this available in the hope that others will be able to improve it even further (and also share the results). One thing I learned from this experience is that the spirit of competition can push things forward, and push things with such impetus that I'm convinced all of us together did more progress in a few months than one alone could\u00a0have achieved\u00a0over\u00a0several\u00a0years.\nDiogo R. Ferreira is professor of information systems at the Technical University of Lisbon, where he works in the area of process mining.\n"}, {"url": "http://blog.kaggle.com/2010/11/27/philipp-weidmann-5th-in-the-elo-comp-on-chess-ratings-and-numerical-optimization/", "link_text": "Philipp Weidmann (5th in the Elo comp) on chess ratings and numerical optimization", "id": 91, "title": "Permalink to Philipp Weidmann (5th in the Elo comp) on chess ratings and numerical optimization", "date": "2010-11-27", "text": "\n\nHaving participated in the contest almost from the beginning and posting 162 submissions by the end, I have tried a large variety of different prediction approaches. The first of them were Elo-based, using ratings updated iteratively as the games were read in sequentially, later ones had Chessmetrics-style simultaneous ratings which eventually culminated in the non-rating, graph theory-based prediction system which held the top spot in the leaderboard for the past weeks yet ended up finishing somewhere in the vicinity of 15th place.\n\nIn between, though, when my progress stalled and I was slowly being pushed back through the ranks, I took a few days\u2019 time to rethink rating systems from the very core, with the idea that a purely mathematical approach might be the ingredient I needed in order to advance again. While my submissions using this approach were only moderately successful, I do think that this is the most interesting and intuitive idea that I tried in the contest, and therefore have decided to write this blog post about it.\nPart I.\u2003Ideal Ratings\nThe basic mathematical idea behind every rating system is the following:\nLet {pk} be the set of players. Consider the set of games {gk}; we wish to predict their individual outcomes (i.e. the white scores) {sk} by using only two numbers for each game gk: The rating rw =\u2005r(w(gk)) of the white player w(gk), and the rating rb =\u2005r(b(gk)) of the white player b(gk). We are therefore looking for a set of ratings {rk} (one rating assigned to each player) with the following property:\n\n(1) sk =\u2005f(rw,\u2005rb)\nfor some function f, the prediction function.\nOf course, this condition is impossible to fulfill in most practical settings. When at least two of the games feature the same two players playing white and black and their outcomes are different, (1\u2191) cannot hold for both of these games simultaneously.\nPart II.\u2003\u201cOptimal Ratings\u201d\nFor simplicity\u2019s sake, let the prediction function f in (1\u2191) be defined as\n\n(2) f(rw,\u2005rb)\u2005=\u2005rw \u2212\u2005rb +\u2005s\nwith s denoting the average outcome, or average white score (in the competition\u2019s training dataset, s =\u20050.5456\u2026). Thus between two players of equal rating, the predicted score will precisely match the average score s.\nThis linear formulation is a very good approximation to the \u201cideal\u201d prediction function, as shown by Jeff Sonas with his Chessmetrics system. It also has the advantage that it can easily be \u201cinverted\u201d, meaning that it is easy to find a function h which, when supplied a game score between two players, gives the expected rating difference that is a maximum likelihood estimator for the players\u2019 actual rating difference. Namely,\n\n(3) h(sk)\u2005=\u2005sk \u2212\u2005s\nThis function allows us to recast the condition (1\u2191) as follows:\n\n(4) sk \u2212\u2005s =\u2005rw \u2212\u2005rb\nthat is, the expected rating difference should match the actual one.\nWhile we have only been considering the individual games gk and their outcomes sk so far, it is easy to formulate (weaker) conditions which look at the average score between two players, and thus more closely approximate their \u201ctrue\u201d relative strengths:\n\n(5) sp1,\u2005p2 \u2212\u2005s =\u2005rp1 \u2212\u2005rp2\nwhere sp1,\u2005p2 denotes the average score p1 has achieved against p2.\nRecognizing that many conditions of this type interlinking the players may be difficult to fulfill, we arrive at the important idea of restating (5\u2191) as a minimization problem:\n(6) minrp1,\u2005rp2\u2225rp1 \u2212\u2005rp2 \u2212\u2005sp1,\u2005p2 +\u2005s\u2225\nExpanding the scope to look at all player connections simultaneously, this becomes\n(7) min{rk}\u2211|{pk}|i,\u2005j =\u20051n(pi,\u2005pj)\u2225rpi \u2212\u2005rpj \u2212\u2005spi,\u2005pj +\u2005s\u2225\nwith the additional idea that the higher the number of games n(pi,\u2005pj) between two players pi and pj becomes, the more weight their individual condition (6\u2191) carries, which makes sense because their score average spi,\u2005pj carries more information. In a mathematical sense, ratings generated using (7\u2191) can justifiably be called \u201coptimal\u201d. Note that for players that didn\u2019t face each other at all (i.e. the vast majority of pairings), n(pi,\u2005pj) will be zero, and thus the evaluation of the term inside the norm does not matter for such pairings.\nPart III.\u2003Implementing \u201cOptimal Ratings\u201d\nFrom a numerical standpoint, (7\u2191) is an unconstrained, quadratic (if \u2225\u22c5\u2225 is taken to be the Euclidean norm) optimization problem - with, in case of the competition, 8631 variables. This is large, especially if you don\u2019t have a supercomputer available, but not intractible. Ordinary computer algebra systems, however, will likely not be up to the task, because the choice of optimization method is paramount for a sensible runtime.\nI used a truncated Newton method with line search and analytically computed first derivatives, implemented in Fortran 90 (Yes!) by Stephen Nash (http://jblevins.org/mirror/amiller/tn.zip), which after some modifications gave convergence times of less than 3 minutes on my notebook. The ratings were then used to generate predictions via (1\u2191). I still remember the excitement I felt when I uploaded my first optimization-based prediction to Kaggle; naturally, I expected it to do really well, given the hyper-advanced number crunching algorithms used.\nThat is, until I saw the public score. It was by far my worst one at that time.\nAt first I couldn\u2019t believe it. I uploaded the same prediction again, with an identical result. Then I started looking for a bug in my code, but couldn\u2019t find one. What was going on?\nPart IV.\u2003Revisiting the Optimization Problem\nWhere there is no software bug, there is likely a flaw within the original line of reasoning. It took a while to find it, but in the end I figured it out.\nConsider the following crosstable of games played among three players, with the average score given in brackets:\n\n\n\nPlayer\n1\n2\n3\n\n\n1\n-\n3 (1.0)\n0\n\n\n2\n3 (0.0)\n-\n2 (1.0)\n\n\n3\n0\n2 (0.0)\n-\n\n\n\nWe see that player 1 has been doing really well against player 2; his score is 100%. Similarly, player 2 has a 100% score against player 3, while player 1 and player 3 did not play against each other.\nA system based on (7\u2191) would make the rating difference between player 1 and player 3 unreasonably large, because there is no connection between those players at all. The optimization engine is thus free to vary the rating distance of those players, even to the point where it generates ratings that would fit those of Rybka and a monkey, respectively.\nThe solution is to tie the ratings of all unconnected players together, with the sensible assumption that players in the same pool are (very roughly) of the same class, and thus of similar strength. This gives rise to a new, slightly more complex optimization problem:\n(8) min{rk}\u2211|{pk}|i,\u2005j =\u20051c(pi,\u2005pj)\nwhere\n(9) c(pi,\u2005pj)\u2005=\n\n\n\nn(pi,\u2005pj)\u2225rpi \u2212\u2005rpj \u2212\u2005spi,\u2005pj +\u2005s\u2225\nn(pi,\u2005pj)\u2005>\u20050\n\n\n\u03b1\u2225rpi \u2212\u2005rpj\u2225\nn(pi,\u2005pj)\u2005=\u20050\n\n\n\nThe scaling factor \u03b1 is necessary because without it, the ties \u2225rpi \u2212\u2005rpj\u2225 bind the ratings much too tightly together: Of 86312 \u2248\u200574.5 million parts in the sum, only about 57,000 (or less than 0.1%) are connections between players (made by games played), while the overwhelming rest are conditions basically stating that all ratings should be equal, which is what they will be if \u03b1 =\u20051. However, I have found a scaling factor of \u03b1 =\u20050.0001 to work quite well in practice, and indeed, minimizing (8\u2191) with this factor gives results similar in quality to that of Chessmetrics, provided that some additional tricks are used (such as a precisely controlled rating pool). This is no surprise, since Chessmetrics is essentially an algorithm designed to accomplish this very optimization (with some slight differences). However, the advantage of this direct numerical approach is that deep insights into the mechanics of the rating system are easier to gain and implement, and doing so might give birth to some completely new ideas.\nIn the contest I soon found a different approach which didn\u2019t require optimization and gave better results, so I abandoned this technique in favor of it. Nevertheless, optimization might be worth investigation further as a means for both prediction and rating generation, in chess and in other sports.\n"}, {"url": "http://blog.kaggle.com/2010/11/24/how-we-did-it-jeremy-howard-on-winning-the-tourism-forecasting-competitoin/", "link_text": "How we did it: Jeremy Howard on winning the tourism forecasting competition", "id": 92, "title": "Permalink to How we did it: Jeremy Howard on winning the tourism forecasting competition", "date": "2010-11-24", "text": "\nThe following Q&A is with Jeremy Howard who together with\u00a0teammate Lee Baker won Kaggle's Tourism Forecasting Competition. (This was a two-part competition -\u00a0Lee previously described his work on Part\u00a0I of the competition in\u00a0a separate blog post).\n\u00a0\n- where you're from, what you have studied\nI live in Melbourne, Australia, recently voted the world's 3rd most livable city... Perhaps this is inspiring a bit of data mining success around here (it's also the home of the tourism2 winner, and the Kaggle CEO)! I studied philosophy at the University of Melbourne.\n- what you do\nAfter university I worked in management consulting (McKinsey & Co, AT Kearney), and then went on to found 2 businesses (FastMail.FM, an email provider, and Optimal Decisions Group, an insurance pricing optimisation specialist). Having sold both in the last couple of years, I now have the free time to follow my interests.\n- core technical approach\nMy goal was to try to stay in line with the approach taken in the paper being submitted by the contest organisers - I wanted to find a general, automated algorithm for forecasting, which I could apply to all time series without any parameter tuning or manual involvement. I had hoped therefore to only do a single submission to the leaderboard. However, an early data problem in the posted data (later rectified by the organisers) unfortunately meant this wasn't possible. After the fixed data was posted, I only did 3 further submissions.\nI realised that a fundamental issue was that the final results were calculated using a novel algorithm called \"MASE\", which is a ratio. The denominator of the ratio could in some cases be extremely small - this occured in series which had close to constant additive seasonality, no growth, and no noise. I found that the contribution of these series to the overall result was so high that in practice the algorithm should be tuned to favor these, even at the expense of other series (which had a relatively high denominator, and thus contributed much less to the overall result).\nTo do this, I only used linear growth (as opposed to exponential) and additive seasonality (as opposed to multiplicative) for all series, since any series which had exponential growth and/or multiplicative seasonality would have very small weights in the overall metric. Later, I experimented with allowing some series to use exponential growth and multiplicative seasonality, if the statistical evidence for those series was particularly strong, and confirmed that the impact was negative, as expected.\n- methodology that proved most effective\nI first created an algorithm to automatically remove outliers. Outliers can occur in, for example, a tourism time series if an area has a once-off event (positive outlier), or temporary closing of a major attraction (negative outlier), which will not impact future results. I used a customised local smoother, and used the residuals to determine seasonality. I ran this twice to create a double-smoothed time series, which I then compared to the original data, and removed data points outside 3 standard deviations of residuals.\nI then fitted a weighted regression (weighted the most recent observations the most heavily) combined with weighted additive seasonality (again weighting the most recent observations the most heavily) on all but the last 2 years of each series. A simple optimiser found the optimal weighting of each in order to predict the final 2 years. This weighted model was then applied to the full data set to create predictions. The intercept of the weighted regression was adjusted such that the residual on the final observation was always zero - this was important for ensuring that the series with a low denominator in the MASE metric were forecast as accurately as possible.\nI've since realised I had some bugs in my code (e.g. failing to truncate series to be positive, and some bugs with going from a validation set to the final predictions). It would be interesting to see how much better the predictions would be if these bugs were fixed.\nThe whole algorithm takes only 10 seconds to complete for the entire set of time series. Since the algorithm is fast, accurate, and automated, I think it is a good system for automated time series prediction. I plan to test it in the future on other data sets (e.g. the \"M3\" time series prediction competition data) to confirm that it can be effectively applied to other types of data.\n- what first attracted you to the competition? \nThe tourism forecasting competition was my first data mining contest - I entered it in order to try to update and strengthen my analysis skills, and to learn something new (having never done time series forecasting before).\n- did you do much background reading or research? \nYes, I read most of the recent papers and online tutorials by one of the conference organisers, Rob J Hyndman. I found that they were a great way for a time-series newbie like myself to get up to speed with the topic.\n- what tools and programming language did you use?\nI used C#, in Visual Studio 2010.\n- how much time did you spend on the competition?\nI spent longer than I expected because the initial data problems left me stumped and confused! Once they were fixed, I had submitted my result within a couple of hours. I estimate I spent a couple of weeks on the problem, including reading and research.\n"}, {"url": "http://blog.kaggle.com/2010/11/23/how-we-did-it-david-slate-and-peter-frey-on-9th-place-in-elo-comp/", "link_text": "How we did it: David Slate and Peter Frey on 9th place in Elo comp", "id": 93, "title": "Permalink to How we did it: David Slate and Peter Frey on 9th place in Elo comp", "date": "2010-11-23", "text": "\nOur team, \"Old Dogs With New Tricks\", consists of me and Peter Frey, a former university professor.  We have worked together for many years on a variety of machine learning and other computer-related projects. Now that we are retired from full-time employment, we have endeavored to keep our skills sharp by participating in machine learning and data mining contests, of which the chess ratings contest was our fourth.\n\nOur approach to this contest has been to treat it primarily as a forecasting problem, not as an exercise in developing a chess ratings system.  We built forecasting models from the training data using a home-grown variant of \"Ensemble Recursive Binary Partitioning\", a method we have previously employed for other contests and\napplications.  Like various other machine-learning/forecasting methods, this one trains a model on data consisting of records (e.g. cases, instances, objects, or in this case chess games) for which the values of a set of predictor variables and an outcome variable are known, and then applies the model to estimate (or forecast) the outcome values for a separate set of test or production data records for which the predictor values but not the outcomes are known.\nSince each game in the chess dataset was supplied with a minimal amount of information (month, player id's, and result), we synthesized a variety of predictor variables based on an analysis of the dataset. We attempted to optimize parameter settings and variable selections by observing model performance both on the leaderboard set and on various holdout sets created from the last 5, 7, or 10 months of the training data.  By the end of the competition we had done over 1100 runs building models on part of the training data and testing them on the remainder, and had created and tested various combinations of approximately 65 home-grown predictors.\nOf our 120 submissions, the 93rd on November 4 produced the best score on the official test data, 0.699472.  The model used for that submission employed 22 predictors:\n1, 2:\tWhite and Black player skill rankings calculated by an iterative process from the results of the training games.\n3, 4:\tCounts of \"quality\" games for White and Black players used in calculating vars 1 and 2.  \"quality\" games are those for which one's opponent has a skill level that is not too dissimilar from one's own.\n5, 6:\tAverage rankings of White's and Black's opponents as calculated for vars 1 and 2.\n7, 8:\tAverage number of games per month up to this game for White and Black.\n9, 10: White and Black ratings calculated from the training data according to an ELO-like algorithm that evolves ratings chronologically from a fixed starting value.\n11, 12: Maximum ratings (as calculated for vars 9 and 10) of opponents beaten by White and Black up to this game.\n13, 14: Minimum ratings (as calculated for vars 9 and 10) of opponents lost to by White and Black up to this game.\n15, 16: Mean ratings of White's and Black's previous opponents.\n17, 18: Mean scores of White and Black against their common opponents.\n19, 20: Rating difference between White's next and current opponents. Similarly for Black.\n21, 22: Rating difference between White's current and previous opponents. Similarly for Black.\nThe computing resources employed for the contest consisted of a few workstations running the Linux operating system.  Our core general purpose data analysis and forecasting engine is written in ANSI C, but was driven by code specific to this contest written in the scripting language Lua. \n"}, {"url": "http://blog.kaggle.com/2010/11/20/how-i-did-it-martin-reichert-on-3rd-place-in-elo-comp/", "link_text": "How I did it: Martin Reichert on 3rd place in Elo comp", "id": 94, "title": "Permalink to How I did it: Martin Reichert on 3rd place in Elo comp", "date": "2010-11-20", "text": "\nAbout\u00a0Martin:\nMartin is a retired Senior Project Manager and IT Consulting Manager at Siemens\u00a0AG with a university degree in physics.\u00a0 He\u00a0now likes to\u00a0develop and improve\u00a0rating methods, especially regarding professional boxing ratings - see\u00a0http://boxrec.com - maybe I will launch a competition regarding these ratings in the future ...\n\nMartin\u2019s Method:\n\u201cI first evaluated established ratings like Elo and Chessmetrics -\u00a0and found Chessmetrics a very promising approach, regarding the parameters:\u00a0performance, opposition quality,\u00a0activity, weighting and\u00a0self consistant rating\u00a0over a time period. By varying the parameters and algorithms and evaluating the predictions against the cross validation data, I step by step could improve my score. Finally my last submission #50 with\u00a0my best\u00a0public\u00a0score was not the submission with my best final score. My prize-winning submission turned-out to be\u00a0my submission #23 - so never give up.\u201d\n\n"}, {"url": "http://blog.kaggle.com/2010/11/19/how-i-did-it-jeremy-howard-on-finishing-second/", "link_text": "How I did it: Jeremy Howard on finishing second", "id": 95, "title": "Permalink to How I did it: Jeremy Howard on finishing second", "date": "2010-11-19", "text": "\nWow, this is a surprise! I looked at this competition for the first time 15 days ago, and set myself the target to break into the top 100. So coming 2nd is a much better result than I had hoped for!...  I'm slightly embarrassed too, because all I really did was to combine the clever techniques that others had already developed - I didn't really invent anything new, I'm afraid. Anyhoo, for those who are interested I'll describe here a bit about how I went about things. I suspect in many ways the process is more interesting than the result, since the lessons I learnt will perhaps be useful to others in future competitions.\n\nI realised that, by starting when there was only 2 weeks to go, I was already a long way behind.  So my best bet was to leverage existing work as much as possible - use stuff which has already been shown to work! Also, I would have to stick to stuff I'm already familiar with, as much as possible. Therefore, I decided initially to look at Microsoft's TrueSkill algorithm: there is already a C# implementation available (a language which I'm very familiar with), and it's been well tested (both in practice, on XBox live, and theoretically, in various papers).\nSo, step one: import the data. The excellent FileHelpers library meant that this was done in 5 minutes.\nStep two: try to understand the algorithm. Jeff Moser has a brilliant exposition about how TrueSkill works, along with full source code, which he most generously provides. I spent a few hours reading and re- reading this, and can't say I ever got to a point where I fully understood it, but at least I got enough of the gist to make a start. I also watched the very interesting Turing Lecture by Chris Bishop (who's book on pattern recognition was amongst the most influential books I've read over the years), which discusses the modern Bayesian Graphical Model approach more generally, and briefly touches on the TrueSkill application.\nStep three: make sure I have a way to track my progress, other than through leaderboard results (since we only get 2 submissions per day). Luckily, the competition provides a validation set, so I tried to use that where possible. I only ever did my modelling (other than final submissions) using the first 95 months of data - there's no point drawing conclusions based on months that overlap with the validation set!\nI also figured I should try to submit twice everyday, just to see how things looked on the leaderboard. My day one submission was just to throw the data at Moser's class using the default settings. I noticed that if I reran the algorithm a few times, feeding in the previous scores as the starting points, I got better results. So I ran it twice, and submitted that. Result: 0.696 (1st place was about 0.640 - a long way away!) (For the predictions based on the scores, assuming the scores for [white, black] are [s1, s2], I simply used (s1+100)/(s1+s2). The 100 on the top is give white a little advantage, and was selected to get the 54% score that white gets on average).\nFor the next few days, I went backwards. Rather than looking at graphs of score difference vs win%, I assumed that I should switch to a logistic function, which I did, and I optimised the parameters to the using a simple hill-climb algorithm. This sent my score back to 0.724. I also tried optimising the individual player scores directly. This sent my score back to 0.701. This wasted effort reminded me that I should look at pictures before I jump into algorithms. A graph of win% against white score (with separate lines for each quartile of black score), clearly showed the a logistic function was inappropriate, and also showed that there were interactions that I needed to think about.\nSo, after 5 days, I still hadn't made much improvement (minor tweaks to Trueskill params had got me to 0.691, barely any improvement from day 1). So I figured I needed a whole different approach. And now I only had 10 days to go...\nIt concerned me that Trueskill took each individual match and updated the scores after every one - it never fed the later results back to re-score the earlier matches. It turns out that (of course!) I wasn't the first person to think about this problem, and that it had been thoroughly tackled in the \" TrueSkill Through Time\" paper from MS Research's Applied Games Group. This uses Bayesian inference to calculate a theoretically-optimal set of scores (both mean and standard deviation, by player).\nUnfortunately the code was written for an old version of F#, so it no longer works with the current version. And it's been a while since I've used F# (actually, all I've done with it is some Project Euler problems, back when Don Syme was first developing it; I've never actually done any Real Work with it). It took a few hours of hacking to get it to compile. I also had to make some changes to make it more convenient to use it as a class from C# (since it was originally designed to be consumed from an F# console app). I also changed my formula for calculating predictions from scores, to use a cumulative gaussian - since that is what is suggested in the TrueSkill Through Time paper. My score now jumped to 0.669.\nThe paper used annual results, but it seemed to me that this was throwing away valuable information. I switched to monthly results, which meant I had to find a new set of parameters appropriate for this very different situation. Through simple trial and error I found which params were the most sensitive, and then used hill-climbing to find the optimum values. This took my score to 0.663.\nThen I added something suggested in the Chessmetrics writeup on the forum - I calculated the average score of the players that each person played against. I then calculated a weighted average of each player's actual score, and the average of their opponents. I used a hill-climb algorithm to find the weighting, and also weighted it by the standard deviation of their weighting (as output by Trueskill/Time). This got me to 0.660 - 20th position, although later someone else jumped above me to push me to 21st.\nThe next 5 days I went backwards again! I tried an ensemble approach (weighted average of TrueSkill, TrueSkill/Time, and ELO), which didn't help - I think because TrueSkill/Time was so much better, and also because the approaches aren't different enough (ensemble approaches are best when combining approaches which are very different). I tried optimising some parameters in both the rating algorithm, and in the gaussian which turns that into probabilities for each result. I also tried directly estimating and using draw probabilities separate from win probabilities.\nI realised that one problem was that my results on the validation set weren't necessarily showing me what would happen on the final leaderboard. I tried doing some resampling of the validation set, and realised that different samples gave very different results. So, the validation set did generally effectively show the impact when I made a change which was based on a solid theoretical basis, but it was also easy to get meaningless increases by thoughtless parameter optimisation.\nOn Nov 15 I finally made an improvement - previously in the gaussian predictor function I had made the standard deviation a linear function of the overall match level [i.e. (s1+s2)/2]. But I realised from looking at graphs that really it's that a stronger black player is better at forcing a draw - it's really driven by that, not by the combined skill. So I made the standard deviation a linear function of black's skill only. Result: 0.659.\nSo, it was now Nov 16 - two days to go, and not yet even in the top 20! I finally decided to actually carefully measure which things were most sensitive, so that I could carefully manage my last 4 submissions. If I had been this thorough a week ago, I wouldn't have wasted so much valuable time! So, I discovered that the following had the biggest impact on the validation set:\n* - Removing the first few months from the training data; removing the first 34 months was optimal for the validation set, so I figured removing the first 40 months would be best for the full set\n* - Adjusting the constant in the calculation of the gaussian's standard deviation - if too high, the predictions varied too much, if too little, the predictions were all too close to 0.5\n* - And a little trick: I don't know much (anything!) about chess, but I figured that there must be some knockout comps, so people who play more perhaps are doing so because they're not getting knocked out! So, I tried using the count of a player's matches in the test set as a predictor! It didn't make a huge difference to the results, but every little bit counts...\nBased on this, my next 3 submissions were:\n* - Remove first 40 months: 0.658\n* - Include count of matches as a prediction: 0.654\n* - Increase the constant in the stdev formula by 5%: 0.653\n(My final submission was a little worse - I tried removing players who hadn't played at least 2 matches, and I also increased the weight of the count of matches: back to 0.654).\nFor me, the main lesson from this process has been that I should more often step back and think about the fundamentals. It's easy to get lost in optimising the minor details, and focus on the solution you already have. But when I stepped away from the PC for a while, did some reading, and got back to basics with pen and paper, is when I had little breakthoughs.\nI also learnt a lot about how to use validation sets and the leaderboard. In particular, I realised that when you're missing a fundamental piece of the solution, then little parameter adjustments that you think are improvements, are actually only acting as factors that happen to correlate with some other more important predictor. So when I came across small improvements in the validation set, I actually didn't include them in my next submitted answer - I only included things that made a big difference. Later in the competition, when I had already included the most important things, I re-tested the little improvements I had earlier put aside.\nPlease let me know if you have any questions. I would say that, overall, TrueSkill would be a great way to handle Chess leaderboards in the future. Not because it did well in this competition (which is better at finding historical ratings), but because, as shown in Chris Bishop's talk, it is amazingly fast at rating people's \"true skill\". Just 3 matches or so is enough for it to give excellent results.\nHere's a little pic of my submission history\n\n\n\n\n\n\n\n\n\n\n\n\nThe points show the public leaderboard score, and the final score, for each submission. As you can see, they are very closely related (r^2 = 0.97). Kaggle has previously shown that a pool of other submissions from the leaderboard do not have such a close relationship - I think this is because I only had a short time, so my submissions really needed to be based on a solid theoretical basis, rather than parameter tuning. Although parameter tuning can easily overfit, you can see from the above picture that carefully targeted changes can allow you can get a good idea of how you're doing from the leaderboard. (I also found a similar level of relationship between my validation set results, and the leaderboard results).\nThe lines connecting the points show the progress of my scores over time. You can clearly see how after the first few days, going from standard TrueSkill over to TrueSkill Through Time, the results make a sudden jump. You can see how up until that time, I was mainly going backwards!\nYou can also see after I switched to TrueSkill Through Time, I had a few days of going nowhere - up until the last few days, when I forced myself to take a more thoughtful approach, due to the lack of time.\nOne takeaway from this chart, I think, is to note that a few days of failure is no reason to give up - improvements tend to be sudden, as new insights are made, rather than gradual. So, doing well in these competitions requires a certain amount of resilience - even when things are going badly on the leaderboard, you're skill learning, and still have the chance to make improvements as long as you're still trying.\nAnother takeaway is that it's better to try lots of completely different things, rather than trying to precisely tune whatever you have so far. At least in this competition, for my results, I saw big improvements from adding substantial pieces to the algorithm, and not much from fine-tuning the details.\n"}, {"url": "http://blog.kaggle.com/2010/10/11/how-i-did-it-the-top-three-from-the-2010-informs-data-mining-contest/", "link_text": "How I did it: The top three from the 2010 INFORMS Data Mining Contest", "id": 96, "title": "Permalink to How I did it: The top three from the 2010 INFORMS Data Mining Contest", "date": "2010-10-11", "text": "\nThe 2010 INFORMS Data Mining Contest has just finished. The competition attracted entries from 147 teams with participants from 27 countries. The winner was Cole Harris, followed by Christopher Hefele and Nan Zhou. Here is some background on the winners and the techniques they applied.\nCole Harris\nAbout Cole:\n\"Since 2002 I have been VP Discovery and cofounder of Exagen Diagnostics. We mine genomic/medical data to identify genetic features that are diagnostic of disease, predictive of drug response, etc. and then develop medical tests from the results. Prior to this (2000-2002), at Quasar/Magnaflux, I developed pattern recognition algorithms for identifying defective metal parts from acoustic spectral data.\u00a0 From 1990-1999 I worked for Veritas Geophysical, most of that time developing algorithms for imaging seismic data. Prior to this I was in grad school (physics): MA 1990 Johns Hopkins University.\"\n\nCole's Method:\n\"As far as techniques, my submissions were mostly based on:\n1. pre-processing - I did many things, but none seemed to make a large difference in my early results on test data, so in the end, other than exclude the non-price data, I didn't filter the data. I did append the data with data advanced 5 min, 60 min, and 65 min. So for each stock there were 16 features (open,hi,lo,last)X(0min,5min,60min,65min)\n2. feature selection - forward stepwise selection of stocks, reverse stepwise selection of particular features for a given stock, evaluated using logistic regression. This resulted in 5-6 features selected from 2 stocks.\nmodels: logistic regression and neural networks (not sure which won).\"\n\nChristopher Hefele\nAbout Christopher: \nChristopher is a Systems Engineer at AT&T. He was a member of The Ensemble, the team which finished second in the $1m Netflix Prize.\nChristopher's Method:\n\"I was using a simple logistic regression on Variable 74 for most of the  contest. During the last few days, when every last bit counted, I  switched to a SVM & added more variables (i.e. Variables 167 &  55, chosen by forward stepwise logistic regression).\nIn the end, to me, this contest really was a  good lesson about the power of proper variable selection &  preprocessing.\"\nNan Zhou\nAbout Nan:\nNan is currently completing his PhD in statistics at the University of Pittsburgh. His PhD research involves the estimation and prediction of  integrated volatility and model  calibration for financial stochastic processes. Prior to this he was a graduate student at Carnege Mellon University (focusing on statistical machine learning).\nNan's Method:\n\"Among lots of other models (Support Vector Machine, Random Forest, Neutral Network, Gradient Boosting, AdaBoost, and etc.) I finally used \u2018Two-Stages\u2019 L1-penalized Logistic Regression, and tune the penalty parameter by 5-folds Cross Validation.\"\nYou can hear more from the winners (and others) on the competition's forum.\n"}, {"url": "http://blog.kaggle.com/2010/09/27/how-i-did-it-lee-baker-on-winning-tourism-forecasting-part-one/", "link_text": "How I did it: Lee Baker on winning Tourism Forecasting Part One", "id": 97, "title": "Permalink to How I did it: Lee Baker on winning Tourism Forecasting Part One", "date": "2010-09-27", "text": "\nAbout me:\nI\u2019m an embedded systems engineer, currently working for a small engineering company in Las Cruces, New Mexico. I graduated from New Mexico Tech in 2007, with degrees in Electrical Engineering and Computer Science. Like many people, I first became interested in algorithm competitions with the Netflix Prize a few years ago. I was quite excited to find the Kaggle site a few months ago, as I enjoy participating in these types of competitions.\nExplanation of Technique:\nThough I tried several different methods, I used a weighted combination of three predictors to come up with the final forecast.\n\n#1: After reviewing Athanasopoulos et al, it became obvious that the naive predictor was a good algorithm with which to start. It is both easy to implement and performed well when compared to the other algorithms in the paper.\nAfter graphing a few of the time series, it became apparent that many of the series increase with time. Indeed, the second sentence of the Athanasopoulos paper states that globally tourism has grown \u201cat a rate of 6% annually.\u201d In order to take advantage of this knowledge, I multiplied the\u00a0 (Naive algorithm\u2019s) predicted value by a factor to take this growth into account. With some testing, I determined a 5.5% growth factor to yield the lowest MASE.\nprediction1 = last_value * (1.055 ** number_of_years_in_the_future)\n#2: I examined fitting a polynomial line to the data and using the line to predict future values. I tried using first through fifth order polynomials to find that the lowest MASE was obtained using a first order polynomial (simple regression line). This best fit line was used to predict future values. I also kept the r**2 value of the fit for use in blending the results of the predictor.\n#3: In thinking about these two predictors, I recognized that the naive predictor, though accurate, throws away most of the provided data, and only uses a single element of the time series. The polynomial line predictor uses all of the data, weighted equally, though the most recent data is probably of more value in indicating future performance than the earlier data in the time series. I examined and eventually used an exponentially-weighted least squares regression to fit a line to the data. This algorithm gave more accurate predictions for many of the time series by itself, and also lowered the MASE when used in combination with the two above predictors. The r**2 value of this fit was also used for blending the predictors.\nBlending stage:\nI started with a basic weighted blend of predictors. I used a constant weight factor for the modified naive predictor, while blending weights for the unweighted and weighted regression lines depending on the r**2 values found in fitting the time series. I selected the logistic function as a way of gradually increasing the weight with increasing r**2 value:\nweight = a * logistic( b * (r**2 - c))\nValues for a, b, and c were determined by trial and error. I also examined using some numeric optimization functions included in Python to minimize a training set MASE. While this succeeded in lower the training set MASE, I discarded this method when I received a lower leaderboard MASE (possibly from overfitting).\nTesting / developing algorithms:\nWhen testing algorithms, I used the last four years of the training dataset (after removing them from the data I was using for training) to test against. While this worked well in the initial stages, I found that once my leaderboard MASE got below about 2.05, this \u2018training MASE\u2019 became a much less reliable indicator of whether the leaderboard MASE would increase or decrease with a change. So, during the last few weeks of the contest, I primarily made small tweaks, and tested their value by submitting a new prediction to Kaggle rather than comparing my MASE results. I believe that this indicates a significant difference in nature of the last 4 years of the training set compared to the 4 years in the test set. If the test set includes data from 2008-2009, I\u2019m speculating that depressed tourism numbers as a result of the global economic recession could have caused a significant difference in the trends.\nPossible improvements:\nWhile the above method seemed to work fairly well at predicting tourism numbers, there are several steps that could have likely improved the score. I only implemented the Naive method implemented in the Athanasopoulos paper; I do think that including a couple of other algorithms\u2019 output into the final blend could have further increased the score. If I had a few more days to work on a solution, I would have tried to implement the Theta and ARIMA methods described in the paper and looked at the effects of including them in the blend.\nI think an investigation into how to come up with a blending method that doesn\u2019t use as much manual tweaking would also be of benefit.\nI enjoyed participating in part one, and look forward to part two of the contest.\n"}]